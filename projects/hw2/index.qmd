---
title: "Poisson Regression Examples"
author: "Brian Pintar"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

_todo: Read in data._
First, we can read in the data and do some exploratory analysis.

```{python}
import pandas as pd

df1 = pd.read_csv('blueprinty.csv')
df1.head()
```

Let's compare histograms and means of number of patents by customer status.
_todo: Compare histograms and means of number of patents by customer status. What do you observe?_
```{python}

import matplotlib.pyplot as plt

# Create two histograms side by side
plt.figure(figsize=(12, 5))

# Histogram for non-customers
plt.subplot(1, 2, 1)
plt.hist(df1[df1['iscustomer'] == 0]['patents'], bins=30, alpha=0.7)
plt.title('Non-Customers: Number of Patents')
plt.xlabel('Number of Patents')
plt.ylabel('Frequency')

# Histogram for customers
plt.subplot(1, 2, 2)
plt.hist(df1[df1['iscustomer'] == 1]['patents'], bins=30, alpha=0.7, color='orange')
plt.title('Customers: Number of Patents')
plt.xlabel('Number of Patents')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()
```

Histograms indicate somewhat similar results. Let's compare means to test for statistical significance.

```{python}
from scipy.stats import ttest_ind

# Separate the groups
patents_customers = df1[df1['iscustomer'] == 1]['patents']
patents_non_customers = df1[df1['iscustomer'] == 0]['patents']

# Calculate means
mean_customers = patents_customers.mean()
mean_non_customers = patents_non_customers.mean()

# Perform the t-test
t_stat, p_value = ttest_ind(patents_customers, patents_non_customers, equal_var=False)

# Print results
print(f"Mean patents (customers): {mean_customers:.2f}")
print(f"Mean patents (non-customers): {mean_non_customers:.2f}")
print(f"T-statistic: {t_stat:.3f}")
print(f"P-value: {p_value:.3f}")
```

Based on our t-test results, we reject the null hypothesis that the means are equal.

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

_todo: Compare regions and ages by customer status. What do you observe?_

Let's compare age between the customer and non customer groups first.
```{python}
import seaborn as sns

plt.figure(figsize=(10, 5))

sns.kdeplot(data=df1, x='age', hue='iscustomer', fill=True, common_norm=False, alpha=0.5)
plt.title('Age Distribution by Customer Status')
plt.xlabel('Age')
plt.ylabel('Density')
plt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])
plt.show()
```

There appears to be a slight difference in age between customers and non customers. Let's run a t-test to check.

```{python}
# Separate the age values
age_customers = df1[df1['iscustomer'] == 1]['age']
age_non_customers = df1[df1['iscustomer'] == 0]['age']

# Calculate means
mean_age_customers = age_customers.mean()
mean_age_non_customers = age_non_customers.mean()

# Perform the t-test
t_stat_age, p_value_age = ttest_ind(age_customers, age_non_customers, equal_var=False)

# Print results
print(f"Mean age (customers): {mean_age_customers:.2f}")
print(f"Mean age (non-customers): {mean_age_non_customers:.2f}")
print(f"T-statistic (age): {t_stat_age:.3f}")
print(f"P-value (age): {p_value_age:.3f}")
```

We fail to reject the null hypothesis, indicating no statistically significant difference in age between the customer and non customer groups. Now we repeat the process for region.

```{python}
# Create a count table (not normalized)
region_counts = pd.crosstab(df1['region'], df1['iscustomer'])
region_counts.columns = ['Non-Customers', 'Customers']

# Plot grouped bar chart
region_counts.plot(kind='bar', figsize=(10, 6))
plt.title('Number of Customers and Non-Customers by Region')
plt.xlabel('Region')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.legend(title='Customer Status')
plt.tight_layout()
plt.show()
```

It appears based on the chart that there are some differences in the proportions of customers vs. non customers in different regions. We'll run a chi-squared test to check this.

```{python}
from scipy.stats import chi2_contingency

# Crosstab: region vs customer status
region_table = pd.crosstab(df1['region'], df1['iscustomer'])

# Chi-squared test
chi2_stat, p_value_region, dof, expected = chi2_contingency(region_table)

print("Chi-squared Statistic:", round(chi2_stat, 2))
print("Degrees of Freedom:", dof)
print("P-value:", round(p_value_region, 4))
```

Based on the low p-value, we can assume there is a statistically significant relationship between region and customer status. Accordingly, the distribution of customers across regions is not random, as some are more or less likely to have customers.

### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

_todo: Write down mathematically the likelihood for_ $Y \sim \text{Poisson}(\lambda)$. Note that $f(Y|\lambda) = e^{-\lambda}\lambda^Y/Y!$.

Let's mathematically write the likelihood for_ $Y \sim \text{Poisson}(\lambda)$. Note that $f(Y|\lambda) = e^{-\lambda}\lambda^Y/Y!$.

The likelihood for all observations is:  
$L(\lambda \mid Y) = \prod_{i=1}^n \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}$

The log-likelihood is:  
$\ell(\lambda \mid Y) = \sum_{i=1}^n \left( -\lambda + Y_i \log \lambda - \log(Y_i!) \right)$

_todo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:_
Now let's write the log-likelihood function for the Poisson model as a function of lambda and Y.

```{python}
import numpy as np
from scipy.special import gammaln  # log(Y!) = gammaln(Y + 1)

def poisson_loglikelihood(lambd, Y):
    """
    Compute the log-likelihood of a Poisson model given lambda and observed data Y.

    Parameters:
    - lambd: float, the Poisson rate parameter (must be > 0)
    - Y: array-like, observed count data

    Returns:
    - log-likelihood value
    """
    if lambd <= 0:
        return -np.inf  # log-likelihood is undefined for lambda <= 0
    return np.sum(-lambd + Y * np.log(lambd) - gammaln(Y + 1))
```

_todo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y)._

Now, let's use the function to plot lambda on the horizontal axis and the log-likelihood on the vertical axis for a range of lambdas.

```{python}

# Get observed patent data
Y_obs = df1['patents'].values

# Define a range of lambda values to evaluate
lambda_vals = np.linspace(0.1, 20, 200)

# Compute the log-likelihood for each lambda
loglik_vals = [poisson_loglikelihood(l, Y_obs) for l in lambda_vals]

# Plot the log-likelihood curve
plt.figure(figsize=(8, 5))
plt.plot(lambda_vals, loglik_vals, color='blue', lw=2)
plt.title('Log-Likelihood of Poisson Model vs. Lambda')
plt.xlabel(r'$\lambda$')
plt.ylabel('Log-Likelihood')
plt.grid(True)
plt.tight_layout()
plt.show()
```

_todo: If you're feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which "feels right" because the mean of a Poisson distribution is lambda._


We start with the log-likelihood:

We start with the log-likelihood:

$$
\ell(\lambda \mid Y) = \sum_{i=1}^n \left( -\lambda + Y_i \log \lambda - \log(Y_i!) \right)
$$

Taking the derivative:

$$
\frac{d\ell}{d\lambda} = \sum_{i=1}^n \left( -1 + \frac{Y_i}{\lambda} \right)
= -n + \frac{1}{\lambda} \sum_{i=1}^n Y_i
$$

Set this equal to zero and solve:

$$
-n + \frac{1}{\lambda} \sum Y_i = 0 \Rightarrow \lambda = \frac{1}{n} \sum Y_i = \bar{Y}
$$

Therefore, the MLE of $\lambda$ is the sample mean $\bar{Y}$.

_todo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python._

Let's find the MLE by optimizing the likelihood function.

```{python}
from scipy.optimize import minimize_scalar

# Reuse observed data
Y_obs = df1['patents'].values

# Define negative log-likelihood function
def neg_loglik_poisson(lambd):
    return -poisson_loglikelihood(lambd, Y_obs)

# Use minimize_scalar to find the lambda that minimizes negative log-likelihood
result = minimize_scalar(neg_loglik_poisson, bounds=(0.01, 50), method='bounded')

# Extract MLE
lambda_mle_numerical = result.x
loglik_at_mle = -result.fun

print(f"MLE (Numerical): {lambda_mle_numerical:.4f}")
print(f"Log-Likelihood at MLE: {loglik_at_mle:.4f}")

# Compare to analytic MLE
print(f"MLE (Analytic Mean): {np.mean(Y_obs):.4f}")
```

### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

_todo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that_ $\lambda_i = e^{X_i'\beta}$. _For example:_

```
poisson_regression_likelihood <- function(beta, Y, X){
   ...
}
```

_todo: Use your function along with R's optim() or Python's sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1's to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors._

_todo: Check your results using R's glm() function or Python sm.GLM() function._

_todo: Interpret the results._ 

_todo: What do you conclude about the effect of Blueprinty's software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences._




## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::


_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._





