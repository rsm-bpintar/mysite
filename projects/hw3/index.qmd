---
title: "Multinomial Logit Model"
author: "Brian Pintar"
date: May 27, 2025
---


This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. 


## 1. Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). 

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term. 

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^3e^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$



## 2. Simulate Conjoint Data

We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a "no choice" option; each simulated respondent must select one of the 3 alternatives. 

Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \$4 to \$32 in increments of \$4.

The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is 

$$
u_{ij} = (1 \times Netflix_j) + (0.5 \times Prime_j) + (-0.8*Ads_j) - 0.1\times Price_j + \varepsilon_{ij}
$$

where the variables are binary indicators and $\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.

The following code provides the simulation of the conjoint data.

```{python}
#| code-fold: true
#| code-summary: "Show code"
import numpy as np
import pandas as pd
from itertools import product

# Set random seed for reproducibility
np.random.seed(123)

# 1. Define attribute levels
brands = ['N', 'P', 'H']  # Netflix, Prime, Hulu
ads = ['Yes', 'No']
prices = np.arange(8, 33, 4)  # $8 to $32

# 2. Generate all possible profiles
profiles = pd.DataFrame(list(product(brands, ads, prices)), columns=['brand', 'ad', 'price'])

# 3. Define utility weights
brand_util = {'N': 1.0, 'P': 0.5, 'H': 0.0}
ad_util = {'Yes': -0.8, 'No': 0.0}
price_util = lambda p: -0.1 * p

# 4. Simulation parameters
n_respondents = 100
n_tasks = 10
n_alts = 3

# 5. Simulate choice tasks for each respondent
simulated_data = []

for respondent_id in range(1, n_respondents + 1):
    for task_id in range(1, n_tasks + 1):
        sampled_profiles = profiles.sample(n=n_alts).reset_index(drop=True)
        sampled_profiles['brand_util'] = sampled_profiles['brand'].map(brand_util)
        sampled_profiles['ad_util'] = sampled_profiles['ad'].map(ad_util)
        sampled_profiles['price_util'] = sampled_profiles['price'].apply(price_util)
        
        # Compute deterministic utility
        sampled_profiles['utility'] = (
            sampled_profiles['brand_util'] +
            sampled_profiles['ad_util'] +
            sampled_profiles['price_util']
        )
        
        # Add Gumbel noise
        gumbel_noise = np.random.gumbel(loc=0, scale=1, size=n_alts)
        sampled_profiles['total_utility'] = sampled_profiles['utility'] + gumbel_noise

        # Determine choice (1 if max utility, else 0)
        choice_index = sampled_profiles['total_utility'].idxmax()
        sampled_profiles['choice'] = 0
        sampled_profiles.loc[choice_index, 'choice'] = 1

        # Add metadata
        sampled_profiles['respondent'] = respondent_id
        sampled_profiles['task'] = task_id

        simulated_data.append(sampled_profiles)

# Combine into a single DataFrame
df_simulated = pd.concat(simulated_data, ignore_index=True)
```



## 3. Preparing the Data for Estimation

The "hard part" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.


```{python}
#| code-fold: true
#| code-summary: "Show code"
# One-hot encode brand and ad (drop the reference levels: brand_H and ad_No)
df_design = pd.get_dummies(df_simulated, columns=['brand', 'ad'], drop_first=True)

# Keep only relevant columns for X matrix
X_columns = ['brand_N', 'brand_P', 'ad_Yes', 'price']
X = df_design[X_columns]

# Outcome variable: 1 if alternative was chosen, 0 otherwise
y = df_design['choice'].values

# Also store respondent and task IDs for future grouping if needed
respondent_ids = df_design['respondent'].values
task_ids = df_design['task'].values

# Show a preview of the prepared design matrix
df_prepared = df_design[['respondent', 'task'] + X_columns + ['choice']]

df_prepared.head()
```

## 4. Estimation via Maximum Likelihood

To estimate the part-worth parameters of the multinomial logit model, we define the log-likelihood function based on the softmax probability formulation. Each respondent's utility for each alternative is calculated as a linear function of the attributes (brand, ad presence, and price). We then maximize the log-likelihood using `scipy.optimize.minimize()` with the BFGS algorithm.

The estimated coefficients correspond to:
- $\beta_{\text{netflix}}$: preference for Netflix (vs. Hulu)
- $\beta_{\text{prime}}$: preference for Prime (vs. Hulu)
- $\beta_{\text{ads}}$: penalty for ad-included options (vs. ad-free)
- $\beta_{\text{price}}$: marginal disutility per dollar

We also compute standard errors from the inverse Hessian matrix and report 95% confidence intervals for each parameter estimate.

```{python}
#| code-fold: true
#| code-summary: "Show code"
# Re-import necessary packages after code execution state reset
import numpy as np
import pandas as pd
from scipy.optimize import minimize

# Ensure X and y are NumPy arrays
X = df_design[['brand_N', 'brand_P', 'ad_Yes', 'price']].to_numpy(dtype=np.float64)
y = df_design['choice'].to_numpy(dtype=np.int64)

# Reshape y into (n_tasks, 3)
y_matrix = y.reshape((-1, 3))
n_tasks_total = y_matrix.shape[0]

# Define negative log-likelihood for MNL
def neg_log_likelihood(beta):
    beta = np.asarray(beta, dtype=np.float64)
    utilities = X @ beta
    utilities = utilities.reshape((-1, 3))
    exp_utils = np.exp(utilities)
    probs = exp_utils / np.sum(exp_utils, axis=1, keepdims=True)
    chosen_probs = probs[np.arange(n_tasks_total), y_matrix.argmax(axis=1)]
    return -np.sum(np.log(chosen_probs))

# Initial guess
beta_init = np.zeros(X.shape[1])

# Minimize the negative log-likelihood
result = minimize(neg_log_likelihood, beta_init, method='BFGS')

# Extract estimates and standard errors
beta_hat = result.x
hessian_inv = result.hess_inv
std_errors = np.sqrt(np.diag(hessian_inv))
z = 1.96

# Confidence intervals
conf_int = np.column_stack([
    beta_hat - z * std_errors,
    beta_hat + z * std_errors
])

# Package results
param_names = ['brand_N', 'brand_P', 'ad_Yes', 'price']
results_df = pd.DataFrame({
    'Parameter': param_names,
    'Estimate': beta_hat,
    'Std. Error': std_errors,
    'CI Lower (95%)': conf_int[:, 0],
    'CI Upper (95%)': conf_int[:, 1]
})

results_df
```

The table above reports the estimated part-worth utilities for each attribute level in the multinomial logit (MNL) model. All four coefficients are statistically significant at the 95% confidence level, as none of their confidence intervals include zero. The signs and magnitudes of the estimates align with economic intuition and the true values used in the simulation.

Brand Preferences: Consumers exhibit a strong preference for Netflix over Hulu (baseline), with an estimated utility gain of approximately 0.98 units. Amazon Prime is also preferred over Hulu, though to a lesser degree (0.43 units). These estimates closely match the simulated values of 1.0 and 0.5, respectively.

Advertising: The presence of advertisements decreases the utility of a streaming offer by about 0.73 units. This negative effect is in line with the simulated disutility of -0.8 and indicates that consumers have a strong preference for ad-free experiences.

Price Sensitivity: Each additional dollar in monthly price reduces utility by approximately 0.11 units. This estimate is tightly bounded and very close to the simulated effect of -0.1, suggesting that the model has successfully captured consumers' price sensitivity.

Overall, the MNL model recovers the true preference structure well, validating both the simulation setup and the estimation procedure.

## 5. Estimation via Bayesian Methods

We now estimate the multinomial logit model using a Bayesian approach via Metropolis-Hastings MCMC. The posterior distribution is defined by combining the log-likelihood with prior distributions:

- Normal priors $\mathcal{N}(0, 5)$ for the binary attribute coefficients (`brand_N`, `brand_P`, `ad_Yes`)
- A tighter prior $\mathcal{N}(0, 1)$ for the continuous `price` coefficient

We run the sampler for 11,000 iterations, discarding the first 1,000 as burn-in, and use the remaining 10,000 to summarize the posterior distribution.

To generate proposals, we use a multivariate normal distribution with independent components:
- $\mathcal{N}(0, 0.05)$ for the three binary variables
- $\mathcal{N}(0, 0.005)$ for the price coefficient

We assess convergence by examining a trace plot and posterior histogram for the `price` parameter.

```{python}
#| code-fold: true
#| code-summary: "Show code"
import matplotlib.pyplot as plt
from itertools import product

# Re-run data simulation and preparation
np.random.seed(123)
brands = ['N', 'P', 'H']
ads = ['Yes', 'No']
prices = np.arange(8, 33, 4)
profiles = pd.DataFrame(list(product(brands, ads, prices)), columns=['brand', 'ad', 'price'])

brand_util = {'N': 1.0, 'P': 0.5, 'H': 0.0}
ad_util = {'Yes': -0.8, 'No': 0.0}
price_util = lambda p: -0.1 * p

n_respondents = 100
n_tasks = 10
n_alts = 3

simulated_data = []
for respondent_id in range(1, n_respondents + 1):
    for task_id in range(1, n_tasks + 1):
        sampled = profiles.sample(n=n_alts).reset_index(drop=True)
        sampled['brand_util'] = sampled['brand'].map(brand_util)
        sampled['ad_util'] = sampled['ad'].map(ad_util)
        sampled['price_util'] = sampled['price'].apply(price_util)
        sampled['utility'] = sampled['brand_util'] + sampled['ad_util'] + sampled['price_util']
        noise = np.random.gumbel(0, 1, n_alts)
        sampled['total_utility'] = sampled['utility'] + noise
        sampled['choice'] = 0
        sampled.loc[sampled['total_utility'].idxmax(), 'choice'] = 1
        sampled['respondent'] = respondent_id
        sampled['task'] = task_id
        simulated_data.append(sampled)

df_simulated = pd.concat(simulated_data, ignore_index=True)
df_design = pd.get_dummies(df_simulated, columns=['brand', 'ad'], drop_first=True)
X = df_design[['brand_N', 'brand_P', 'ad_Yes', 'price']].to_numpy(dtype=np.float64)
y = df_design['choice'].to_numpy(dtype=np.int64)
y_matrix = y.reshape((-1, 3))
n_tasks_total = y_matrix.shape[0]

# Define functions
def log_likelihood(beta):
    utilities = X @ beta
    utilities = utilities.reshape((-1, 3))
    exp_utilities = np.exp(utilities)
    probs = exp_utilities / exp_utilities.sum(axis=1, keepdims=True)
    chosen_probs = probs[np.arange(n_tasks_total), y_matrix.argmax(axis=1)]
    return np.sum(np.log(chosen_probs))

def log_prior(beta):
    return (
        -0.5 * (beta[0]**2 / 25)
        -0.5 * (beta[1]**2 / 25)
        -0.5 * (beta[2]**2 / 25)
        -0.5 * (beta[3]**2 / 1)
    )

def log_posterior(beta):
    return log_likelihood(beta) + log_prior(beta)

# Metropolis-Hastings MCMC
n_iter = 11000
beta_samples = np.zeros((n_iter, 4))
beta_current = np.zeros(4)
log_post_current = log_posterior(beta_current)

for i in range(1, n_iter):
    proposal = beta_current + np.random.normal(loc=0, scale=[0.05, 0.05, 0.05, 0.005])
    log_post_proposal = log_posterior(proposal)
    accept_prob = np.exp(log_post_proposal - log_post_current)
    if np.random.rand() < accept_prob:
        beta_current = proposal
        log_post_current = log_post_proposal
    beta_samples[i] = beta_current

# Posterior summaries after burn-in
burn_in = 1000
beta_samples_post = beta_samples[burn_in:]
posterior_means = beta_samples_post.mean(axis=0)
posterior_sds = beta_samples_post.std(axis=0)
cred_intervals = np.percentile(beta_samples_post, [2.5, 97.5], axis=0).T

# Results DataFrame
param_names = ['brand_N', 'brand_P', 'ad_Yes', 'price']
bayes_results_df = pd.DataFrame({
    'Parameter': param_names,
    'Posterior Mean': posterior_means,
    'Posterior SD': posterior_sds,
    'CI Lower (95%)': cred_intervals[:, 0],
    'CI Upper (95%)': cred_intervals[:, 1]
})

# Trace plot and histogram for price
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(beta_samples_post[:, 3])
plt.title('Trace Plot: price')
plt.xlabel('Iteration')
plt.ylabel('Value')

plt.subplot(1, 2, 2)
plt.hist(beta_samples_post[:, 3], bins=30, edgecolor='k')
plt.title('Posterior Histogram: price')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()


bayes_results_df.head()
```

The posterior summary table shows the means, standard deviations, and 95% credible intervals for each parameter. These estimates closely align with those obtained from maximum likelihood estimation in Section 4.

- The **posterior means** match the MLE estimates within small margins.
- The **credible intervals** are tight and all exclude zero, indicating strong evidence for the direction and magnitude of each effect.
- The **trace plot** for the price parameter shows good mixing behavior, and the posterior distribution appears approximately normal.

These results confirm that both MLE and Bayesian methods yield consistent and interpretable insights about consumer preferences in this simulated conjoint setting.

## 6. Discussion

### Interpreting Parameter Estimates

Even without knowing the true data-generating process (i.e., in a real-world setting), the estimated parameters from the multinomial logit model align with strong economic intuition:

- **$\beta_{\text{Netflix}} > \beta_{\text{Prime}}$** implies that, on average, consumers have a stronger preference for Netflix than Amazon Prime, relative to the baseline brand (Hulu). This could be due to perceived content quality, popularity, or brand loyalty.

- A **negative $\beta_{\text{price}}$** is expected and logical. It means that, all else equal, consumers are less likely to choose a more expensive plan. The magnitude reflects how sensitive utility is to price — a critical insight for pricing strategy.

- The ad penalty coefficient, **$\beta_{\text{ads}} < 0$**, indicates that consumers value ad-free experiences and are willing to give up some utility (or pay more) to avoid ads.

These results demonstrate how the MNL model can quantify the impact of product features on consumer decision-making, even when the true parameters are unknown.

### Toward Hierarchical (Multilevel) Models

In real-world conjoint studies, different consumers often have different preferences — not everyone values brand or price the same way. To capture this heterogeneity, we would move from a fixed-parameter MNL model to a **hierarchical (random-parameter) model**.

This change involves two key modifications:

1. **Data Simulation**: Instead of assigning one fixed set of $\beta$ values to all respondents, we would draw individual-level parameters from a population distribution. This would better reflect individual variation in preferences.

2. **Estimation**: We would use a **Bayesian hierarchical model** (via MCMC) or a **Mixed Logit** model (via simulation-based MLE) to estimate both:
   - The population-level means $\mu$
   - The variation across individuals, captured by $\Sigma$

These models are more flexible and realistic, and are commonly used in industry applications of conjoint analysis.

In summary, while the standard MNL model is a solid starting point, moving to a hierarchical framework is critical for analyzing real consumer choice data with preference heterogeneity.










