---
title: "A Replication of Karlan and List (2007)"
author: "Brian Pintar"
date: April 22, 2025
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Introduction

Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the _American Economic Review_ in 2007. The article and supporting data are available from the [AEA website](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774) and from Innovations for Poverty Action as part of [Harvard's Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27853&version=4.2).

Published in 2007, this experiment was inspired by America's significant increase in private charitable giving in the decades prior. It was alluded to the fact that a combination of increased wealth and an aging population in America likely caused this increase. Their analysis originally concluded that the match offer increases both the revenue per solicitation and the response rate. However, larger match ratios (i.e., $3:$1 instead of $1:$1) did not have any additional impact. It was also concluded that the matching had a much larger effect in red states than blue states.

This project seeks to replicate their results.


## Data

### Description

```{python}
import pandas as pd
df = pd.read_stata('karlan_list_2007.dta')
df.head()
```

:::: {.callout-note collapse="true"}
### Variable Definitions

| Variable             | Description                                                         |
|----------------------|---------------------------------------------------------------------|
| `treatment`          | Treatment                                                           |
| `control`            | Control                                                             |
| `ratio`              | Match ratio                                                         |
| `ratio2`             | 2:1 match ratio                                                     |
| `ratio3`             | 3:1 match ratio                                                     |
| `size`               | Match threshold                                                     |
| `size25`             | \$25,000 match threshold                                            |
| `size50`             | \$50,000 match threshold                                            |
| `size100`            | \$100,000 match threshold                                           |
| `sizeno`             | Unstated match threshold                                            |
| `ask`                | Suggested donation amount                                           |
| `askd1`              | Suggested donation was highest previous contribution                |
| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |
| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |
| `ask1`               | Highest previous contribution (for suggestion)                      |
| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |
| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |
| `amount`             | Dollars given                                                       |
| `gave`               | Gave anything                                                       |
| `amountchange`       | Change in amount given                                              |
| `hpa`                | Highest previous contribution                                       |
| `ltmedmra`           | Small prior donor: last gift was less than median \$35              |
| `freq`               | Number of prior donations                                           |
| `years`              | Number of years since initial donation                              |
| `year5`              | At least 5 years since initial donation                             |
| `mrm2`               | Number of months since last donation                                |
| `dormant`            | Already donated in 2005                                             |
| `female`             | Female                                                              |
| `couple`             | Couple                                                              |
| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |
| `nonlit`             | Nonlitigation                                                       |
| `cases`              | Court cases from state in 2004-5 in which organization was involved |
| `statecnt`           | Percent of sample from state                                        |
| `stateresponse`      | Proportion of sample from the state who gave                        |
| `stateresponset`     | Proportion of treated sample from the state who gave                |
| `stateresponsec`     | Proportion of control sample from the state who gave                |
| `stateresponsetminc` | stateresponset - stateresponsec                                     |
| `perbush`            | State vote share for Bush                                           |
| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |
| `red0`               | Red state                                                           |
| `blue0`              | Blue state                                                          |
| `redcty`             | Red county                                                          |
| `bluecty`            | Blue county                                                         |
| `pwhite`             | Proportion white within zip code                                    |
| `pblack`             | Proportion black within zip code                                    |
| `page18_39`          | Proportion age 18-39 within zip code                                |
| `ave_hh_sz`          | Average household size within zip code                              |
| `median_hhincome`    | Median household income within zip code                             |
| `powner`             | Proportion house owner within zip code                              |
| `psch_atlstba`       | Proportion who finished college within zip code                     |
| `pop_propurban`      | Proportion of population urban within zip code                      |

::::


### Balance Test 

As an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.

```{python}
import numpy as np
from scipy.stats import t

def t_test_randomization_check(df, column, treat_col='treatment', control_col='control'):
    """
    Performs Welch's t-test to compare treatment and control groups for a given variable.

    Parameters:
    - df: pandas DataFrame
    - column: name of the column to compare (e.g., 'mrm2')
    - treat_col: name of the treatment indicator column (default: 'treatment')
    - control_col: name of the control indicator column (default: 'control')
    """

    # Extract the groups
    treat = df[df[treat_col] == 1][column].dropna()
    control = df[df[control_col] == 1][column].dropna()

    # Means and standard deviations
    mean_treat = treat.mean()
    mean_control = control.mean()
    std_treat = treat.std(ddof=1)
    std_control = control.std(ddof=1)

    # Sample sizes
    n_treat = len(treat)
    n_control = len(control)

    # t-statistic
    numerator = mean_treat - mean_control
    denominator = np.sqrt((std_treat**2)/n_treat + (std_control**2)/n_control)
    t_stat = numerator / denominator

    # Degrees of freedom (Welch-Satterthwaite)
    var_treat = std_treat**2 / n_treat
    var_control = std_control**2 / n_control
    df_numerator = (var_treat + var_control)**2
    df_denominator = (var_treat**2 / (n_treat - 1)) + (var_control**2 / (n_control - 1))
    df_welch = df_numerator / df_denominator

    # p-value
    p_value = 2 * (1 - t.cdf(np.abs(t_stat), df=df_welch))

    # Output
    print(f"\n🧪 T-Test for '{column}': Treatment vs Control")
    print(f"Treatment mean: {mean_treat:.2f}; Control mean: {mean_control:.2f}")
    print(f"Treatment std dev: {std_treat:.2f}; Control std dev: {std_control:.2f}")
    print(f"t-statistic: {t_stat:.4f}")
    print(f"Degrees of freedom: {df_welch:.2f}")
    print(f"p-value: {p_value:.4f}")

    # Decision
    if p_value < 0.05:
        print("❗ Reject the null hypothesis: The groups are significantly different.")
    else:
        print("✅ Fail to reject the null hypothesis: No significant difference between groups.")


t_test_randomization_check(df, 'mrm2')
t_test_randomization_check(df, 'female')
t_test_randomization_check(df, 'red0')

```

The above results confirm randomization of the experiment. There was no significant difference in the treatment and control groups regarding the number of months since last donation, the percentage of male/female participants, or participants in red/blue states, which could have affected the results. Let's compare results with a linear regression for the months since last donation variable 'mrm2' to confirm our approach.

```{python}
import statsmodels.api as sm

df_reg = df[['mrm2', 'treatment']].dropna()

#define x and y
X = df_reg['treatment']
X = sm.add_constant(X)
y = df_reg['mrm2']

#fit model
model = sm.OLS(y,X).fit()

#extract values
coef = model.params['treatment']
t_stat = model.tvalues['treatment']
p_value = model.pvalues['treatment']

print(f"Coefficient (treatment): {coef:.4f}")
print(f"t-statistic: {t_stat:.4f}")
print(f"p-value: {p_value:.4f}")
```

The t-statistics and p-values match accordingly, confirming the method.

Table 1 is included in the original report to provide evidence of randomization of the treatment and control groups. This is proven to be true, being that we fail to reject the null hypothesis with no statistically significant difference in demographics for treatment and control groups.

## Experimental Results

### Charitable Contribution Made

First, I analyze whether matched donations lead to an increased response rate of making a donation. 

```{python}
import matplotlib.pyplot as plt

#calculate proportions
prop_treatment = df[df['treatment'] == 1]['gave'].mean()
prop_control = df[df['control'] == 1]['gave'].mean()

#data for plot
groups = ['Treatment', 'Control']
proportions = [prop_treatment, prop_control]

#plot
plt.figure(figsize=(6,4))
plt.bar(groups, proportions)
plt.ylabel('Proportion Donated')
plt.title('Donation Rate by Group')
plt.ylim(0,.05)
plt.show()
```

Based on the bar chart above, it appears that the treatment had a slight effect on donation rates.

```{python}
df_lpm = df[['gave', 'treatment']].dropna()

#define outcome and predictor
X = sm.add_constant(df_lpm['treatment'])
y = df_lpm['gave']

#fit linear regression
model = sm.OLS(y, X).fit()

#extract stats
coef = model.params['treatment']
t_stat = model.tvalues['treatment']
p_value = model.pvalues['treatment']

#output
print(f"Coefficient (treatment effect): {coef:.4f}")
print(f"t-statistic: {t_stat:.4f}")
print(f"p-value: {p_value:.4f}")
```

While the effect is small, it is statistically significant based on the p-value of less than .05. This implies that people in general provided donations more when told that donations would be matched. 

```{python}
df_probit = df[['gave', 'treatment']].dropna()

#define predictors and outcome
X = sm.add_constant(df_probit['treatment'])
y = df_probit['gave']

#fit probit model
probit_model = sm.Probit(y, X).fit()

mfx = probit_model.get_margeff()
print(mfx.summary())

```

The probit regression indicates an increase in probability of donating by 0.43%, holding all else constant, when given the treatment (matching). This is consistent with the linear regression results, as effect is small. P-values also match, and are statistically significant. Treatment has a statistically significant but modest positive effect on probability of donation.

_NOTE: Linear regression results appear replicate Table 3 column 1 in the paper. Probit results do not, despite Table 3 indicating its results come from probit regressions..._

### Differences between Match Rates

Next, I assess the effectiveness of different sizes of matched donations on the response rate.

```{python}
def t_test_donation_rate(df, ratio_a, ratio_b, outcome_col='gave', ratio_col='ratio'):
    """
    Performs and prints a Welch's t-test comparing donation rates between two match ratio groups.
    
    Parameters:
    - df: pandas DataFrame
    - ratio_a: first ratio value (e.g., 1, 2, 3, or "Control")
    - ratio_b: second ratio value
    - outcome_col: name of the binary outcome column (default: 'gave')
    - ratio_col: name of the match ratio column (default: 'ratio')
    
    Returns:
    - Dictionary of test results
    """
    
    group_a = df[df[ratio_col] == ratio_a][outcome_col].dropna()
    group_b = df[df[ratio_col] == ratio_b][outcome_col].dropna()

    n1, n2 = len(group_a), len(group_b)
    p1, p2 = group_a.mean(), group_b.mean()

    # Standard error
    se = np.sqrt((p1 * (1 - p1)) / n1 + (p2 * (1 - p2)) / n2)

    # t-statistic
    t_stat = (p2 - p1) / se

    # Degrees of freedom
    var1 = (p1 * (1 - p1)) / n1
    var2 = (p2 * (1 - p2)) / n2
    df_welch = (var1 + var2)**2 / ((var1**2)/(n1 - 1) + (var2**2)/(n2 - 1))

    # Two-tailed p-value
    p_value = 2 * (1 - t.cdf(np.abs(t_stat), df=df_welch))

    # Print results
    print(f"\n🎯 T-Test: {ratio_b}:1 vs {ratio_a}:1 Match Ratio")
    print("-" * 40)
    print(f"Sample size ({ratio_a}:1): {n1}")
    print(f"Sample size ({ratio_b}:1): {n2}")
    print(f"Donation rate ({ratio_a}:1): {p1:.4f}")
    print(f"Donation rate ({ratio_b}:1): {p2:.4f}")
    print(f"t-statistic: {t_stat:.4f}")
    print(f"Degrees of freedom: {df_welch:.2f}")
    print(f"p-value: {p_value:.4f}")
    
    if p_value < 0.05:
        print("✅ Statistically significant at the 5% level.")
    else:
        print("❌ Not statistically significant at the 5% level.")

t_test_donation_rate(df, 1, 2)  # 1:1 vs 2:1
t_test_donation_rate(df, 2, 3)
t_test_donation_rate(df, 1, 3)

```

Author suggests that neither the different match thresholds or example amount had a meaningful influence on behavior. Results above suggest the same. Not enough evidence to conclude that the difference in donation rates is statistically significant for different treatment options.

Create the ratio1 variable.
```{python}
#create the ratio1 variable
df['ratio_str'] = df['ratio'].astype('str')
df['ratio1'] = (df['ratio_str'] == '1').astype(int)
df['ratio2'] = (df['ratio_str'] == '2').astype(int)
df['ratio3'] = (df['ratio_str'] == '3').astype(int)
print(df[['ratio', 'ratio1', 'ratio2', 'ratio3']].head(10))
```

Regression results are presented below.

```{python}
def compare_match_ratios(df, ratio_a, ratio_b, ratio_col='ratio', outcome_col='gave'):
    """
    Compare two match ratios using linear regression on a binary outcome.

    Parameters:
    - df: DataFrame with data
    - ratio_a: first match ratio to compare (e.g., 1 for 1:1)
    - ratio_b: second match ratio to compare (e.g., 2 for 2:1)
    - ratio_col: name of the column containing match ratios
    - outcome_col: name of the binary outcome column

    Returns:
    - Dictionary with coefficient and p-value
    """

    # Filter to only the two groups being compared
    df_sub = df[df[ratio_col].isin([ratio_a, ratio_b])].copy()

    # Create indicator for being in ratio_b group
    df_sub['is_ratio_b'] = (df_sub[ratio_col] == ratio_b).astype(int)

    # Run regression
    X = sm.add_constant(df_sub['is_ratio_b'])
    y = df_sub[outcome_col]

    model = sm.OLS(y, X).fit()

    coef = model.params['is_ratio_b']
    p_value = model.pvalues['is_ratio_b']

    print(f"\n📊 Comparing donation rates: {ratio_b}:1 vs {ratio_a}:1")
    print(f"Coefficient (diff in donation rate): {coef:.4f}")
    print(f"p-value: {p_value:.4f}")

compare_match_ratios(df, 1, 2)  # Compare 2:1 vs 1:1
compare_match_ratios(df, 2, 3)
compare_match_ratios(df, 1, 3) 
```

The regression results above indicate the same. The differences between donation rates of the different match ratios are not statistically significant, and therefore do not indicate any effect.

```{python}
# Response (donation) rates
p_1 = df[df['ratio'] == 1]['gave'].mean()
p_2 = df[df['ratio'] == 2]['gave'].mean()
p_3 = df[df['ratio'] == 3]['gave'].mean()

# Differences in raw proportions
diff_2v1 = p_2 - p_1
diff_3v2 = p_3 - p_2
diff_3v1 = p_3 - p_1

print(f"Direct from data:")
print(f"2:1 vs 1:1: {diff_2v1:.4f}")
print(f"3:1 vs 2:1: {diff_3v2:.4f}")
print(f"3:1 vs 1:1: {diff_3v1:.4f}")
```

Results from data match the regression coefficients. We can safely conclude that there is no effect that match ratios have on donation rates.

### Size of Charitable Contribution

In this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.

_todo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?_
```{python}
df_reg = df[['amount', 'treatment']].dropna()

X = sm.add_constant(df_reg['treatment'])
y = df_reg['amount']

model = sm.OLS(y, X).fit()

coef = model.params['treatment']
pval = model.pvalues['treatment']

print(f"Treatment effect (difference in donation amount): {coef:.4f}")
print(f"p-value: {pval:.4f}")
```

While the results from the regression analysis above indicate an increase of 0.15 in the donation amount when given the treatment letter, the p-value is not statistically significant at the 95% confidence level. However, the difference calculated is accurate compared to Table2A of original analysis.  

```{python}
# Filter to donors only
df_donors = df[(df['amount'] > 0) & df['treatment'].notna()][['amount', 'treatment']]

X = sm.add_constant(df_donors['treatment'])
y = df_donors['amount']

model = sm.OLS(y, X).fit()

coef = model.params['treatment']
pval = model.pvalues['treatment']

print(f"Treatment effect among donors: {coef:.2f}")
print(f"p-value: {pval:.4f}")
```

While the results of the above regression, only considering those who actually donated to begin with, indicate a small decrease in the donation amount, this amount is not statistically significant. Therefore, we cannot conclude that the treatment letters (those with the match ratios) have any significant causal effect on the actual donation amount. 

```{python}
# Filter to donors only
df_donors = df[df['amount'] > 0]

# Split groups
treatment_amounts = df_donors[df_donors['treatment'] == 1]['amount']
control_amounts = df_donors[df_donors['control'] == 1]['amount']

# Calculate means
mean_treatment = treatment_amounts.mean()
mean_control = control_amounts.mean()

# Plot: Control Group
plt.figure(figsize=(8, 5))
plt.hist(control_amounts, bins=30, alpha=0.7, color='gray', edgecolor='black')
plt.axvline(mean_control, color='red', linestyle='--', linewidth=2, label=f'Mean: ${mean_control:.2f}')
plt.title('Donation Amounts (Control Group)')
plt.xlabel('Donation Amount')
plt.ylabel('Number of Donors')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot: Treatment Group
plt.figure(figsize=(8, 5))
plt.hist(treatment_amounts, bins=30, alpha=0.7, color='gray', edgecolor='black')
plt.axvline(mean_treatment, color='red', linestyle='--', linewidth=2, label=f'Mean: ${mean_treatment:.2f}')
plt.title('Donation Amounts (Treatment Group)')
plt.xlabel('Donation Amount')
plt.ylabel('Number of Donors')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
```

The two histograms above indicate a similar average and similar distribution of donation amounts among those who donated to begin with.

## Simulation Experiment

As a reminder of how the t-statistic "works," in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.

Suppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. 

Further suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.

### Law of Large Numbers

```{python}
# Set random seed for reproducibility
np.random.seed(42)

# Step 1: Simulate 10k Bernoulli draws for each group
n_draws = 10000
control_draws = np.random.binomial(1, 0.018, size=n_draws)
treatment_draws = np.random.binomial(1, 0.022, size=n_draws)

# Step 2: Compute the vector of differences
diffs = treatment_draws - control_draws  # element-wise difference

# Step 3: Compute cumulative average of the differences
cumulative_avg = np.cumsum(diffs) / np.arange(1, n_draws + 1)

# Step 4: Plot cumulative average
plt.figure(figsize=(10, 6))
plt.plot(cumulative_avg, label='Cumulative Average Treatment Effect')
plt.axhline(y=0.004, color='red', linestyle='--', label='True Difference (0.022 - 0.018)')
plt.title('Cumulative Average of Simulated Treatment Effect')
plt.xlabel('Number of Simulations')
plt.ylabel('Cumulative Average Difference')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
```

Accordingly, in the simulation above, the cumulative average eventually converges towards the true difference of 0.004. As the sample size of the random simulation gets larger, the average eventually converges to the true average.

### Central Limit Theorem

```{python}
# Set seed
np.random.seed(42)

# Parameters
p_control = 0.018
p_treatment = 0.022
n_simulations = 1000
sample_sizes = [50, 200, 500, 1000]

# Create subplots
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.flatten()

for i, n in enumerate(sample_sizes):
    mean_diffs = []
    
    for _ in range(n_simulations):
        # Sample from each distribution
        control_sample = np.random.binomial(1, p_control, size=n)
        treatment_sample = np.random.binomial(1, p_treatment, size=n)

        # Calculate mean difference
        diff = treatment_sample.mean() - control_sample.mean()
        mean_diffs.append(diff)
    
    # Plot histogram
    axes[i].hist(mean_diffs, bins=30, edgecolor='black', alpha=0.75)
    axes[i].axvline(np.mean(mean_diffs), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(mean_diffs):.4f}')
    axes[i].set_title(f'Sample Size = {n}')
    axes[i].set_xlabel('Mean Difference')
    axes[i].set_ylabel('Frequency')
    axes[i].legend()
    axes[i].grid(True)

# Final layout
plt.suptitle('Sampling Distribution of Mean Differences (Treatment - Control)', fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
```
When running a simulation experiment, the sampling distribution takes on a bell-shape very quickly, even at sample size of 50, but gets smoothed out by the time sample size is 1000. Central Limit Theorem takes place earlier in the sequence than anticipated, approximating a normal distribution, but more filled in and smooth by 1000 sample size.



