---
title: "Machine Learning"
author: "Brian Pintar"
date: today
engine: knitr
jupyter: python3
---


## K-Means Clustering: Implemented from Scratch

In this post, I implement the K-Means clustering algorithm **from scratch** and apply it to the Palmer Penguins dataset.

The goal is to:
1. Write the K-Means algorithm by hand in Python.
2. Visualize how it works on real data.
3. Compare it with Python’s built-in `KMeans` implementation.
4. Evaluate model performance using **within-cluster sum of squares (WCSS)** and **silhouette scores**.
5. Determine the “right” number of clusters.

We’ll use two features: **bill length** and **flipper length** — a natural fit for 2D visualization of clustering in action.

---

How the Algorithm Works:

K-Means is an **unsupervised learning** algorithm that partitions data into `K` clusters. The algorithm seeks to minimize the **within-cluster sum of squares (WCSS)** and follows these steps:

1. Randomly initialize `K` centroids.
2. Assign each data point to the nearest centroid.
3. Recompute each centroid based on current cluster assignments.
4. Repeat until centroids converge or max iterations is reached.

---

Implementing K-Means in Python:


```{python}
#| code-fold: true
#| code-summary: "Show code"
import numpy as np
import matplotlib.pyplot as plt

def initialize_centroids(X, k):
    indices = np.random.choice(len(X), size=k, replace=False)
    return X[indices]

def assign_clusters(X, centroids):
    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
    return np.argmin(distances, axis=1)

def update_centroids(X, labels, k):
    return np.array([X[labels == i].mean(axis=0) for i in range(k)])

def kmeans(X, k, max_iters=100, tol=1e-4):
    centroids = initialize_centroids(X, k)
    for i in range(max_iters):
        old_centroids = centroids
        labels = assign_clusters(X, centroids)
        centroids = update_centroids(X, labels, k)
        if np.allclose(centroids, old_centroids, atol=tol):
            break
    return centroids, labels
```

We define functions to:
- Initialize centroids randomly
- Assign each point to the nearest centroid
- Update centroids using the mean of assigned points
- Iterate until convergence

Now let’s test it on the **Palmer Penguins** dataset.

```{python}
#| code-fold: true
#| code-summary: "Show code"

#| code-fold: true
#| code-summary: "Show code"
import seaborn as sns
import pandas as pd

penguins = sns.load_dataset("penguins").dropna(subset=["bill_length_mm", "flipper_length_mm"])
X = penguins[["bill_length_mm", "flipper_length_mm"]].values

plt.figure(figsize=(6, 5))
plt.scatter(X[:, 0], X[:, 1], c='gray', edgecolor='black')
plt.xlabel("Bill Length (mm)")
plt.ylabel("Flipper Length (mm)")
plt.title("Palmer Penguins: Bill Length vs Flipper Length")
plt.show()

```

To better understand how K-Means converges, I modified the algorithm to store the centroids and cluster labels at each iteration.

Executing such visualization here:

```{python}
#| code-fold: true
#| code-summary: "Show code"
from matplotlib.animation import FuncAnimation

def kmeans_with_plot(X, k, max_iters=10):
    centroids = initialize_centroids(X, k)
    history = []
    for i in range(max_iters):
        labels = assign_clusters(X, centroids)
        history.append((centroids.copy(), labels.copy()))
        new_centroids = update_centroids(X, labels, k)
        if np.allclose(new_centroids, centroids):
            break
        centroids = new_centroids
    history.append((centroids.copy(), labels.copy()))
    return history

# Run it and store the steps
history = kmeans_with_plot(X, k=3)

# Animate it
fig, ax = plt.subplots(figsize=(6, 5))

def animate(i):
    ax.clear()
    centroids, labels = history[i]
    for j in range(len(centroids)):
        ax.scatter(X[labels == j, 0], X[labels == j, 1], label=f"Cluster {j}")
    ax.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=100, label='Centroids')
    ax.set_title(f"K-Means Iteration {i}")
    ax.set_xlabel("Bill Length (mm)")
    ax.set_ylabel("Flipper Length (mm)")
    ax.legend()

ani = FuncAnimation(fig, animate, frames=len(history), interval=1000, repeat=False)

# Save the animation to your output folder (this file must exist in same dir)
ani.save("kmeans_penguins_animation.mp4", writer='ffmpeg')

```
> ![](kmeans_penguins_animation.mp4)

Each frame above shows the K-Means algorithm as it:
- Reassigns data points to the nearest centroid
- Moves centroids to the mean of their assigned points
- Gradually stabilizes and stops updating

This visualization helps demonstrate how K-Means converges over a few iterations. It's especially helpful for building intuition around how cluster boundaries emerge.

```{python}
#| code-fold: true
#| code-summary: "Show code"
k = 3
centroids, labels = kmeans(X, k)

plt.figure(figsize=(6, 5))
for i in range(k):
    cluster_points = X[labels == i]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i}')
plt.scatter(centroids[:, 0], centroids[:, 1], color='black', marker='x', s=100, label='Centroids')
plt.xlabel("Bill Length (mm)")
plt.ylabel("Flipper Length (mm)")
plt.title("Clusters Found by Custom K-Means")
plt.legend()
plt.show()
```

We use the `seaborn` penguins dataset and select two continuous variables: **bill length** and **flipper length**.

This makes it easy to visualize clustering in 2D.

We choose `k=3` and run our K-Means function.

Each cluster is color-coded, and the final centroids are marked with black X’s.

```{python}
#| code-fold: true
#| code-summary: "Show code"
from sklearn.cluster import KMeans

kmeans_sklearn = KMeans(n_clusters=3, random_state=42).fit(X)
labels_sklearn = kmeans_sklearn.labels_
centroids_sklearn = kmeans_sklearn.cluster_centers_

plt.figure(figsize=(6, 5))
for i in range(3):
    plt.scatter(X[labels_sklearn == i, 0], X[labels_sklearn == i, 1], label=f'Cluster {i}')
plt.scatter(centroids_sklearn[:, 0], centroids_sklearn[:, 1], color='red', marker='x', s=100, label='Centroids')
plt.title("Clusters from sklearn KMeans")
plt.xlabel("Bill Length (mm)")
plt.ylabel("Flipper Length (mm)")
plt.legend()
plt.show()

```

Here’s a comparison using `sklearn`’s `KMeans`. The cluster separation and centroid locations are nearly identical, validating our implementation.

```{python}
#| code-fold: true
#| code-summary: "Show code"
from sklearn.metrics import silhouette_score

wcss = []
silhouettes = []
K_range = range(2, 8)

for k in K_range:
    km = KMeans(n_clusters=k, random_state=42).fit(X)
    wcss.append(km.inertia_)
    silhouettes.append(silhouette_score(X, km.labels_))

plt.figure()
plt.plot(K_range, wcss, marker='o')
plt.title("Elbow Plot (WCSS)")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Within-Cluster Sum of Squares")
plt.show()

plt.figure()
plt.plot(K_range, silhouettes, marker='s')
plt.title("Silhouette Score vs K")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Silhouette Score")
plt.show()
```

We calculate:
- **Within-Cluster Sum of Squares (WCSS)**: Lower is better; the “elbow” suggests the optimal `k`.
- **Silhouette Score**: Measures cohesion and separation; higher is better.

In this case, both metrics suggest `k=3` is a strong choice — consistent with our earlier result.

I implemented K-Means from scratch and applied it to the Palmer Penguins dataset. I also compared my implementation to the built-in version from `sklearn`, and evaluated the results using WCSS and silhouette scores.

The optimal number of clusters based on both metrics appears to be **3**, which aligns well with the natural grouping of the data.

This hands-on implementation helped solidify my understanding of clustering mechanics — and provided great visuals for exploring the algorithm step-by-step.


## Key Drivers Analysis: What Influences Satisfaction

_todo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, "usefulness", Shapley values for a linear regression, Johnson's relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations "by hand."_

_If you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables._


In this section, I replicate a key drivers analysis from class to explore which perceptual attributes most strongly explain **satisfaction** for **Brand 1**.

The dataset includes survey responses where each row represents a customer and the columns include:
- A set of **perception variables** (e.g., trust, ease of use, appeal)
- A **satisfaction score** (the target variable we aim to explain)

I perform a **Key Drivers Analysis** using six different variable importance metrics, each of which offers a unique lens for interpreting what matters most:
1. **Pearson Correlation**
2. **Standardized Regression Coefficients**
3. **Usefulness** (R² from univariate regressions)
4. **Shapley Values (LMG)**
5. **Johnson's Relative Weights**
6. **Random Forest Gini Importance**

This helps compare **statistical**, **machine learning**, and **variance decomposition** approaches to identifying the strongest drivers of customer satisfaction.

For simplicity and clarity, I focus on **Brand 1** only.

Let's load and filter the data.

```{python}
#| code-fold: true
#| code-summary: "Load and filter Brand 1 data"
import pandas as pd

# Load the dataset
df = pd.read_csv("data_for_drivers_analysis.csv")

# Filter to just Brand 1
df_brand1 = df[df["brand"] == 1].copy()

# Define predictors and target
X = df_brand1.drop(columns=["brand", "id", "satisfaction"])
y = df_brand1["satisfaction"]

```

### Pearson Correlation

The Pearson correlation measures the **linear relationship** between each perception variable and satisfaction. It ranges from -1 to +1:
- A positive value means the perception increases as satisfaction increases.
- A larger absolute value means a stronger relationship.

This is a simple and intuitive measure of importance, though it doesn't account for interactions or multicollinearity.

```{python}
#| code-fold: true
#| code-summary: "Compute Pearson correlations"
import numpy as np

# Compute correlations between each column in X and y
pearson_corr = X.corrwith(y)

# Convert to DataFrame for later merging
pearson_df = pearson_corr.to_frame(name="Pearson Correlation")
pearson_df = pearson_df.sort_values(by="Pearson Correlation", ascending=False)
pearson_df

```

### Standardized Regression Coefficients

This method involves fitting a **linear regression** on standardized variables (z-scores). Standardizing ensures that all predictors are on the same scale, so we can directly compare the magnitude of their coefficients.

Larger coefficients (in absolute value) indicate variables that contribute more to predicting satisfaction. This approach adjusts for **collinearity** better than Pearson correlation, but it still assumes a linear and additive relationship.

Below are the standardized coefficients for each perception variable:

```{python}
#| code-fold: true
#| code-summary: "Standardized regression coefficients"
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

# Standardize X and y
scaler_X = StandardScaler()
scaler_y = StandardScaler()
X_std = scaler_X.fit_transform(X)
y_std = scaler_y.fit_transform(y.values.reshape(-1, 1)).ravel()

# Fit linear regression
lr = LinearRegression()
lr.fit(X_std, y_std)

# Store coefficients
std_coef_df = pd.DataFrame({
    "Standardized Coefficient": lr.coef_
}, index=X.columns).sort_values(by="Standardized Coefficient", ascending=False)

std_coef_df
```

### Usefulness: Univariate R²

"Usefulness" is measured by the **R² value** from a simple linear regression using each predictor individually. It tells us how much of the variance in satisfaction can be explained by that predictor alone — without adjusting for overlap with other variables.

This metric is easy to interpret and useful for identifying high-signal variables, though it can be misleading if predictors are strongly correlated with each other.

Here are the results for Brand 1:

```{python}
#| code-fold: true
#| code-summary: "Univariate R² (Usefulness)"
from sklearn.linear_model import LinearRegression

r2_scores = {}

for col in X.columns:
    model = LinearRegression()
    model.fit(X[[col]], y)
    r2_scores[col] = model.score(X[[col]], y)

usefulness_df = pd.DataFrame.from_dict(r2_scores, orient="index", columns=["Usefulness (R²)"])
usefulness_df = usefulness_df.sort_values(by="Usefulness (R²)", ascending=False)

usefulness_df

```

### Summary of the First Three Metrics

So far, we’ve explored three statistical perspectives on what drives satisfaction:

- **Pearson Correlation** gives us a simple view of linear relationships.
- **Standardized Coefficients** control for overlap between variables in a multivariate setting.
- **Usefulness (R²)** shows how much variance each variable explains on its own.

Despite their different methodologies, the same few variables consistently emerge as top drivers:
- `appealing` and `rewarding` show up at the top in all three.
- `impact`, `easy`, and `trust` are also moderately strong across metrics.
- `build` scores low across the board — potentially indicating that customers don’t associate this dimension with satisfaction.

This early convergence increases confidence in our results — though we'll validate further with machine learning and variance decomposition techniques next.

### Shapley Values (LMG Decomposition in R)

To correctly run Shapley value (LMG) decomposition for a linear regression model, I used R’s `relaimpo` package. This avoids the misbehavior I encountered using Python’s `dominance-analysis` package, which defaults to logistic regression.

The table below shows the relative importance (as a % of R²) each predictor contributes based on averaging over all possible model combinations.

```{r}
#| label: "lmg-decomposition"
#| warning: false
#| message: false

# Load the library
library(relaimpo)

# Load the CSV using correct relative path
df <- read.csv("/home/jovyan/mgta495/mysite/projects/hw4/data_for_drivers_analysis.csv")

# Filter Brand 1
df1 <- subset(df, brand == 1)

# Fit model
model <- lm(satisfaction ~ trust + build + differs + easy + appealing +
              rewarding + popular + service + impact, data = df1)

# Run Shapley / LMG decomposition
result <- calc.relimp(model, type = "lmg", rela = TRUE)

# Show results
as.data.frame(result$lmg)

```

To accurately compute Shapley values (LMG decomposition) for a linear model, I used the `relaimpo` package in R. This method attributes each predictor’s average contribution to the model's R² by considering all possible combinations of variable orderings.

Here are the results for Brand 1:

| Variable   | LMG (Relative Importance) |
|------------|----------------------------|
| **appealing**  | 0.226 |
| **rewarding**  | 0.214 |
| **impact**     | 0.116 |
| **easy**       | 0.099 |
| **trust**      | 0.084 |
| **popular**    | 0.089 |
| **build**      | 0.079 |
| **service**    | 0.065 |
| **differs**    | 0.028 |

Together, these add up to 1.0 (100% of model R²), showing how each perception variable contributes to explaining satisfaction.

As with earlier metrics, `appealing` and `rewarding` again stand out as the most important drivers — further validated by this comprehensive decomposition.

### Johnson’s Relative Weights

Johnson’s Relative Weights decompose the model’s R² into contributions from each predictor by first transforming them into **orthogonal variables** (uncorrelated components), then mapping those contributions back.

This approach is **computationally efficient** and handles multicollinearity well — while still providing results on the same scale as R².

```{python}
#| code-fold: true
#| code-summary: "Compute Johnson's Relative Weights"
import numpy as np
from sklearn.linear_model import LinearRegression

# Standardize predictors
X_std = (X - X.mean()) / X.std()

# Step 1: Correlation matrix and eigen decomposition
R = np.corrcoef(X_std.T)
eigvals, eigvecs = np.linalg.eigh(R)

# Step 2: Project predictors onto eigenvectors
Lambda = eigvecs @ np.diag(np.sqrt(eigvals)) @ eigvecs.T
Z = X_std @ Lambda  # uncorrelated predictors

# Step 3: Regress on transformed Z
model = LinearRegression().fit(Z, y)
beta = model.coef_

# Step 4: Map weights back to original variables
raw_weights = (Lambda @ beta) ** 2
rel_weights = raw_weights / np.sum(raw_weights)

# Format results
johnson_df = pd.DataFrame({
    "Johnson's Relative Weight": rel_weights
}, index=X.columns).sort_values(by="Johnson's Relative Weight", ascending=False)

johnson_df

```
