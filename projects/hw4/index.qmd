---
title: "Machine Learning"
author: "Brian Pintar"
date: today
---

_todo: do two analyses.  Do one of either 1a or 1b, AND one of either 2a or 2b._


## 1a. K-Means

_todo: write your own code to implement the k-means algorithm.  Make plots of the various steps the algorithm takes so you can "see" the algorithm working.  Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables.  Compare your results to the built-in `kmeans` function in R or Python._

_todo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,...,7). What is the "right" number of clusters as suggested by these two metrics?_

_If you want a challenge, add your plots as an animated gif on your website so that the result looks something like [this](https://www.youtube.com/shorts/XCsoWZU9oN8)._

I implement the K-Means clustering algorithm **from scratch** and apply it to the Palmer Penguins dataset.

The goal is to:
1. Write the K-Means algorithm by hand in Python.
2. Visualize how it works on real data.
3. Compare it with Python’s built-in `KMeans` implementation.
4. Evaluate model performance using **within-cluster sum of squares (WCSS)** and **silhouette scores**.
5. Determine the “right” number of clusters.

We'll use two features: **bill length** and **flipper length** — a natural fit for 2D visualization of clustering in action.

K-Means is an **unsupervised machine learning** algorithm that partitions data into `K` clusters, where each data point belongs to the cluster with the nearest mean.

The algorithm follows these steps:
1. Randomly initialize `K` centroids.
2. Assign each data point to the closest centroid (based on Euclidean distance).
3. Recompute centroids as the mean of points assigned to each cluster.
4. Repeat steps 2–3 until convergence (centroids stop moving or max iterations is reached).

It’s a classic example of an **iterative optimization algorithm** that seeks to minimize the total **within-cluster sum of squares** (WCSS).

```{python}
#| code-fold: true
#| code-summary: "Show code"
import numpy as np
import matplotlib.pyplot as plt

def initialize_centroids(X, k):
    """Randomly initialize k centroids from the data."""
    indices = np.random.choice(len(X), size=k, replace=False)
    return X[indices]

def assign_clusters(X, centroids):
    """Assign each point to the nearest centroid."""
    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
    return np.argmin(distances, axis=1)

def update_centroids(X, labels, k):
    """Recompute centroids as the mean of all points in each cluster."""
    return np.array([X[labels == i].mean(axis=0) for i in range(k)])

def kmeans(X, k, max_iters=100, tol=1e-4):
    """Full K-Means clustering algorithm."""
    centroids = initialize_centroids(X, k)
    for i in range(max_iters):
        old_centroids = centroids
        labels = assign_clusters(X, centroids)
        centroids = update_centroids(X, labels, k)
        if np.allclose(centroids, old_centroids, atol=tol):
            break
    return centroids, labels
```

We define functions to:
- Initialize centroids randomly
- Assign each point to the nearest centroid
- Update centroids using the mean of assigned points
- Iterate until convergence

Now let’s test it on the **Palmer Penguins** dataset.

```{python}
#| code-fold: true
#| code-summary: "Show code"
import seaborn as sns
import pandas as pd

# Load the penguins dataset
penguins = sns.load_dataset("penguins").dropna(subset=["bill_length_mm", "flipper_length_mm"])
X = penguins[["bill_length_mm", "flipper_length_mm"]].values

# Visualize raw data
plt.figure(figsize=(6, 5))
plt.scatter(X[:, 0], X[:, 1], c='gray', edgecolor='black')
plt.xlabel("Bill Length (mm)")
plt.ylabel("Flipper Length (mm)")
plt.title("Palmer Penguins: Bill Length vs Flipper Length")
plt.show()
```

We use the `seaborn` penguins dataset and select two continuous variables: **bill length** and **flipper length**.

This makes it easy to visualize clustering in 2D.

```{python}
#| code-fold: true
#| code-summary: "Show code"

# Set number of clusters
k = 3

# Run custom K-Means
centroids, labels = kmeans(X, k)

# Plot clusters
plt.figure(figsize=(6, 5))
for i in range(k):
    cluster_points = X[labels == i]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i}')
plt.scatter(centroids[:, 0], centroids[:, 1], color='black', marker='x', s=100, label='Centroids')
plt.xlabel("Bill Length (mm)")
plt.ylabel("Flipper Length (mm)")
plt.title("Clusters Found by Custom K-Means")
plt.legend()
plt.show()

```

We choose `k=3` and run our K-Means function.

Each cluster is color-coded, and the final centroids are marked with black X’s.

```{python}
#| code-fold: true
#| code-summary: "Show code"

from sklearn.cluster import KMeans

kmeans_sklearn = KMeans(n_clusters=3, random_state=42).fit(X)
labels_sklearn = kmeans_sklearn.labels_
centroids_sklearn = kmeans_sklearn.cluster_centers_

# Plot built-in results
plt.figure(figsize=(6, 5))
for i in range(3):
    plt.scatter(X[labels_sklearn == i, 0], X[labels_sklearn == i, 1], label=f'Cluster {i}')
plt.scatter(centroids_sklearn[:, 0], centroids_sklearn[:, 1], color='red', marker='x', s=100, label='Centroids')
plt.title("Clusters from sklearn KMeans")
plt.xlabel("Bill Length (mm)")
plt.ylabel("Flipper Length (mm)")
plt.legend()
plt.show()

```

Here’s a comparison using `sklearn`’s `KMeans`. The cluster separation and centroid locations are nearly identical, validating our implementation.

```{python}
#| code-fold: true
#| code-summary: "Show code"

from sklearn.metrics import silhouette_score

wcss = []
silhouettes = []

K_range = range(2, 8)
for k in K_range:
    km = KMeans(n_clusters=k, random_state=42).fit(X)
    wcss.append(km.inertia_)
    silhouettes.append(silhouette_score(X, km.labels_))

# Plot WCSS (Elbow)
plt.figure()
plt.plot(K_range, wcss, marker='o')
plt.title("Elbow Plot (WCSS)")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Within-Cluster Sum of Squares")
plt.show()

# Plot Silhouette Scores
plt.figure()
plt.plot(K_range, silhouettes, marker='s')
plt.title("Silhouette Score vs K")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Silhouette Score")
plt.show()


```

We calculate:
- **Within-Cluster Sum of Squares (WCSS)**: Lower is better; the “elbow” suggests the optimal `k`.
- **Silhouette Score**: Measures cohesion and separation; higher is better.

In this case, both metrics suggest `k=3` is a strong choice — consistent with our earlier result.

I implemented K-Means from scratch and applied it to the Palmer Penguins dataset. I also compared my implementation to the built-in version from `sklearn`, and evaluated the results using WCSS and silhouette scores.

The optimal number of clusters based on both metrics appears to be **3**, which aligns well with the natural grouping of the data.

This hands-on implementation helped solidify my understanding of clustering mechanics — and provided great visuals for exploring the algorithm step-by-step.


## 2b. Key Drivers Analysis

_todo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, "usefulness", Shapley values for a linear regression, Johnson's relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations "by hand."_

_If you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables._






