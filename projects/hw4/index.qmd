---
title: "Machine Learning"
author: "Brian Pintar"
date: today
---


## K-Means Clustering: Implemented from Scratch

In this post, I implement the K-Means clustering algorithm **from scratch** and apply it to the Palmer Penguins dataset.

The goal is to:
1. Write the K-Means algorithm by hand in Python.
2. Visualize how it works on real data.
3. Compare it with Python’s built-in `KMeans` implementation.
4. Evaluate model performance using **within-cluster sum of squares (WCSS)** and **silhouette scores**.
5. Determine the “right” number of clusters.

We’ll use two features: **bill length** and **flipper length** — a natural fit for 2D visualization of clustering in action.

---

How the Algorithm Works:

K-Means is an **unsupervised learning** algorithm that partitions data into `K` clusters. The algorithm seeks to minimize the **within-cluster sum of squares (WCSS)** and follows these steps:

1. Randomly initialize `K` centroids.
2. Assign each data point to the nearest centroid.
3. Recompute each centroid based on current cluster assignments.
4. Repeat until centroids converge or max iterations is reached.

---

Step 1: Implementing K-Means in Python:


```{python}
#| code-fold: true
#| code-summary: "Show code"
import numpy as np
import matplotlib.pyplot as plt

def initialize_centroids(X, k):
    indices = np.random.choice(len(X), size=k, replace=False)
    return X[indices]

def assign_clusters(X, centroids):
    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
    return np.argmin(distances, axis=1)

def update_centroids(X, labels, k):
    return np.array([X[labels == i].mean(axis=0) for i in range(k)])

def kmeans(X, k, max_iters=100, tol=1e-4):
    centroids = initialize_centroids(X, k)
    for i in range(max_iters):
        old_centroids = centroids
        labels = assign_clusters(X, centroids)
        centroids = update_centroids(X, labels, k)
        if np.allclose(centroids, old_centroids, atol=tol):
            break
    return centroids, labels
```

We define functions to:
- Initialize centroids randomly
- Assign each point to the nearest centroid
- Update centroids using the mean of assigned points
- Iterate until convergence

Now let’s test it on the **Palmer Penguins** dataset.

```{python}
#| code-fold: true
#| code-summary: "Show code"

#| code-fold: true
#| code-summary: "Show code"
import seaborn as sns
import pandas as pd

penguins = sns.load_dataset("penguins").dropna(subset=["bill_length_mm", "flipper_length_mm"])
X = penguins[["bill_length_mm", "flipper_length_mm"]].values

plt.figure(figsize=(6, 5))
plt.scatter(X[:, 0], X[:, 1], c='gray', edgecolor='black')
plt.xlabel("Bill Length (mm)")
plt.ylabel("Flipper Length (mm)")
plt.title("Palmer Penguins: Bill Length vs Flipper Length")
plt.show()

```

To better understand how K-Means converges, I modified the algorithm to store the centroids and cluster labels at each iteration.

Executing such visualization here:

```{python}
#| code-fold: true
#| code-summary: "Show code"
def kmeans_with_plot(X, k, max_iters=10):
    centroids = initialize_centroids(X, k)
    history = []
    for i in range(max_iters):
        labels = assign_clusters(X, centroids)
        history.append((centroids.copy(), labels.copy()))
        new_centroids = update_centroids(X, labels, k)
        if np.allclose(new_centroids, centroids):
            break
        centroids = new_centroids
    history.append((centroids.copy(), labels.copy()))
    return history

```

Each frame above shows the K-Means algorithm as it:
- Reassigns data points to the nearest centroid
- Moves centroids to the mean of their assigned points
- Gradually stabilizes and stops updating

This visualization helps demonstrate how K-Means converges over a few iterations. It's especially helpful for building intuition around how cluster boundaries emerge.

```{python}
#| code-fold: true
#| code-summary: "Show code"
k = 3
centroids, labels = kmeans(X, k)

plt.figure(figsize=(6, 5))
for i in range(k):
    cluster_points = X[labels == i]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i}')
plt.scatter(centroids[:, 0], centroids[:, 1], color='black', marker='x', s=100, label='Centroids')
plt.xlabel("Bill Length (mm)")
plt.ylabel("Flipper Length (mm)")
plt.title("Clusters Found by Custom K-Means")
plt.legend()
plt.show()
```

We use the `seaborn` penguins dataset and select two continuous variables: **bill length** and **flipper length**.

This makes it easy to visualize clustering in 2D.

We choose `k=3` and run our K-Means function.

Each cluster is color-coded, and the final centroids are marked with black X’s.

```{python}
#| code-fold: true
#| code-summary: "Show code"
from sklearn.cluster import KMeans

kmeans_sklearn = KMeans(n_clusters=3, random_state=42).fit(X)
labels_sklearn = kmeans_sklearn.labels_
centroids_sklearn = kmeans_sklearn.cluster_centers_

plt.figure(figsize=(6, 5))
for i in range(3):
    plt.scatter(X[labels_sklearn == i, 0], X[labels_sklearn == i, 1], label=f'Cluster {i}')
plt.scatter(centroids_sklearn[:, 0], centroids_sklearn[:, 1], color='red', marker='x', s=100, label='Centroids')
plt.title("Clusters from sklearn KMeans")
plt.xlabel("Bill Length (mm)")
plt.ylabel("Flipper Length (mm)")
plt.legend()
plt.show()

```

Here’s a comparison using `sklearn`’s `KMeans`. The cluster separation and centroid locations are nearly identical, validating our implementation.

```{python}
#| code-fold: true
#| code-summary: "Show code"
from sklearn.metrics import silhouette_score

wcss = []
silhouettes = []
K_range = range(2, 8)

for k in K_range:
    km = KMeans(n_clusters=k, random_state=42).fit(X)
    wcss.append(km.inertia_)
    silhouettes.append(silhouette_score(X, km.labels_))

plt.figure()
plt.plot(K_range, wcss, marker='o')
plt.title("Elbow Plot (WCSS)")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Within-Cluster Sum of Squares")
plt.show()

plt.figure()
plt.plot(K_range, silhouettes, marker='s')
plt.title("Silhouette Score vs K")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Silhouette Score")
plt.show()
```

We calculate:
- **Within-Cluster Sum of Squares (WCSS)**: Lower is better; the “elbow” suggests the optimal `k`.
- **Silhouette Score**: Measures cohesion and separation; higher is better.

In this case, both metrics suggest `k=3` is a strong choice — consistent with our earlier result.

I implemented K-Means from scratch and applied it to the Palmer Penguins dataset. I also compared my implementation to the built-in version from `sklearn`, and evaluated the results using WCSS and silhouette scores.

The optimal number of clusters based on both metrics appears to be **3**, which aligns well with the natural grouping of the data.

This hands-on implementation helped solidify my understanding of clustering mechanics — and provided great visuals for exploring the algorithm step-by-step.


## 2b. Key Drivers Analysis

_todo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, "usefulness", Shapley values for a linear regression, Johnson's relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations "by hand."_

_If you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables._






