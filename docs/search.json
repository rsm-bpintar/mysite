[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Brian‚Äôs Resume",
    "section": "",
    "text": "Last updated 8/24/24\nDownload PDF file."
  },
  {
    "objectID": "projects/hw1/index.html",
    "href": "projects/hw1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nPublished in 2007, this experiment was inspired by America‚Äôs significant increase in private charitable giving in the decades prior. It was alluded to the fact that a combination of increased wealth and an aging population in America likely caused this increase. Their analysis originally concluded that the match offer increases both the revenue per solicitation and the response rate. However, larger match ratios (i.e., $3:$1 instead of $1:$1) did not have any additional impact. It was also concluded that the matching had a much larger effect in red states than blue states.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/hw1/index.html#introduction",
    "href": "projects/hw1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nPublished in 2007, this experiment was inspired by America‚Äôs significant increase in private charitable giving in the decades prior. It was alluded to the fact that a combination of increased wealth and an aging population in America likely caused this increase. Their analysis originally concluded that the match offer increases both the revenue per solicitation and the response rate. However, larger match ratios (i.e., $3:$1 instead of $1:$1) did not have any additional impact. It was also concluded that the matching had a much larger effect in red states than blue states.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/hw1/index.html#data",
    "href": "projects/hw1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\ndf = pd.read_stata('karlan_list_2007.dta')\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows √ó 51 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nimport numpy as np\nfrom scipy.stats import t\n\ndef t_test_randomization_check(df, column, treat_col='treatment', control_col='control'):\n    \"\"\"\n    Performs Welch's t-test to compare treatment and control groups for a given variable.\n\n    Parameters:\n    - df: pandas DataFrame\n    - column: name of the column to compare (e.g., 'mrm2')\n    - treat_col: name of the treatment indicator column (default: 'treatment')\n    - control_col: name of the control indicator column (default: 'control')\n    \"\"\"\n\n    # Extract the groups\n    treat = df[df[treat_col] == 1][column].dropna()\n    control = df[df[control_col] == 1][column].dropna()\n\n    # Means and standard deviations\n    mean_treat = treat.mean()\n    mean_control = control.mean()\n    std_treat = treat.std(ddof=1)\n    std_control = control.std(ddof=1)\n\n    # Sample sizes\n    n_treat = len(treat)\n    n_control = len(control)\n\n    # t-statistic\n    numerator = mean_treat - mean_control\n    denominator = np.sqrt((std_treat**2)/n_treat + (std_control**2)/n_control)\n    t_stat = numerator / denominator\n\n    # Degrees of freedom (Welch-Satterthwaite)\n    var_treat = std_treat**2 / n_treat\n    var_control = std_control**2 / n_control\n    df_numerator = (var_treat + var_control)**2\n    df_denominator = (var_treat**2 / (n_treat - 1)) + (var_control**2 / (n_control - 1))\n    df_welch = df_numerator / df_denominator\n\n    # p-value\n    p_value = 2 * (1 - t.cdf(np.abs(t_stat), df=df_welch))\n\n    # Output\n    print(f\"\\nüß™ T-Test for '{column}': Treatment vs Control\")\n    print(f\"Treatment mean: {mean_treat:.2f}; Control mean: {mean_control:.2f}\")\n    print(f\"Treatment std dev: {std_treat:.2f}; Control std dev: {std_control:.2f}\")\n    print(f\"t-statistic: {t_stat:.4f}\")\n    print(f\"Degrees of freedom: {df_welch:.2f}\")\n    print(f\"p-value: {p_value:.4f}\")\n\n    # Decision\n    if p_value &lt; 0.05:\n        print(\"‚ùó Reject the null hypothesis: The groups are significantly different.\")\n    else:\n        print(\"‚úÖ Fail to reject the null hypothesis: No significant difference between groups.\")\n\n\nt_test_randomization_check(df, 'mrm2')\nt_test_randomization_check(df, 'female')\nt_test_randomization_check(df, 'red0')\n\n\nüß™ T-Test for 'mrm2': Treatment vs Control\nTreatment mean: 13.01; Control mean: 13.00\nTreatment std dev: 12.09; Control std dev: 12.07\nt-statistic: 0.1195\nDegrees of freedom: 33394.48\np-value: 0.9049\n‚úÖ Fail to reject the null hypothesis: No significant difference between groups.\n\nüß™ T-Test for 'female': Treatment vs Control\nTreatment mean: 0.28; Control mean: 0.28\nTreatment std dev: 0.45; Control std dev: 0.45\nt-statistic: -1.7535\nDegrees of freedom: 32450.81\np-value: 0.0795\n‚úÖ Fail to reject the null hypothesis: No significant difference between groups.\n\nüß™ T-Test for 'red0': Treatment vs Control\nTreatment mean: 0.41; Control mean: 0.40\nTreatment std dev: 0.49; Control std dev: 0.49\nt-statistic: 1.8773\nDegrees of freedom: 33450.52\np-value: 0.0605\n‚úÖ Fail to reject the null hypothesis: No significant difference between groups.\n\n\nThe above results confirm randomization of the experiment. There was no significant difference in the treatment and control groups regarding the number of months since last donation, the percentage of male/female participants, or participants in red/blue states, which could have affected the results. Let‚Äôs compare results with a linear regression for the months since last donation variable ‚Äòmrm2‚Äô to confirm our approach.\n\nimport statsmodels.api as sm\n\ndf_reg = df[['mrm2', 'treatment']].dropna()\n\n#define x and y\nX = df_reg['treatment']\nX = sm.add_constant(X)\ny = df_reg['mrm2']\n\n#fit model\nmodel = sm.OLS(y,X).fit()\n\n#extract values\ncoef = model.params['treatment']\nt_stat = model.tvalues['treatment']\np_value = model.pvalues['treatment']\n\nprint(f\"Coefficient (treatment): {coef:.4f}\")\nprint(f\"t-statistic: {t_stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\nCoefficient (treatment): 0.0137\nt-statistic: 0.1195\np-value: 0.9049\n\n\nThe t-statistics and p-values match accordingly, confirming the method.\nTable 1 is included in the original report to provide evidence of randomization of the treatment and control groups. This is proven to be true, being that we fail to reject the null hypothesis with no statistically significant difference in demographics for treatment and control groups."
  },
  {
    "objectID": "projects/hw1/index.html#experimental-results",
    "href": "projects/hw1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n\n#calculate proportions\nprop_treatment = df[df['treatment'] == 1]['gave'].mean()\nprop_control = df[df['control'] == 1]['gave'].mean()\n\n#data for plot\ngroups = ['Treatment', 'Control']\nproportions = [prop_treatment, prop_control]\n\n#plot\nplt.figure(figsize=(6,4))\nplt.bar(groups, proportions)\nplt.ylabel('Proportion Donated')\nplt.title('Donation Rate by Group')\nplt.ylim(0,.05)\nplt.show()\n\n\n\n\n\n\n\n\nBased on the bar chart above, it appears that the treatment had a slight effect on donation rates.\n\ndf_lpm = df[['gave', 'treatment']].dropna()\n\n#define outcome and predictor\nX = sm.add_constant(df_lpm['treatment'])\ny = df_lpm['gave']\n\n#fit linear regression\nmodel = sm.OLS(y, X).fit()\n\n#extract stats\ncoef = model.params['treatment']\nt_stat = model.tvalues['treatment']\np_value = model.pvalues['treatment']\n\n#output\nprint(f\"Coefficient (treatment effect): {coef:.4f}\")\nprint(f\"t-statistic: {t_stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\nCoefficient (treatment effect): 0.0042\nt-statistic: 3.1014\np-value: 0.0019\n\n\nWhile the effect is small, it is statistically significant based on the p-value of less than .05. This implies that people in general provided donations more when told that donations would be matched.\n\ndf_probit = df[['gave', 'treatment']].dropna()\n\n#define predictors and outcome\nX = sm.add_constant(df_probit['treatment'])\ny = df_probit['gave']\n\n#fit probit model\nprobit_model = sm.Probit(y, X).fit()\n\nmfx = probit_model.get_margeff()\nprint(mfx.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\nThe probit regression indicates an increase in probability of donating by 0.43%, holding all else constant, when given the treatment (matching). This is consistent with the linear regression results, as effect is small. P-values also match, and are statistically significant. Treatment has a statistically significant but modest positive effect on probability of donation.\nNOTE: Linear regression results appear replicate Table 3 column 1 in the paper. Probit results do not, despite Table 3 indicating its results come from probit regressions‚Ä¶\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\ndef t_test_donation_rate(df, ratio_a, ratio_b, outcome_col='gave', ratio_col='ratio'):\n    \"\"\"\n    Performs and prints a Welch's t-test comparing donation rates between two match ratio groups.\n    \n    Parameters:\n    - df: pandas DataFrame\n    - ratio_a: first ratio value (e.g., 1, 2, 3, or \"Control\")\n    - ratio_b: second ratio value\n    - outcome_col: name of the binary outcome column (default: 'gave')\n    - ratio_col: name of the match ratio column (default: 'ratio')\n    \n    Returns:\n    - Dictionary of test results\n    \"\"\"\n    \n    group_a = df[df[ratio_col] == ratio_a][outcome_col].dropna()\n    group_b = df[df[ratio_col] == ratio_b][outcome_col].dropna()\n\n    n1, n2 = len(group_a), len(group_b)\n    p1, p2 = group_a.mean(), group_b.mean()\n\n    # Standard error\n    se = np.sqrt((p1 * (1 - p1)) / n1 + (p2 * (1 - p2)) / n2)\n\n    # t-statistic\n    t_stat = (p2 - p1) / se\n\n    # Degrees of freedom\n    var1 = (p1 * (1 - p1)) / n1\n    var2 = (p2 * (1 - p2)) / n2\n    df_welch = (var1 + var2)**2 / ((var1**2)/(n1 - 1) + (var2**2)/(n2 - 1))\n\n    # Two-tailed p-value\n    p_value = 2 * (1 - t.cdf(np.abs(t_stat), df=df_welch))\n\n    # Print results\n    print(f\"\\nüéØ T-Test: {ratio_b}:1 vs {ratio_a}:1 Match Ratio\")\n    print(\"-\" * 40)\n    print(f\"Sample size ({ratio_a}:1): {n1}\")\n    print(f\"Sample size ({ratio_b}:1): {n2}\")\n    print(f\"Donation rate ({ratio_a}:1): {p1:.4f}\")\n    print(f\"Donation rate ({ratio_b}:1): {p2:.4f}\")\n    print(f\"t-statistic: {t_stat:.4f}\")\n    print(f\"Degrees of freedom: {df_welch:.2f}\")\n    print(f\"p-value: {p_value:.4f}\")\n    \n    if p_value &lt; 0.05:\n        print(\"‚úÖ Statistically significant at the 5% level.\")\n    else:\n        print(\"‚ùå Not statistically significant at the 5% level.\")\n\nt_test_donation_rate(df, 1, 2)  # 1:1 vs 2:1\nt_test_donation_rate(df, 2, 3)\nt_test_donation_rate(df, 1, 3)\n\n\nüéØ T-Test: 2:1 vs 1:1 Match Ratio\n----------------------------------------\nSample size (1:1): 11133\nSample size (2:1): 11134\nDonation rate (1:1): 0.0207\nDonation rate (2:1): 0.0226\nt-statistic: 0.9651\nDegrees of freedom: 22225.08\np-value: 0.3345\n‚ùå Not statistically significant at the 5% level.\n\nüéØ T-Test: 3:1 vs 2:1 Match Ratio\n----------------------------------------\nSample size (2:1): 11134\nSample size (3:1): 11129\nDonation rate (2:1): 0.0226\nDonation rate (3:1): 0.0227\nt-statistic: 0.0501\nDegrees of freedom: 22260.85\np-value: 0.9600\n‚ùå Not statistically significant at the 5% level.\n\nüéØ T-Test: 3:1 vs 1:1 Match Ratio\n----------------------------------------\nSample size (1:1): 11133\nSample size (3:1): 11129\nDonation rate (1:1): 0.0207\nDonation rate (3:1): 0.0227\nt-statistic: 1.0151\nDegrees of freedom: 22215.05\np-value: 0.3101\n‚ùå Not statistically significant at the 5% level.\n\n\nAuthor suggests that neither the different match thresholds or example amount had a meaningful influence on behavior. Results above suggest the same. Not enough evidence to conclude that the difference in donation rates is statistically significant for different treatment options.\nCreate the ratio1 variable.\n\n#create the ratio1 variable\ndf['ratio_str'] = df['ratio'].astype('str')\ndf['ratio1'] = (df['ratio_str'] == '1').astype(int)\ndf['ratio2'] = (df['ratio_str'] == '2').astype(int)\ndf['ratio3'] = (df['ratio_str'] == '3').astype(int)\nprint(df[['ratio', 'ratio1', 'ratio2', 'ratio3']].head(10))\n\n     ratio  ratio1  ratio2  ratio3\n0  Control       0       0       0\n1  Control       0       0       0\n2        1       1       0       0\n3        1       1       0       0\n4        1       1       0       0\n5  Control       0       0       0\n6        1       1       0       0\n7        2       0       1       0\n8        2       0       1       0\n9        1       1       0       0\n\n\nRegression results are presented below.\n\ndef compare_match_ratios(df, ratio_a, ratio_b, ratio_col='ratio', outcome_col='gave'):\n    \"\"\"\n    Compare two match ratios using linear regression on a binary outcome.\n\n    Parameters:\n    - df: DataFrame with data\n    - ratio_a: first match ratio to compare (e.g., 1 for 1:1)\n    - ratio_b: second match ratio to compare (e.g., 2 for 2:1)\n    - ratio_col: name of the column containing match ratios\n    - outcome_col: name of the binary outcome column\n\n    Returns:\n    - Dictionary with coefficient and p-value\n    \"\"\"\n\n    # Filter to only the two groups being compared\n    df_sub = df[df[ratio_col].isin([ratio_a, ratio_b])].copy()\n\n    # Create indicator for being in ratio_b group\n    df_sub['is_ratio_b'] = (df_sub[ratio_col] == ratio_b).astype(int)\n\n    # Run regression\n    X = sm.add_constant(df_sub['is_ratio_b'])\n    y = df_sub[outcome_col]\n\n    model = sm.OLS(y, X).fit()\n\n    coef = model.params['is_ratio_b']\n    p_value = model.pvalues['is_ratio_b']\n\n    print(f\"\\nüìä Comparing donation rates: {ratio_b}:1 vs {ratio_a}:1\")\n    print(f\"Coefficient (diff in donation rate): {coef:.4f}\")\n    print(f\"p-value: {p_value:.4f}\")\n\ncompare_match_ratios(df, 1, 2)  # Compare 2:1 vs 1:1\ncompare_match_ratios(df, 2, 3)\ncompare_match_ratios(df, 1, 3) \n\n\nüìä Comparing donation rates: 2:1 vs 1:1\nCoefficient (diff in donation rate): 0.0019\np-value: 0.3345\n\nüìä Comparing donation rates: 3:1 vs 2:1\nCoefficient (diff in donation rate): 0.0001\np-value: 0.9600\n\nüìä Comparing donation rates: 3:1 vs 1:1\nCoefficient (diff in donation rate): 0.0020\np-value: 0.3101\n\n\nThe regression results above indicate the same. The differences between donation rates of the different match ratios are not statistically significant, and therefore do not indicate any effect.\n\n# Response (donation) rates\np_1 = df[df['ratio'] == 1]['gave'].mean()\np_2 = df[df['ratio'] == 2]['gave'].mean()\np_3 = df[df['ratio'] == 3]['gave'].mean()\n\n# Differences in raw proportions\ndiff_2v1 = p_2 - p_1\ndiff_3v2 = p_3 - p_2\ndiff_3v1 = p_3 - p_1\n\nprint(f\"Direct from data:\")\nprint(f\"2:1 vs 1:1: {diff_2v1:.4f}\")\nprint(f\"3:1 vs 2:1: {diff_3v2:.4f}\")\nprint(f\"3:1 vs 1:1: {diff_3v1:.4f}\")\n\nDirect from data:\n2:1 vs 1:1: 0.0019\n3:1 vs 2:1: 0.0001\n3:1 vs 1:1: 0.0020\n\n\nResults from data match the regression coefficients. We can safely conclude that there is no effect that match ratios have on donation rates.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\n\ndf_reg = df[['amount', 'treatment']].dropna()\n\nX = sm.add_constant(df_reg['treatment'])\ny = df_reg['amount']\n\nmodel = sm.OLS(y, X).fit()\n\ncoef = model.params['treatment']\npval = model.pvalues['treatment']\n\nprint(f\"Treatment effect (difference in donation amount): {coef:.4f}\")\nprint(f\"p-value: {pval:.4f}\")\n\nTreatment effect (difference in donation amount): 0.1536\np-value: 0.0628\n\n\nWhile the results from the regression analysis above indicate an increase of 0.15 in the donation amount when given the treatment letter, the p-value is not statistically significant at the 95% confidence level. However, the difference calculated is accurate compared to Table2A of original analysis.\n\n# Filter to donors only\ndf_donors = df[(df['amount'] &gt; 0) & df['treatment'].notna()][['amount', 'treatment']]\n\nX = sm.add_constant(df_donors['treatment'])\ny = df_donors['amount']\n\nmodel = sm.OLS(y, X).fit()\n\ncoef = model.params['treatment']\npval = model.pvalues['treatment']\n\nprint(f\"Treatment effect among donors: {coef:.2f}\")\nprint(f\"p-value: {pval:.4f}\")\n\nTreatment effect among donors: -1.67\np-value: 0.5615\n\n\nWhile the results of the above regression, only considering those who actually donated to begin with, indicate a small decrease in the donation amount, this amount is not statistically significant. Therefore, we cannot conclude that the treatment letters (those with the match ratios) have any significant causal effect on the actual donation amount.\n\n# Filter to donors only\ndf_donors = df[df['amount'] &gt; 0]\n\n# Split groups\ntreatment_amounts = df_donors[df_donors['treatment'] == 1]['amount']\ncontrol_amounts = df_donors[df_donors['control'] == 1]['amount']\n\n# Calculate means\nmean_treatment = treatment_amounts.mean()\nmean_control = control_amounts.mean()\n\n# Plot: Control Group\nplt.figure(figsize=(8, 5))\nplt.hist(control_amounts, bins=30, alpha=0.7, color='gray', edgecolor='black')\nplt.axvline(mean_control, color='red', linestyle='--', linewidth=2, label=f'Mean: ${mean_control:.2f}')\nplt.title('Donation Amounts (Control Group)')\nplt.xlabel('Donation Amount')\nplt.ylabel('Number of Donors')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Plot: Treatment Group\nplt.figure(figsize=(8, 5))\nplt.hist(treatment_amounts, bins=30, alpha=0.7, color='gray', edgecolor='black')\nplt.axvline(mean_treatment, color='red', linestyle='--', linewidth=2, label=f'Mean: ${mean_treatment:.2f}')\nplt.title('Donation Amounts (Treatment Group)')\nplt.xlabel('Donation Amount')\nplt.ylabel('Number of Donors')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe two histograms above indicate a similar average and similar distribution of donation amounts among those who donated to begin with."
  },
  {
    "objectID": "projects/hw1/index.html#simulation-experiment",
    "href": "projects/hw1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic ‚Äúworks,‚Äù in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Step 1: Simulate 10k Bernoulli draws for each group\nn_draws = 10000\ncontrol_draws = np.random.binomial(1, 0.018, size=n_draws)\ntreatment_draws = np.random.binomial(1, 0.022, size=n_draws)\n\n# Step 2: Compute the vector of differences\ndiffs = treatment_draws - control_draws  # element-wise difference\n\n# Step 3: Compute cumulative average of the differences\ncumulative_avg = np.cumsum(diffs) / np.arange(1, n_draws + 1)\n\n# Step 4: Plot cumulative average\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg, label='Cumulative Average Treatment Effect')\nplt.axhline(y=0.004, color='red', linestyle='--', label='True Difference (0.022 - 0.018)')\nplt.title('Cumulative Average of Simulated Treatment Effect')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Cumulative Average Difference')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAccordingly, in the simulation above, the cumulative average eventually converges towards the true difference of 0.004. As the sample size of the random simulation gets larger, the average eventually converges to the true average.\n\n\nCentral Limit Theorem\n\n# Set seed\nnp.random.seed(42)\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\nn_simulations = 1000\nsample_sizes = [50, 200, 500, 1000]\n\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    mean_diffs = []\n    \n    for _ in range(n_simulations):\n        # Sample from each distribution\n        control_sample = np.random.binomial(1, p_control, size=n)\n        treatment_sample = np.random.binomial(1, p_treatment, size=n)\n\n        # Calculate mean difference\n        diff = treatment_sample.mean() - control_sample.mean()\n        mean_diffs.append(diff)\n    \n    # Plot histogram\n    axes[i].hist(mean_diffs, bins=30, edgecolor='black', alpha=0.75)\n    axes[i].axvline(np.mean(mean_diffs), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(mean_diffs):.4f}')\n    axes[i].set_title(f'Sample Size = {n}')\n    axes[i].set_xlabel('Mean Difference')\n    axes[i].set_ylabel('Frequency')\n    axes[i].legend()\n    axes[i].grid(True)\n\n# Final layout\nplt.suptitle('Sampling Distribution of Mean Differences (Treatment - Control)', fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nWhen running a simulation experiment, the sampling distribution takes on a bell-shape very quickly, even at sample size of 50, but gets smoothed out by the time sample size is 1000. Central Limit Theorem takes place earlier in the sequence than anticipated, approximating a normal distribution, but more filled in and smooth by 1000 sample size."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Machine Learning\n\n\n\n\nBrian Pintar\nJun 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\nBrian Pintar\nMay 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\nBrian Pintar\nMay 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\nBrian Pintar\nApr 22, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brian Pintar",
    "section": "",
    "text": "This website is created for MGTA 495 assignments.\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "projects/hw3/index.html",
    "href": "projects/hw3/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "projects/hw3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "projects/hw3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer‚Äôs decision as the selection of the product that provides the most utility, and we‚Äôll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "projects/hw3/index.html#simulate-conjoint-data",
    "href": "projects/hw3/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a ‚Äúno choice‚Äù option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\nShow code\nimport numpy as np\nimport pandas as pd\nfrom itertools import product\n\n# Set random seed for reproducibility\nnp.random.seed(123)\n\n# 1. Define attribute levels\nbrands = ['N', 'P', 'H']  # Netflix, Prime, Hulu\nads = ['Yes', 'No']\nprices = np.arange(8, 33, 4)  # $8 to $32\n\n# 2. Generate all possible profiles\nprofiles = pd.DataFrame(list(product(brands, ads, prices)), columns=['brand', 'ad', 'price'])\n\n# 3. Define utility weights\nbrand_util = {'N': 1.0, 'P': 0.5, 'H': 0.0}\nad_util = {'Yes': -0.8, 'No': 0.0}\nprice_util = lambda p: -0.1 * p\n\n# 4. Simulation parameters\nn_respondents = 100\nn_tasks = 10\nn_alts = 3\n\n# 5. Simulate choice tasks for each respondent\nsimulated_data = []\n\nfor respondent_id in range(1, n_respondents + 1):\n    for task_id in range(1, n_tasks + 1):\n        sampled_profiles = profiles.sample(n=n_alts).reset_index(drop=True)\n        sampled_profiles['brand_util'] = sampled_profiles['brand'].map(brand_util)\n        sampled_profiles['ad_util'] = sampled_profiles['ad'].map(ad_util)\n        sampled_profiles['price_util'] = sampled_profiles['price'].apply(price_util)\n        \n        # Compute deterministic utility\n        sampled_profiles['utility'] = (\n            sampled_profiles['brand_util'] +\n            sampled_profiles['ad_util'] +\n            sampled_profiles['price_util']\n        )\n        \n        # Add Gumbel noise\n        gumbel_noise = np.random.gumbel(loc=0, scale=1, size=n_alts)\n        sampled_profiles['total_utility'] = sampled_profiles['utility'] + gumbel_noise\n\n        # Determine choice (1 if max utility, else 0)\n        choice_index = sampled_profiles['total_utility'].idxmax()\n        sampled_profiles['choice'] = 0\n        sampled_profiles.loc[choice_index, 'choice'] = 1\n\n        # Add metadata\n        sampled_profiles['respondent'] = respondent_id\n        sampled_profiles['task'] = task_id\n\n        simulated_data.append(sampled_profiles)\n\n# Combine into a single DataFrame\ndf_simulated = pd.concat(simulated_data, ignore_index=True)"
  },
  {
    "objectID": "projects/hw3/index.html#preparing-the-data-for-estimation",
    "href": "projects/hw3/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe ‚Äúhard part‚Äù of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n\nShow code\n# One-hot encode brand and ad (drop the reference levels: brand_H and ad_No)\ndf_design = pd.get_dummies(df_simulated, columns=['brand', 'ad'], drop_first=True)\n\n# Keep only relevant columns for X matrix\nX_columns = ['brand_N', 'brand_P', 'ad_Yes', 'price']\nX = df_design[X_columns]\n\n# Outcome variable: 1 if alternative was chosen, 0 otherwise\ny = df_design['choice'].values\n\n# Also store respondent and task IDs for future grouping if needed\nrespondent_ids = df_design['respondent'].values\ntask_ids = df_design['task'].values\n\n# Show a preview of the prepared design matrix\ndf_prepared = df_design[['respondent', 'task'] + X_columns + ['choice']]\n\ndf_prepared.head()\n\n\n\n\n\n\n\n\n\nrespondent\ntask\nbrand_N\nbrand_P\nad_Yes\nprice\nchoice\n\n\n\n\n0\n1\n1\nFalse\nTrue\nFalse\n32\n0\n\n\n1\n1\n1\nTrue\nFalse\nFalse\n28\n1\n\n\n2\n1\n1\nTrue\nFalse\nFalse\n24\n0\n\n\n3\n1\n2\nFalse\nFalse\nFalse\n28\n0\n\n\n4\n1\n2\nFalse\nFalse\nFalse\n8\n1"
  },
  {
    "objectID": "projects/hw3/index.html#estimation-via-maximum-likelihood",
    "href": "projects/hw3/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nTo estimate the part-worth parameters of the multinomial logit model, we define the log-likelihood function based on the softmax probability formulation. Each respondent‚Äôs utility for each alternative is calculated as a linear function of the attributes (brand, ad presence, and price). We then maximize the log-likelihood using scipy.optimize.minimize() with the BFGS algorithm.\nThe estimated coefficients correspond to: - \\(\\beta_{\\text{netflix}}\\): preference for Netflix (vs.¬†Hulu) - \\(\\beta_{\\text{prime}}\\): preference for Prime (vs.¬†Hulu) - \\(\\beta_{\\text{ads}}\\): penalty for ad-included options (vs.¬†ad-free) - \\(\\beta_{\\text{price}}\\): marginal disutility per dollar\nWe also compute standard errors from the inverse Hessian matrix and report 95% confidence intervals for each parameter estimate.\n\n\nShow code\n# Re-import necessary packages after code execution state reset\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\n\n# Ensure X and y are NumPy arrays\nX = df_design[['brand_N', 'brand_P', 'ad_Yes', 'price']].to_numpy(dtype=np.float64)\ny = df_design['choice'].to_numpy(dtype=np.int64)\n\n# Reshape y into (n_tasks, 3)\ny_matrix = y.reshape((-1, 3))\nn_tasks_total = y_matrix.shape[0]\n\n# Define negative log-likelihood for MNL\ndef neg_log_likelihood(beta):\n    beta = np.asarray(beta, dtype=np.float64)\n    utilities = X @ beta\n    utilities = utilities.reshape((-1, 3))\n    exp_utils = np.exp(utilities)\n    probs = exp_utils / np.sum(exp_utils, axis=1, keepdims=True)\n    chosen_probs = probs[np.arange(n_tasks_total), y_matrix.argmax(axis=1)]\n    return -np.sum(np.log(chosen_probs))\n\n# Initial guess\nbeta_init = np.zeros(X.shape[1])\n\n# Minimize the negative log-likelihood\nresult = minimize(neg_log_likelihood, beta_init, method='BFGS')\n\n# Extract estimates and standard errors\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\nz = 1.96\n\n# Confidence intervals\nconf_int = np.column_stack([\n    beta_hat - z * std_errors,\n    beta_hat + z * std_errors\n])\n\n# Package results\nparam_names = ['brand_N', 'brand_P', 'ad_Yes', 'price']\nresults_df = pd.DataFrame({\n    'Parameter': param_names,\n    'Estimate': beta_hat,\n    'Std. Error': std_errors,\n    'CI Lower (95%)': conf_int[:, 0],\n    'CI Upper (95%)': conf_int[:, 1]\n})\n\nresults_df\n\n\n\n\n\n\n\n\n\nParameter\nEstimate\nStd. Error\nCI Lower (95%)\nCI Upper (95%)\n\n\n\n\n0\nbrand_N\n0.974795\n0.125815\n0.728199\n1.221392\n\n\n1\nbrand_P\n0.426656\n0.115195\n0.200874\n0.652439\n\n\n2\nad_Yes\n-0.728157\n0.095708\n-0.915744\n-0.540569\n\n\n3\nprice\n-0.111804\n0.006604\n-0.124747\n-0.098861\n\n\n\n\n\n\n\nThe table above reports the estimated part-worth utilities for each attribute level in the multinomial logit (MNL) model. All four coefficients are statistically significant at the 95% confidence level, as none of their confidence intervals include zero. The signs and magnitudes of the estimates align with economic intuition and the true values used in the simulation.\nBrand Preferences: Consumers exhibit a strong preference for Netflix over Hulu (baseline), with an estimated utility gain of approximately 0.98 units. Amazon Prime is also preferred over Hulu, though to a lesser degree (0.43 units). These estimates closely match the simulated values of 1.0 and 0.5, respectively.\nAdvertising: The presence of advertisements decreases the utility of a streaming offer by about 0.73 units. This negative effect is in line with the simulated disutility of -0.8 and indicates that consumers have a strong preference for ad-free experiences.\nPrice Sensitivity: Each additional dollar in monthly price reduces utility by approximately 0.11 units. This estimate is tightly bounded and very close to the simulated effect of -0.1, suggesting that the model has successfully captured consumers‚Äô price sensitivity.\nOverall, the MNL model recovers the true preference structure well, validating both the simulation setup and the estimation procedure."
  },
  {
    "objectID": "projects/hw3/index.html#estimation-via-bayesian-methods",
    "href": "projects/hw3/index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nWe now estimate the multinomial logit model using a Bayesian approach via Metropolis-Hastings MCMC. The posterior distribution is defined by combining the log-likelihood with prior distributions:\n\nNormal priors \\(\\mathcal{N}(0, 5)\\) for the binary attribute coefficients (brand_N, brand_P, ad_Yes)\nA tighter prior \\(\\mathcal{N}(0, 1)\\) for the continuous price coefficient\n\nWe run the sampler for 11,000 iterations, discarding the first 1,000 as burn-in, and use the remaining 10,000 to summarize the posterior distribution.\nTo generate proposals, we use a multivariate normal distribution with independent components: - \\(\\mathcal{N}(0, 0.05)\\) for the three binary variables - \\(\\mathcal{N}(0, 0.005)\\) for the price coefficient\nWe assess convergence by examining a trace plot and posterior histogram for the price parameter.\n\n\nShow code\nimport matplotlib.pyplot as plt\nfrom itertools import product\n\n# Re-run data simulation and preparation\nnp.random.seed(123)\nbrands = ['N', 'P', 'H']\nads = ['Yes', 'No']\nprices = np.arange(8, 33, 4)\nprofiles = pd.DataFrame(list(product(brands, ads, prices)), columns=['brand', 'ad', 'price'])\n\nbrand_util = {'N': 1.0, 'P': 0.5, 'H': 0.0}\nad_util = {'Yes': -0.8, 'No': 0.0}\nprice_util = lambda p: -0.1 * p\n\nn_respondents = 100\nn_tasks = 10\nn_alts = 3\n\nsimulated_data = []\nfor respondent_id in range(1, n_respondents + 1):\n    for task_id in range(1, n_tasks + 1):\n        sampled = profiles.sample(n=n_alts).reset_index(drop=True)\n        sampled['brand_util'] = sampled['brand'].map(brand_util)\n        sampled['ad_util'] = sampled['ad'].map(ad_util)\n        sampled['price_util'] = sampled['price'].apply(price_util)\n        sampled['utility'] = sampled['brand_util'] + sampled['ad_util'] + sampled['price_util']\n        noise = np.random.gumbel(0, 1, n_alts)\n        sampled['total_utility'] = sampled['utility'] + noise\n        sampled['choice'] = 0\n        sampled.loc[sampled['total_utility'].idxmax(), 'choice'] = 1\n        sampled['respondent'] = respondent_id\n        sampled['task'] = task_id\n        simulated_data.append(sampled)\n\ndf_simulated = pd.concat(simulated_data, ignore_index=True)\ndf_design = pd.get_dummies(df_simulated, columns=['brand', 'ad'], drop_first=True)\nX = df_design[['brand_N', 'brand_P', 'ad_Yes', 'price']].to_numpy(dtype=np.float64)\ny = df_design['choice'].to_numpy(dtype=np.int64)\ny_matrix = y.reshape((-1, 3))\nn_tasks_total = y_matrix.shape[0]\n\n# Define functions\ndef log_likelihood(beta):\n    utilities = X @ beta\n    utilities = utilities.reshape((-1, 3))\n    exp_utilities = np.exp(utilities)\n    probs = exp_utilities / exp_utilities.sum(axis=1, keepdims=True)\n    chosen_probs = probs[np.arange(n_tasks_total), y_matrix.argmax(axis=1)]\n    return np.sum(np.log(chosen_probs))\n\ndef log_prior(beta):\n    return (\n        -0.5 * (beta[0]**2 / 25)\n        -0.5 * (beta[1]**2 / 25)\n        -0.5 * (beta[2]**2 / 25)\n        -0.5 * (beta[3]**2 / 1)\n    )\n\ndef log_posterior(beta):\n    return log_likelihood(beta) + log_prior(beta)\n\n# Metropolis-Hastings MCMC\nn_iter = 11000\nbeta_samples = np.zeros((n_iter, 4))\nbeta_current = np.zeros(4)\nlog_post_current = log_posterior(beta_current)\n\nfor i in range(1, n_iter):\n    proposal = beta_current + np.random.normal(loc=0, scale=[0.05, 0.05, 0.05, 0.005])\n    log_post_proposal = log_posterior(proposal)\n    accept_prob = np.exp(log_post_proposal - log_post_current)\n    if np.random.rand() &lt; accept_prob:\n        beta_current = proposal\n        log_post_current = log_post_proposal\n    beta_samples[i] = beta_current\n\n# Posterior summaries after burn-in\nburn_in = 1000\nbeta_samples_post = beta_samples[burn_in:]\nposterior_means = beta_samples_post.mean(axis=0)\nposterior_sds = beta_samples_post.std(axis=0)\ncred_intervals = np.percentile(beta_samples_post, [2.5, 97.5], axis=0).T\n\n# Results DataFrame\nparam_names = ['brand_N', 'brand_P', 'ad_Yes', 'price']\nbayes_results_df = pd.DataFrame({\n    'Parameter': param_names,\n    'Posterior Mean': posterior_means,\n    'Posterior SD': posterior_sds,\n    'CI Lower (95%)': cred_intervals[:, 0],\n    'CI Upper (95%)': cred_intervals[:, 1]\n})\n\n# Trace plot and histogram for price\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(beta_samples_post[:, 3])\nplt.title('Trace Plot: price')\nplt.xlabel('Iteration')\nplt.ylabel('Value')\n\nplt.subplot(1, 2, 2)\nplt.hist(beta_samples_post[:, 3], bins=30, edgecolor='k')\nplt.title('Posterior Histogram: price')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.tight_layout()\nplt.show()\n\n\nbayes_results_df.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nPosterior Mean\nPosterior SD\nCI Lower (95%)\nCI Upper (95%)\n\n\n\n\n0\nbrand_N\n0.964579\n0.109686\n0.753379\n1.184491\n\n\n1\nbrand_P\n0.415216\n0.107461\n0.210608\n0.625212\n\n\n2\nad_Yes\n-0.722899\n0.092084\n-0.902845\n-0.545688\n\n\n3\nprice\n-0.111793\n0.006545\n-0.124939\n-0.099420\n\n\n\n\n\n\n\nThe posterior summary table shows the means, standard deviations, and 95% credible intervals for each parameter. These estimates closely align with those obtained from maximum likelihood estimation in Section 4.\n\nThe posterior means match the MLE estimates within small margins.\nThe credible intervals are tight and all exclude zero, indicating strong evidence for the direction and magnitude of each effect.\nThe trace plot for the price parameter shows good mixing behavior, and the posterior distribution appears approximately normal.\n\nThese results confirm that both MLE and Bayesian methods yield consistent and interpretable insights about consumer preferences in this simulated conjoint setting."
  },
  {
    "objectID": "projects/hw3/index.html#discussion",
    "href": "projects/hw3/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\n\nInterpreting Parameter Estimates\nEven without knowing the true data-generating process (i.e., in a real-world setting), the estimated parameters from the multinomial logit model align with strong economic intuition:\n\n\\(\\beta_{\\text{Netflix}} &gt; \\beta_{\\text{Prime}}\\) implies that, on average, consumers have a stronger preference for Netflix than Amazon Prime, relative to the baseline brand (Hulu). This could be due to perceived content quality, popularity, or brand loyalty.\nA negative \\(\\beta_{\\text{price}}\\) is expected and logical. It means that, all else equal, consumers are less likely to choose a more expensive plan. The magnitude reflects how sensitive utility is to price ‚Äî a critical insight for pricing strategy.\nThe ad penalty coefficient, \\(\\beta_{\\text{ads}} &lt; 0\\), indicates that consumers value ad-free experiences and are willing to give up some utility (or pay more) to avoid ads.\n\nThese results demonstrate how the MNL model can quantify the impact of product features on consumer decision-making, even when the true parameters are unknown.\n\n\nToward Hierarchical (Multilevel) Models\nIn real-world conjoint studies, different consumers often have different preferences ‚Äî not everyone values brand or price the same way. To capture this heterogeneity, we would move from a fixed-parameter MNL model to a hierarchical (random-parameter) model.\nThis change involves two key modifications:\n\nData Simulation: Instead of assigning one fixed set of \\(\\beta\\) values to all respondents, we would draw individual-level parameters from a population distribution. This would better reflect individual variation in preferences.\nEstimation: We would use a Bayesian hierarchical model (via MCMC) or a Mixed Logit model (via simulation-based MLE) to estimate both:\n\nThe population-level means \\(\\mu\\)\nThe variation across individuals, captured by \\(\\Sigma\\)\n\n\nThese models are more flexible and realistic, and are commonly used in industry applications of conjoint analysis.\nIn summary, while the standard MNL model is a solid starting point, moving to a hierarchical framework is critical for analyzing real consumer choice data with preference heterogeneity."
  },
  {
    "objectID": "projects/hw2/index.html",
    "href": "projects/hw2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\nFirst, we can read in the data and do some exploratory analysis.\n\nimport pandas as pd\n\ndf1 = pd.read_csv('blueprinty.csv')\ndf1.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nLet‚Äôs compare histograms and means of number of patents by customer status.\n\nimport matplotlib.pyplot as plt\n\n# Create two histograms side by side\nplt.figure(figsize=(12, 5))\n\n# Histogram for non-customers\nplt.subplot(1, 2, 1)\nplt.hist(df1[df1['iscustomer'] == 0]['patents'], bins=30, alpha=0.7)\nplt.title('Non-Customers: Number of Patents')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\n\n# Histogram for customers\nplt.subplot(1, 2, 2)\nplt.hist(df1[df1['iscustomer'] == 1]['patents'], bins=30, alpha=0.7, color='orange')\nplt.title('Customers: Number of Patents')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nHistograms indicate somewhat similar results. Let‚Äôs compare means to test for statistical significance.\n\nfrom scipy.stats import ttest_ind\n\n# Separate the groups\npatents_customers = df1[df1['iscustomer'] == 1]['patents']\npatents_non_customers = df1[df1['iscustomer'] == 0]['patents']\n\n# Calculate means\nmean_customers = patents_customers.mean()\nmean_non_customers = patents_non_customers.mean()\n\n# Perform the t-test\nt_stat, p_value = ttest_ind(patents_customers, patents_non_customers, equal_var=False)\n\n# Print results\nprint(f\"Mean patents (customers): {mean_customers:.2f}\")\nprint(f\"Mean patents (non-customers): {mean_non_customers:.2f}\")\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_value:.3f}\")\n\nMean patents (customers): 4.13\nMean patents (non-customers): 3.47\nT-statistic: 4.873\nP-value: 0.000\n\n\nBased on our t-test results, we reject the null hypothesis that the means are equal.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nLet‚Äôs compare age between the customer and non customer groups first.\n\nimport seaborn as sns\n\nplt.figure(figsize=(10, 5))\n\nsns.kdeplot(data=df1, x='age', hue='iscustomer', fill=True, common_norm=False, alpha=0.5)\nplt.title('Age Distribution by Customer Status')\nplt.xlabel('Age')\nplt.ylabel('Density')\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.show()\n\n\n\n\n\n\n\n\nThere appears to be a slight difference in age between customers and non customers. Let‚Äôs run a t-test to check.\n\n# Separate the age values\nage_customers = df1[df1['iscustomer'] == 1]['age']\nage_non_customers = df1[df1['iscustomer'] == 0]['age']\n\n# Calculate means\nmean_age_customers = age_customers.mean()\nmean_age_non_customers = age_non_customers.mean()\n\n# Perform the t-test\nt_stat_age, p_value_age = ttest_ind(age_customers, age_non_customers, equal_var=False)\n\n# Print results\nprint(f\"Mean age (customers): {mean_age_customers:.2f}\")\nprint(f\"Mean age (non-customers): {mean_age_non_customers:.2f}\")\nprint(f\"T-statistic (age): {t_stat_age:.3f}\")\nprint(f\"P-value (age): {p_value_age:.3f}\")\n\nMean age (customers): 26.90\nMean age (non-customers): 26.10\nT-statistic (age): 1.913\nP-value (age): 0.056\n\n\nWe fail to reject the null hypothesis, indicating no statistically significant difference in age between the customer and non customer groups. Now we repeat the process for region.\n\n# Create a count table (not normalized)\nregion_counts = pd.crosstab(df1['region'], df1['iscustomer'])\nregion_counts.columns = ['Non-Customers', 'Customers']\n\n# Plot grouped bar chart\nregion_counts.plot(kind='bar', figsize=(10, 6))\nplt.title('Number of Customers and Non-Customers by Region')\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.legend(title='Customer Status')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIt appears based on the chart that there are some differences in the proportions of customers vs.¬†non customers in different regions. We‚Äôll run a chi-squared test to check this.\n\nfrom scipy.stats import chi2_contingency\n\n# Crosstab: region vs customer status\nregion_table = pd.crosstab(df1['region'], df1['iscustomer'])\n\n# Chi-squared test\nchi2_stat, p_value_region, dof, expected = chi2_contingency(region_table)\n\nprint(\"Chi-squared Statistic:\", round(chi2_stat, 2))\nprint(\"Degrees of Freedom:\", dof)\nprint(\"P-value:\", round(p_value_region, 4))\n\nChi-squared Statistic: 233.63\nDegrees of Freedom: 4\nP-value: 0.0\n\n\nBased on the low p-value, we can assume there is a statistically significant relationship between region and customer status. Accordingly, the distribution of customers across regions is not random, as some are more or less likely to have customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nLet‚Äôs mathematically write the likelihood for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nThe likelihood for all observations is:\n\\(L(\\lambda \\mid Y) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\\)\nThe log-likelihood is:\n\\(\\ell(\\lambda \\mid Y) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\\)\nNow let‚Äôs write the log-likelihood function for the Poisson model as a function of lambda and Y.\n\nimport numpy as np\nfrom scipy.special import gammaln  # log(Y!) = gammaln(Y + 1)\n\ndef poisson_loglikelihood(lambd, Y):\n    \"\"\"\n    Compute the log-likelihood of a Poisson model given lambda and observed data Y.\n\n    Parameters:\n    - lambd: float, the Poisson rate parameter (must be &gt; 0)\n    - Y: array-like, observed count data\n\n    Returns:\n    - log-likelihood value\n    \"\"\"\n    if lambd &lt;= 0:\n        return -np.inf  # log-likelihood is undefined for lambda &lt;= 0\n    return np.sum(-lambd + Y * np.log(lambd) - gammaln(Y + 1))\n\nNow, let‚Äôs use the function to plot lambda on the horizontal axis and the log-likelihood on the vertical axis for a range of lambdas.\n\n# Get observed patent data\nY_obs = df1['patents'].values\n\n# Define a range of lambda values to evaluate\nlambda_vals = np.linspace(0.1, 20, 200)\n\n# Compute the log-likelihood for each lambda\nloglik_vals = [poisson_loglikelihood(l, Y_obs) for l in lambda_vals]\n\n# Plot the log-likelihood curve\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='blue', lw=2)\nplt.title('Log-Likelihood of Poisson Model vs. Lambda')\nplt.xlabel(r'$\\lambda$')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nBased on the graph above, the highest log-likelihood appears to occur when lambda is around 3. Now let‚Äôs check and see if our numerical mean is the same as our analytic mean.\nWe start with the log-likelihood:\n\\[\n\\ell(\\lambda \\mid Y) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nTaking the derivative:\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSet this equal to zero and solve:\n\\[\n-n + \\frac{1}{\\lambda} \\sum Y_i = 0 \\Rightarrow \\lambda = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\nTherefore, the MLE of \\(\\lambda\\) is the sample mean \\(\\bar{Y}\\).\nLet‚Äôs find the MLE by optimizing the likelihood function.\n\nfrom scipy.optimize import minimize_scalar\n\n# Reuse observed data\nY_obs = df1['patents'].values\n\n# Define negative log-likelihood function\ndef neg_loglik_poisson(lambd):\n    return -poisson_loglikelihood(lambd, Y_obs)\n\n# Use minimize_scalar to find the lambda that minimizes negative log-likelihood\nresult = minimize_scalar(neg_loglik_poisson, bounds=(0.01, 50), method='bounded')\n\n# Extract MLE\nlambda_mle_numerical = result.x\nloglik_at_mle = -result.fun\n\nprint(f\"MLE (Numerical): {lambda_mle_numerical:.4f}\")\nprint(f\"Log-Likelihood at MLE: {loglik_at_mle:.4f}\")\n\n# Compare to analytic MLE\nprint(f\"MLE (Analytic Mean): {np.mean(Y_obs):.4f}\")\n\nMLE (Numerical): 3.6847\nLog-Likelihood at MLE: -3367.6838\nMLE (Analytic Mean): 3.6847\n\n\nBoth methods agree that the best fitting Poisson model indicates that firms receive, on average, 3.68 patents every five years.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nLet‚Äôs define our updated regression function.\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"\n    Log-likelihood for Poisson regression.\n\n    Parameters:\n    - beta: (p,) vector of coefficients\n    - Y: (n,) vector of observed counts\n    - X: (n, p) matrix of covariates\n\n    Returns:\n    - scalar log-likelihood value\n    \"\"\"\n    beta = np.asarray(beta, dtype=np.float64)  # ensure correct type\n    X = np.asarray(X, dtype=np.float64)        # ensure matrix format\n    Xb = np.dot(X, beta)                       # matrix multiplication\n    lambdas = np.exp(Xb)                       # element-wise exponential\n    return np.sum(-lambdas + Y * Xb - gammaln(Y + 1))\n\nNow, let‚Äôs create the covariate matrix X.\n\nimport patsy  # optional, but can simplify dummy creation\n\n# Create a copy of the data to work with\ndf = df1.copy()\n\n# Create new variables\n# Rescale age and age_squared\ndf['age'] = df['age'] / 10\ndf['age_squared'] = df['age'] ** 2\n\n# One-hot encode region, dropping one to avoid multicollinearity\nregion_dummies = pd.get_dummies(df['region'], drop_first=True)\n\n# Combine into one design matrix\nX = pd.concat([\n    pd.Series(1, index=df.index, name='intercept'),\n    df[['age', 'age_squared', 'iscustomer']],\n    pd.get_dummies(df['region'], drop_first=True)\n], axis=1)\n\nY = df['patents'].values\nX_mat = X.values  # for optimization input only\n\n# Column names\ncolumn_names = X.columns\n\nLet‚Äôs now optimize to find the MLE vector and the Hessian of the Poisson model with covariates.\n\nfrom scipy.optimize import minimize\n\nX_mat = X.to_numpy().astype(np.float64)  # make sure X is a NumPy array\nY_vec = Y.astype(np.float64)             # make sure Y is a NumPy array\ninit_beta = np.zeros(X_mat.shape[1])     # starting guess for beta (all 0s)\n\n# Define the negative log-likelihood function for use in optimizer\nneg_loglik = lambda b: -poisson_regression_loglikelihood(b, Y_vec, X_mat)\n\n# Optimize\nresult = minimize(neg_loglik, init_beta, method='BFGS')\n\nLet‚Äôs now extract the results.\n\n# Extract estimated coefficients and standard errors\nbeta_hat = result.x\nvcov_beta = result.hess_inv\nstandard_errors = np.sqrt(np.diag(vcov_beta))\n\n# Show results in a table\nsummary_table = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': standard_errors\n}, index=X.columns)\n\nsummary_table\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.508925\n0.185670\n\n\nage\n1.486199\n0.139210\n\n\nage_squared\n-0.297048\n0.025840\n\n\niscustomer\n0.207591\n0.032190\n\n\nNortheast\n0.029170\n0.046083\n\n\nNorthwest\n-0.017574\n0.055659\n\n\nSouth\n0.056562\n0.054619\n\n\nSouthwest\n0.050576\n0.048116\n\n\n\n\n\n\n\nThe Poisson regression estimates how the expected number of patents awarded to a firm varies with its characteristics. To improve numerical stability during optimization, the age variable was scaled by a factor of 10 (i.e., a one-unit change in age represents a 10-year difference).\nThe coefficient for age is 1.49, and for age_squared is ‚àí0.30, indicating a non-linear relationship: as firms age, their expected number of patents initially increases, but the rate of increase slows and eventually declines with age.\nThe coefficient on iscustomer is 0.208, meaning that, holding all else equal, firms that use Blueprinty‚Äôs software have a materially higher expected patent count compared to non-customers.\nRegional effects appear minimal, with all region dummy coefficients close to zero and standard errors indicating low statistical significance relative to the reference region.\nNow let‚Äôs compare results against the statsmodels ‚Äòsm.GLM()‚Äô function.\n\nimport statsmodels.api as sm\n\n# Recreate X_sm and ensure it's numeric\nX_sm = sm.add_constant(X.drop(columns='intercept'), has_constant='add')\nX_sm = X_sm.astype(float)  # &lt;== This line prevents the object dtype error\n\n# Also ensure Y is numeric\nY_numeric = Y.astype(float)\n\n# Fit GLM model\nglm_model = sm.GLM(Y_numeric, X_sm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary\n# Create a clean summary table\nsummary_table = pd.DataFrame({\n    'Coefficient': glm_results.params,\n    'Std. Error': glm_results.bse,\n    'z value': glm_results.tvalues,\n    'P&gt;|z|': glm_results.pvalues\n})\n\n# Optional: round for readability\nsummary_table = summary_table.round(4)\n\nsummary_table\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz value\nP&gt;|z|\n\n\n\n\nconst\n-0.5089\n0.1832\n-2.7783\n0.0055\n\n\nage\n1.4862\n0.1387\n10.7162\n0.0000\n\n\nage_squared\n-0.2970\n0.0258\n-11.5132\n0.0000\n\n\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n\n\nNortheast\n0.0292\n0.0436\n0.6686\n0.5037\n\n\nNorthwest\n-0.0176\n0.0538\n-0.3268\n0.7438\n\n\nSouth\n0.0566\n0.0527\n1.0740\n0.2828\n\n\nSouthwest\n0.0506\n0.0472\n1.0716\n0.2839\n\n\n\n\n\n\n\nThe results closely matched those from the custom maximum likelihood estimation (MLE) approach, with all coefficients and standard errors agreeing to several decimal places. This confirms that our manually coded log-likelihood function and optimization routine are correctly specified.\nWe‚Äôll now try to quantify the effect of using Blueprinty‚Äôs software by simulating predicted patent counts for the firms in our data in two scenarios, one where all firms are non-customers, and one where all were customers, holding all else constant.\n\n# Create counterfactual datasets\nX_0 = X.copy()\nX_1 = X.copy()\n\n# Set iscustomer to 0 and 1 respectively\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n# Ensure matrix and beta vector are both float64\nX0_mat = X_0.to_numpy().astype(np.float64)\nX1_mat = X_1.to_numpy().astype(np.float64)\nbeta_vec = np.asarray(beta_hat, dtype=np.float64)  # just to be safe\n\n# Predict expected patent counts\ny_pred_0 = np.exp(np.dot(X0_mat, beta_vec))\ny_pred_1 = np.exp(np.dot(X1_mat, beta_vec))\n\neffect_diff = y_pred_1 - y_pred_0\navg_effect = np.mean(effect_diff)\n\nprint(f\"Average predicted increase in patent count from using Blueprinty's software: {avg_effect:.3f}\")\n\nAverage predicted increase in patent count from using Blueprinty's software: 0.793\n\n\nWe find that the average predicted number of patents increased by about 0.793 when firms used Blueprinty‚Äôs software. Resultingly, Blueprinty customers are expected to recieve one additional patent over a five year period compared to similar non-customers."
  },
  {
    "objectID": "projects/hw2/index.html#blueprinty-case-study",
    "href": "projects/hw2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\nFirst, we can read in the data and do some exploratory analysis.\n\nimport pandas as pd\n\ndf1 = pd.read_csv('blueprinty.csv')\ndf1.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nLet‚Äôs compare histograms and means of number of patents by customer status.\n\nimport matplotlib.pyplot as plt\n\n# Create two histograms side by side\nplt.figure(figsize=(12, 5))\n\n# Histogram for non-customers\nplt.subplot(1, 2, 1)\nplt.hist(df1[df1['iscustomer'] == 0]['patents'], bins=30, alpha=0.7)\nplt.title('Non-Customers: Number of Patents')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\n\n# Histogram for customers\nplt.subplot(1, 2, 2)\nplt.hist(df1[df1['iscustomer'] == 1]['patents'], bins=30, alpha=0.7, color='orange')\nplt.title('Customers: Number of Patents')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nHistograms indicate somewhat similar results. Let‚Äôs compare means to test for statistical significance.\n\nfrom scipy.stats import ttest_ind\n\n# Separate the groups\npatents_customers = df1[df1['iscustomer'] == 1]['patents']\npatents_non_customers = df1[df1['iscustomer'] == 0]['patents']\n\n# Calculate means\nmean_customers = patents_customers.mean()\nmean_non_customers = patents_non_customers.mean()\n\n# Perform the t-test\nt_stat, p_value = ttest_ind(patents_customers, patents_non_customers, equal_var=False)\n\n# Print results\nprint(f\"Mean patents (customers): {mean_customers:.2f}\")\nprint(f\"Mean patents (non-customers): {mean_non_customers:.2f}\")\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_value:.3f}\")\n\nMean patents (customers): 4.13\nMean patents (non-customers): 3.47\nT-statistic: 4.873\nP-value: 0.000\n\n\nBased on our t-test results, we reject the null hypothesis that the means are equal.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nLet‚Äôs compare age between the customer and non customer groups first.\n\nimport seaborn as sns\n\nplt.figure(figsize=(10, 5))\n\nsns.kdeplot(data=df1, x='age', hue='iscustomer', fill=True, common_norm=False, alpha=0.5)\nplt.title('Age Distribution by Customer Status')\nplt.xlabel('Age')\nplt.ylabel('Density')\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.show()\n\n\n\n\n\n\n\n\nThere appears to be a slight difference in age between customers and non customers. Let‚Äôs run a t-test to check.\n\n# Separate the age values\nage_customers = df1[df1['iscustomer'] == 1]['age']\nage_non_customers = df1[df1['iscustomer'] == 0]['age']\n\n# Calculate means\nmean_age_customers = age_customers.mean()\nmean_age_non_customers = age_non_customers.mean()\n\n# Perform the t-test\nt_stat_age, p_value_age = ttest_ind(age_customers, age_non_customers, equal_var=False)\n\n# Print results\nprint(f\"Mean age (customers): {mean_age_customers:.2f}\")\nprint(f\"Mean age (non-customers): {mean_age_non_customers:.2f}\")\nprint(f\"T-statistic (age): {t_stat_age:.3f}\")\nprint(f\"P-value (age): {p_value_age:.3f}\")\n\nMean age (customers): 26.90\nMean age (non-customers): 26.10\nT-statistic (age): 1.913\nP-value (age): 0.056\n\n\nWe fail to reject the null hypothesis, indicating no statistically significant difference in age between the customer and non customer groups. Now we repeat the process for region.\n\n# Create a count table (not normalized)\nregion_counts = pd.crosstab(df1['region'], df1['iscustomer'])\nregion_counts.columns = ['Non-Customers', 'Customers']\n\n# Plot grouped bar chart\nregion_counts.plot(kind='bar', figsize=(10, 6))\nplt.title('Number of Customers and Non-Customers by Region')\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.legend(title='Customer Status')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIt appears based on the chart that there are some differences in the proportions of customers vs.¬†non customers in different regions. We‚Äôll run a chi-squared test to check this.\n\nfrom scipy.stats import chi2_contingency\n\n# Crosstab: region vs customer status\nregion_table = pd.crosstab(df1['region'], df1['iscustomer'])\n\n# Chi-squared test\nchi2_stat, p_value_region, dof, expected = chi2_contingency(region_table)\n\nprint(\"Chi-squared Statistic:\", round(chi2_stat, 2))\nprint(\"Degrees of Freedom:\", dof)\nprint(\"P-value:\", round(p_value_region, 4))\n\nChi-squared Statistic: 233.63\nDegrees of Freedom: 4\nP-value: 0.0\n\n\nBased on the low p-value, we can assume there is a statistically significant relationship between region and customer status. Accordingly, the distribution of customers across regions is not random, as some are more or less likely to have customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nLet‚Äôs mathematically write the likelihood for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nThe likelihood for all observations is:\n\\(L(\\lambda \\mid Y) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\\)\nThe log-likelihood is:\n\\(\\ell(\\lambda \\mid Y) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\\)\nNow let‚Äôs write the log-likelihood function for the Poisson model as a function of lambda and Y.\n\nimport numpy as np\nfrom scipy.special import gammaln  # log(Y!) = gammaln(Y + 1)\n\ndef poisson_loglikelihood(lambd, Y):\n    \"\"\"\n    Compute the log-likelihood of a Poisson model given lambda and observed data Y.\n\n    Parameters:\n    - lambd: float, the Poisson rate parameter (must be &gt; 0)\n    - Y: array-like, observed count data\n\n    Returns:\n    - log-likelihood value\n    \"\"\"\n    if lambd &lt;= 0:\n        return -np.inf  # log-likelihood is undefined for lambda &lt;= 0\n    return np.sum(-lambd + Y * np.log(lambd) - gammaln(Y + 1))\n\nNow, let‚Äôs use the function to plot lambda on the horizontal axis and the log-likelihood on the vertical axis for a range of lambdas.\n\n# Get observed patent data\nY_obs = df1['patents'].values\n\n# Define a range of lambda values to evaluate\nlambda_vals = np.linspace(0.1, 20, 200)\n\n# Compute the log-likelihood for each lambda\nloglik_vals = [poisson_loglikelihood(l, Y_obs) for l in lambda_vals]\n\n# Plot the log-likelihood curve\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='blue', lw=2)\nplt.title('Log-Likelihood of Poisson Model vs. Lambda')\nplt.xlabel(r'$\\lambda$')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nBased on the graph above, the highest log-likelihood appears to occur when lambda is around 3. Now let‚Äôs check and see if our numerical mean is the same as our analytic mean.\nWe start with the log-likelihood:\n\\[\n\\ell(\\lambda \\mid Y) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nTaking the derivative:\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSet this equal to zero and solve:\n\\[\n-n + \\frac{1}{\\lambda} \\sum Y_i = 0 \\Rightarrow \\lambda = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\nTherefore, the MLE of \\(\\lambda\\) is the sample mean \\(\\bar{Y}\\).\nLet‚Äôs find the MLE by optimizing the likelihood function.\n\nfrom scipy.optimize import minimize_scalar\n\n# Reuse observed data\nY_obs = df1['patents'].values\n\n# Define negative log-likelihood function\ndef neg_loglik_poisson(lambd):\n    return -poisson_loglikelihood(lambd, Y_obs)\n\n# Use minimize_scalar to find the lambda that minimizes negative log-likelihood\nresult = minimize_scalar(neg_loglik_poisson, bounds=(0.01, 50), method='bounded')\n\n# Extract MLE\nlambda_mle_numerical = result.x\nloglik_at_mle = -result.fun\n\nprint(f\"MLE (Numerical): {lambda_mle_numerical:.4f}\")\nprint(f\"Log-Likelihood at MLE: {loglik_at_mle:.4f}\")\n\n# Compare to analytic MLE\nprint(f\"MLE (Analytic Mean): {np.mean(Y_obs):.4f}\")\n\nMLE (Numerical): 3.6847\nLog-Likelihood at MLE: -3367.6838\nMLE (Analytic Mean): 3.6847\n\n\nBoth methods agree that the best fitting Poisson model indicates that firms receive, on average, 3.68 patents every five years.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nLet‚Äôs define our updated regression function.\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"\n    Log-likelihood for Poisson regression.\n\n    Parameters:\n    - beta: (p,) vector of coefficients\n    - Y: (n,) vector of observed counts\n    - X: (n, p) matrix of covariates\n\n    Returns:\n    - scalar log-likelihood value\n    \"\"\"\n    beta = np.asarray(beta, dtype=np.float64)  # ensure correct type\n    X = np.asarray(X, dtype=np.float64)        # ensure matrix format\n    Xb = np.dot(X, beta)                       # matrix multiplication\n    lambdas = np.exp(Xb)                       # element-wise exponential\n    return np.sum(-lambdas + Y * Xb - gammaln(Y + 1))\n\nNow, let‚Äôs create the covariate matrix X.\n\nimport patsy  # optional, but can simplify dummy creation\n\n# Create a copy of the data to work with\ndf = df1.copy()\n\n# Create new variables\n# Rescale age and age_squared\ndf['age'] = df['age'] / 10\ndf['age_squared'] = df['age'] ** 2\n\n# One-hot encode region, dropping one to avoid multicollinearity\nregion_dummies = pd.get_dummies(df['region'], drop_first=True)\n\n# Combine into one design matrix\nX = pd.concat([\n    pd.Series(1, index=df.index, name='intercept'),\n    df[['age', 'age_squared', 'iscustomer']],\n    pd.get_dummies(df['region'], drop_first=True)\n], axis=1)\n\nY = df['patents'].values\nX_mat = X.values  # for optimization input only\n\n# Column names\ncolumn_names = X.columns\n\nLet‚Äôs now optimize to find the MLE vector and the Hessian of the Poisson model with covariates.\n\nfrom scipy.optimize import minimize\n\nX_mat = X.to_numpy().astype(np.float64)  # make sure X is a NumPy array\nY_vec = Y.astype(np.float64)             # make sure Y is a NumPy array\ninit_beta = np.zeros(X_mat.shape[1])     # starting guess for beta (all 0s)\n\n# Define the negative log-likelihood function for use in optimizer\nneg_loglik = lambda b: -poisson_regression_loglikelihood(b, Y_vec, X_mat)\n\n# Optimize\nresult = minimize(neg_loglik, init_beta, method='BFGS')\n\nLet‚Äôs now extract the results.\n\n# Extract estimated coefficients and standard errors\nbeta_hat = result.x\nvcov_beta = result.hess_inv\nstandard_errors = np.sqrt(np.diag(vcov_beta))\n\n# Show results in a table\nsummary_table = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': standard_errors\n}, index=X.columns)\n\nsummary_table\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.508925\n0.185670\n\n\nage\n1.486199\n0.139210\n\n\nage_squared\n-0.297048\n0.025840\n\n\niscustomer\n0.207591\n0.032190\n\n\nNortheast\n0.029170\n0.046083\n\n\nNorthwest\n-0.017574\n0.055659\n\n\nSouth\n0.056562\n0.054619\n\n\nSouthwest\n0.050576\n0.048116\n\n\n\n\n\n\n\nThe Poisson regression estimates how the expected number of patents awarded to a firm varies with its characteristics. To improve numerical stability during optimization, the age variable was scaled by a factor of 10 (i.e., a one-unit change in age represents a 10-year difference).\nThe coefficient for age is 1.49, and for age_squared is ‚àí0.30, indicating a non-linear relationship: as firms age, their expected number of patents initially increases, but the rate of increase slows and eventually declines with age.\nThe coefficient on iscustomer is 0.208, meaning that, holding all else equal, firms that use Blueprinty‚Äôs software have a materially higher expected patent count compared to non-customers.\nRegional effects appear minimal, with all region dummy coefficients close to zero and standard errors indicating low statistical significance relative to the reference region.\nNow let‚Äôs compare results against the statsmodels ‚Äòsm.GLM()‚Äô function.\n\nimport statsmodels.api as sm\n\n# Recreate X_sm and ensure it's numeric\nX_sm = sm.add_constant(X.drop(columns='intercept'), has_constant='add')\nX_sm = X_sm.astype(float)  # &lt;== This line prevents the object dtype error\n\n# Also ensure Y is numeric\nY_numeric = Y.astype(float)\n\n# Fit GLM model\nglm_model = sm.GLM(Y_numeric, X_sm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary\n# Create a clean summary table\nsummary_table = pd.DataFrame({\n    'Coefficient': glm_results.params,\n    'Std. Error': glm_results.bse,\n    'z value': glm_results.tvalues,\n    'P&gt;|z|': glm_results.pvalues\n})\n\n# Optional: round for readability\nsummary_table = summary_table.round(4)\n\nsummary_table\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz value\nP&gt;|z|\n\n\n\n\nconst\n-0.5089\n0.1832\n-2.7783\n0.0055\n\n\nage\n1.4862\n0.1387\n10.7162\n0.0000\n\n\nage_squared\n-0.2970\n0.0258\n-11.5132\n0.0000\n\n\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n\n\nNortheast\n0.0292\n0.0436\n0.6686\n0.5037\n\n\nNorthwest\n-0.0176\n0.0538\n-0.3268\n0.7438\n\n\nSouth\n0.0566\n0.0527\n1.0740\n0.2828\n\n\nSouthwest\n0.0506\n0.0472\n1.0716\n0.2839\n\n\n\n\n\n\n\nThe results closely matched those from the custom maximum likelihood estimation (MLE) approach, with all coefficients and standard errors agreeing to several decimal places. This confirms that our manually coded log-likelihood function and optimization routine are correctly specified.\nWe‚Äôll now try to quantify the effect of using Blueprinty‚Äôs software by simulating predicted patent counts for the firms in our data in two scenarios, one where all firms are non-customers, and one where all were customers, holding all else constant.\n\n# Create counterfactual datasets\nX_0 = X.copy()\nX_1 = X.copy()\n\n# Set iscustomer to 0 and 1 respectively\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n# Ensure matrix and beta vector are both float64\nX0_mat = X_0.to_numpy().astype(np.float64)\nX1_mat = X_1.to_numpy().astype(np.float64)\nbeta_vec = np.asarray(beta_hat, dtype=np.float64)  # just to be safe\n\n# Predict expected patent counts\ny_pred_0 = np.exp(np.dot(X0_mat, beta_vec))\ny_pred_1 = np.exp(np.dot(X1_mat, beta_vec))\n\neffect_diff = y_pred_1 - y_pred_0\navg_effect = np.mean(effect_diff)\n\nprint(f\"Average predicted increase in patent count from using Blueprinty's software: {avg_effect:.3f}\")\n\nAverage predicted increase in patent count from using Blueprinty's software: 0.793\n\n\nWe find that the average predicted number of patents increased by about 0.793 when firms used Blueprinty‚Äôs software. Resultingly, Blueprinty customers are expected to recieve one additional patent over a five year period compared to similar non-customers."
  },
  {
    "objectID": "projects/hw2/index.html#airbnb-case-study",
    "href": "projects/hw2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped off 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nLet‚Äôs load the data and clean it.\n\ndf2 = pd.read_csv('airbnb.csv')\n\ndf2.info()\ndf2.describe()\ndf2.isnull().sum()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 40628 entries, 0 to 40627\nData columns (total 14 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   Unnamed: 0                 40628 non-null  int64  \n 1   id                         40628 non-null  int64  \n 2   days                       40628 non-null  int64  \n 3   last_scraped               40628 non-null  object \n 4   host_since                 40593 non-null  object \n 5   room_type                  40628 non-null  object \n 6   bathrooms                  40468 non-null  float64\n 7   bedrooms                   40552 non-null  float64\n 8   price                      40628 non-null  int64  \n 9   number_of_reviews          40628 non-null  int64  \n 10  review_scores_cleanliness  30433 non-null  float64\n 11  review_scores_location     30374 non-null  float64\n 12  review_scores_value        30372 non-null  float64\n 13  instant_bookable           40628 non-null  object \ndtypes: float64(5), int64(5), object(4)\nmemory usage: 4.3+ MB\n\n\nUnnamed: 0                       0\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\nLet‚Äôs drop the null values in the non-review score columns. Since we‚Äôre using number of reviews as a proxy for bookings, review_scores_‚Ä¶ are not relevant to predict bookings since they happen after the stay. Accordingly, we won‚Äôt remove all those rows with null review scores.\n\n# Keep only relevant, pre-booking predictors\ncols = ['number_of_reviews', 'days', 'room_type', 'bathrooms', 'bedrooms', 'price', 'instant_bookable']\ndf_model = df2[cols].dropna().copy()\n\nLet‚Äôs now prepare the X matrix and outcome Y variables for another Poisson regression model.\n\n# Rescale numeric variables to improve model stability\ndf_model['days'] = df_model['days'] / 100\ndf_model['price'] = df_model['price'] / 100\ndf_model['bathrooms'] = df_model['bathrooms'] / 10\ndf_model['bedrooms'] = df_model['bedrooms'] / 10\n\n# Convert instant_bookable to binary\ndf_model['instant_bookable'] = (df_model['instant_bookable'] == 't').astype(int)\n\n# One-hot encode room_type (drop first category to avoid multicollinearity)\nroom_dummies = pd.get_dummies(df_model['room_type'], drop_first=True)\n\n# Combine into design matrix\nX = pd.concat([\n    df_model[['days', 'bathrooms', 'bedrooms', 'price', 'instant_bookable']],\n    room_dummies\n], axis=1)\n\n# Outcome variable\nY = df_model['number_of_reviews']\n\nX.head()\n\n\n\n\n\n\n\n\ndays\nbathrooms\nbedrooms\nprice\ninstant_bookable\nPrivate room\nShared room\n\n\n\n\n0\n31.30\n0.1\n0.1\n0.59\n0\nTrue\nFalse\n\n\n1\n31.27\n0.1\n0.0\n2.30\n0\nFalse\nFalse\n\n\n2\n30.50\n0.1\n0.1\n1.50\n0\nTrue\nFalse\n\n\n3\n30.38\n0.1\n0.1\n0.89\n0\nFalse\nFalse\n\n\n5\n29.81\n0.1\n0.1\n2.12\n0\nFalse\nFalse\n\n\n\n\n\n\n\nLet‚Äôs fit the Poisson model with statsmodels.\n\nimport statsmodels.api as sm\n\n# Add intercept\nX_sm = sm.add_constant(X)\n\nX_sm = sm.add_constant(X).astype(float)\nY_sm = Y.astype(float)\n\n# Fit the model\nglm_model = sm.GLM(Y, X_sm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Print results\n# Create a clean summary table\nsummary_table = pd.DataFrame({\n    'Coefficient': glm_results.params,\n    'Std. Error': glm_results.bse,\n    'z value': glm_results.tvalues,\n    'P&gt;|z|': glm_results.pvalues\n})\n\n# Optional: round for readability\nsummary_table = summary_table.round(4)\n\nsummary_table\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz value\nP&gt;|z|\n\n\n\n\nconst\n2.7715\n0.0045\n622.5201\n0.0\n\n\ndays\n0.0050\n0.0000\n140.8254\n0.0\n\n\nbathrooms\n-1.0642\n0.0384\n-27.7226\n0.0\n\n\nbedrooms\n0.9817\n0.0200\n48.9806\n0.0\n\n\nprice\n-0.0461\n0.0012\n-37.5604\n0.0\n\n\ninstant_bookable\n0.3748\n0.0029\n130.0457\n0.0\n\n\nPrivate room\n-0.1531\n0.0029\n-53.6916\n0.0\n\n\nShared room\n-0.4080\n0.0087\n-47.1200\n0.0\n\n\n\n\n\n\n\nBased on the output above, the Poisson regression results show several strong predictors of bookings (per the reviews proxy) Listings that are instant bookable recieve almost 50% more bookings, holding other features constant. More bedrooms are also strongly associated with more bookings, whereas higher prices are associated with fewer bookings. Listings classified as private rooms or shared rooms have fewer bookings compared to entire homes. While number of days a listing has been active is positively associated with bookings, this could be a reflection of the cumulative exposure over time."
  },
  {
    "objectID": "projects/hw4/index.html",
    "href": "projects/hw4/index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "In this post, I implement the K-Means clustering algorithm from scratch and apply it to the Palmer Penguins dataset.\nThe goal is to: 1. Write the K-Means algorithm by hand in Python. 2. Visualize how it works on real data. 3. Compare it with Python‚Äôs built-in KMeans implementation. 4. Evaluate model performance using within-cluster sum of squares (WCSS) and silhouette scores. 5. Determine the ‚Äúright‚Äù number of clusters.\nWe‚Äôll use two features: bill length and flipper length ‚Äî a natural fit for 2D visualization of clustering in action.\n\nHow the Algorithm Works:\nK-Means is an unsupervised learning algorithm that partitions data into K clusters. The algorithm seeks to minimize the within-cluster sum of squares (WCSS) and follows these steps:\n\nRandomly initialize K centroids.\nAssign each data point to the nearest centroid.\nRecompute each centroid based on current cluster assignments.\nRepeat until centroids converge or max iterations is reached.\n\n\n\n\nWe define functions to: - Initialize centroids randomly - Assign each point to the nearest centroid - Update centroids using the mean of assigned points - Iterate until convergence\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef initialize_centroids(X, k):\n    indices = np.random.choice(len(X), size=k, replace=False)\n    return X[indices]\n\ndef assign_clusters(X, centroids):\n    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n    return np.argmin(distances, axis=1)\n\ndef update_centroids(X, labels, k):\n    return np.array([X[labels == i].mean(axis=0) for i in range(k)])\n\ndef kmeans(X, k, max_iters=100, tol=1e-4):\n    centroids = initialize_centroids(X, k)\n    for i in range(max_iters):\n        old_centroids = centroids\n        labels = assign_clusters(X, centroids)\n        centroids = update_centroids(X, labels, k)\n        if np.allclose(centroids, old_centroids, atol=tol):\n            break\n    return centroids, labels\n\n\nNow let‚Äôs test it on the Palmer Penguins dataset.\n\n\nShow code\n#| code-fold: true\n#| code-summary: \"Show code\"\nimport seaborn as sns\nimport pandas as pd\n\npenguins = sns.load_dataset(\"penguins\").dropna(subset=[\"bill_length_mm\", \"flipper_length_mm\"])\nX = penguins[[\"bill_length_mm\", \"flipper_length_mm\"]].values\n\nplt.figure(figsize=(6, 5))\nplt.scatter(X[:, 0], X[:, 1], c='gray', edgecolor='black')\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.title(\"Palmer Penguins: Bill Length vs Flipper Length\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nTo better understand how K-Means converges, I modified the algorithm to store the centroids and cluster labels at each iteration.\nExecuting such visualization here:\n\n\nShow code\nfrom matplotlib.animation import FuncAnimation\n\ndef kmeans_with_plot(X, k, max_iters=10):\n    centroids = initialize_centroids(X, k)\n    history = []\n    for i in range(max_iters):\n        labels = assign_clusters(X, centroids)\n        history.append((centroids.copy(), labels.copy()))\n        new_centroids = update_centroids(X, labels, k)\n        if np.allclose(new_centroids, centroids):\n            break\n        centroids = new_centroids\n    history.append((centroids.copy(), labels.copy()))\n    return history\n\n# Run it and store the steps\nhistory = kmeans_with_plot(X, k=3)\n\n# Animate it\nfig, ax = plt.subplots(figsize=(6, 5))\n\ndef animate(i):\n    ax.clear()\n    centroids, labels = history[i]\n    for j in range(len(centroids)):\n        ax.scatter(X[labels == j, 0], X[labels == j, 1], label=f\"Cluster {j}\")\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=100, label='Centroids')\n    ax.set_title(f\"K-Means Iteration {i}\")\n    ax.set_xlabel(\"Bill Length (mm)\")\n    ax.set_ylabel(\"Flipper Length (mm)\")\n    ax.legend()\n\nani = FuncAnimation(fig, animate, frames=len(history), interval=1000, repeat=False)\n\n# Save the animation to your output folder (this file must exist in same dir)\nani.save(\"kmeans_penguins_animation.mp4\", writer='ffmpeg')\n\n\n\n\n\n\n\n\n\n\nVideo\n\nEach frame above shows the K-Means algorithm as it: - Reassigns data points to the nearest centroid - Moves centroids to the mean of their assigned points - Gradually stabilizes and stops updating\nThis visualization helps demonstrate how K-Means converges over a few iterations. It‚Äôs especially helpful for building intuition around how cluster boundaries emerge.\n\n\nShow code\nk = 3\ncentroids, labels = kmeans(X, k)\n\nplt.figure(figsize=(6, 5))\nfor i in range(k):\n    cluster_points = X[labels == i]\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i}')\nplt.scatter(centroids[:, 0], centroids[:, 1], color='black', marker='x', s=100, label='Centroids')\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.title(\"Clusters Found by Custom K-Means\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe use the seaborn penguins dataset and select two continuous variables: bill length and flipper length.\nThis makes it easy to visualize clustering in 2D.\nWe choose k=3 and run our K-Means function.\nEach cluster is color-coded, and the final centroids are marked with black X‚Äôs.\n\n\n\nHere‚Äôs a comparison using sklearn‚Äôs KMeans. The cluster separation and centroid locations are nearly identical, validating our implementation.\n\n\nShow code\nfrom sklearn.cluster import KMeans\n\nkmeans_sklearn = KMeans(n_clusters=3, random_state=42).fit(X)\nlabels_sklearn = kmeans_sklearn.labels_\ncentroids_sklearn = kmeans_sklearn.cluster_centers_\n\nplt.figure(figsize=(6, 5))\nfor i in range(3):\n    plt.scatter(X[labels_sklearn == i, 0], X[labels_sklearn == i, 1], label=f'Cluster {i}')\nplt.scatter(centroids_sklearn[:, 0], centroids_sklearn[:, 1], color='red', marker='x', s=100, label='Centroids')\nplt.title(\"Clusters from sklearn KMeans\")\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWe calculate: - Within-Cluster Sum of Squares (WCSS): Lower is better; the ‚Äúelbow‚Äù suggests the optimal k. - Silhouette Score: Measures cohesion and separation; higher is better.\n\n\nShow code\nfrom sklearn.metrics import silhouette_score\n\nwcss = []\nsilhouettes = []\nK_range = range(2, 8)\n\nfor k in K_range:\n    km = KMeans(n_clusters=k, random_state=42).fit(X)\n    wcss.append(km.inertia_)\n    silhouettes.append(silhouette_score(X, km.labels_))\n\nplt.figure()\nplt.plot(K_range, wcss, marker='o')\nplt.title(\"Elbow Plot (WCSS)\")\nplt.xlabel(\"Number of Clusters (K)\")\nplt.ylabel(\"Within-Cluster Sum of Squares\")\nplt.show()\n\n\n\n\n\n\n\n\n\nShow code\nplt.figure()\nplt.plot(K_range, silhouettes, marker='s')\nplt.title(\"Silhouette Score vs K\")\nplt.xlabel(\"Number of Clusters (K)\")\nplt.ylabel(\"Silhouette Score\")\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this case, both metrics suggest k=3 is a strong choice ‚Äî consistent with our earlier result.\n\n\n\nI implemented K-Means from scratch and applied it to the Palmer Penguins dataset. I also compared my implementation to the built-in version from sklearn, and evaluated the results using WCSS and silhouette scores.\nThe optimal number of clusters based on both metrics appears to be 3, which aligns well with the natural grouping of the data.\nThis hands-on implementation helped solidify my understanding of clustering mechanics ‚Äî and provided great visuals for exploring the algorithm step-by-step."
  },
  {
    "objectID": "projects/hw4/index.html#a.-k-means",
    "href": "projects/hw4/index.html#a.-k-means",
    "title": "Machine Learning",
    "section": "1a. K-Means",
    "text": "1a. K-Means\ntodo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can ‚Äúsee‚Äù the algorithm working. Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables. Compare your results to the built-in kmeans function in R or Python.\ntodo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,‚Ä¶,7). What is the ‚Äúright‚Äù number of clusters as suggested by these two metrics?\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this.\nI implement the K-Means clustering algorithm from scratch and apply it to the Palmer Penguins dataset.\nThe goal is to: 1. Write the K-Means algorithm by hand in Python. 2. Visualize how it works on real data. 3. Compare it with Python‚Äôs built-in KMeans implementation. 4. Evaluate model performance using within-cluster sum of squares (WCSS) and silhouette scores. 5. Determine the ‚Äúright‚Äù number of clusters.\nWe‚Äôll use two features: bill length and flipper length ‚Äî a natural fit for 2D visualization of clustering in action.\nK-Means is an unsupervised machine learning algorithm that partitions data into K clusters, where each data point belongs to the cluster with the nearest mean.\nThe algorithm follows these steps: 1. Randomly initialize K centroids. 2. Assign each data point to the closest centroid (based on Euclidean distance). 3. Recompute centroids as the mean of points assigned to each cluster. 4. Repeat steps 2‚Äì3 until convergence (centroids stop moving or max iterations is reached).\nIt‚Äôs a classic example of an iterative optimization algorithm that seeks to minimize the total within-cluster sum of squares (WCSS).\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef initialize_centroids(X, k):\n    \"\"\"Randomly initialize k centroids from the data.\"\"\"\n    indices = np.random.choice(len(X), size=k, replace=False)\n    return X[indices]\n\ndef assign_clusters(X, centroids):\n    \"\"\"Assign each point to the nearest centroid.\"\"\"\n    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n    return np.argmin(distances, axis=1)\n\ndef update_centroids(X, labels, k):\n    \"\"\"Recompute centroids as the mean of all points in each cluster.\"\"\"\n    return np.array([X[labels == i].mean(axis=0) for i in range(k)])\n\ndef kmeans(X, k, max_iters=100, tol=1e-4):\n    \"\"\"Full K-Means clustering algorithm.\"\"\"\n    centroids = initialize_centroids(X, k)\n    for i in range(max_iters):\n        old_centroids = centroids\n        labels = assign_clusters(X, centroids)\n        centroids = update_centroids(X, labels, k)\n        if np.allclose(centroids, old_centroids, atol=tol):\n            break\n    return centroids, labels\n\n\nWe define functions to: - Initialize centroids randomly - Assign each point to the nearest centroid - Update centroids using the mean of assigned points - Iterate until convergence\nNow let‚Äôs test it on the Palmer Penguins dataset.\n\n\nShow code\nimport seaborn as sns\nimport pandas as pd\n\n# Load the penguins dataset\npenguins = sns.load_dataset(\"penguins\").dropna(subset=[\"bill_length_mm\", \"flipper_length_mm\"])\nX = penguins[[\"bill_length_mm\", \"flipper_length_mm\"]].values\n\n# Visualize raw data\nplt.figure(figsize=(6, 5))\nplt.scatter(X[:, 0], X[:, 1], c='gray', edgecolor='black')\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.title(\"Palmer Penguins: Bill Length vs Flipper Length\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWe use the seaborn penguins dataset and select two continuous variables: bill length and flipper length.\nThis makes it easy to visualize clustering in 2D.\n\n\nShow code\n# Set number of clusters\nk = 3\n\n# Run custom K-Means\ncentroids, labels = kmeans(X, k)\n\n# Plot clusters\nplt.figure(figsize=(6, 5))\nfor i in range(k):\n    cluster_points = X[labels == i]\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i}')\nplt.scatter(centroids[:, 0], centroids[:, 1], color='black', marker='x', s=100, label='Centroids')\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.title(\"Clusters Found by Custom K-Means\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe choose k=3 and run our K-Means function.\nEach cluster is color-coded, and the final centroids are marked with black X‚Äôs.\n\n\nShow code\nfrom sklearn.cluster import KMeans\n\nkmeans_sklearn = KMeans(n_clusters=3, random_state=42).fit(X)\nlabels_sklearn = kmeans_sklearn.labels_\ncentroids_sklearn = kmeans_sklearn.cluster_centers_\n\n# Plot built-in results\nplt.figure(figsize=(6, 5))\nfor i in range(3):\n    plt.scatter(X[labels_sklearn == i, 0], X[labels_sklearn == i, 1], label=f'Cluster {i}')\nplt.scatter(centroids_sklearn[:, 0], centroids_sklearn[:, 1], color='red', marker='x', s=100, label='Centroids')\nplt.title(\"Clusters from sklearn KMeans\")\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nHere‚Äôs a comparison using sklearn‚Äôs KMeans. The cluster separation and centroid locations are nearly identical, validating our implementation.\n\n\nShow code\nfrom sklearn.metrics import silhouette_score\n\nwcss = []\nsilhouettes = []\n\nK_range = range(2, 8)\nfor k in K_range:\n    km = KMeans(n_clusters=k, random_state=42).fit(X)\n    wcss.append(km.inertia_)\n    silhouettes.append(silhouette_score(X, km.labels_))\n\n# Plot WCSS (Elbow)\nplt.figure()\nplt.plot(K_range, wcss, marker='o')\nplt.title(\"Elbow Plot (WCSS)\")\nplt.xlabel(\"Number of Clusters (K)\")\nplt.ylabel(\"Within-Cluster Sum of Squares\")\nplt.show()\n\n# Plot Silhouette Scores\nplt.figure()\nplt.plot(K_range, silhouettes, marker='s')\nplt.title(\"Silhouette Score vs K\")\nplt.xlabel(\"Number of Clusters (K)\")\nplt.ylabel(\"Silhouette Score\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe calculate: - Within-Cluster Sum of Squares (WCSS): Lower is better; the ‚Äúelbow‚Äù suggests the optimal k. - Silhouette Score: Measures cohesion and separation; higher is better.\nIn this case, both metrics suggest k=3 is a strong choice ‚Äî consistent with our earlier result.\nI implemented K-Means from scratch and applied it to the Palmer Penguins dataset. I also compared my implementation to the built-in version from sklearn, and evaluated the results using WCSS and silhouette scores.\nThe optimal number of clusters based on both metrics appears to be 3, which aligns well with the natural grouping of the data.\nThis hands-on implementation helped solidify my understanding of clustering mechanics ‚Äî and provided great visuals for exploring the algorithm step-by-step."
  },
  {
    "objectID": "projects/hw4/index.html#b.-key-drivers-analysis",
    "href": "projects/hw4/index.html#b.-key-drivers-analysis",
    "title": "Machine Learning",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, ‚Äúusefulness‚Äù, Shapley values for a linear regression, Johnson‚Äôs relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations ‚Äúby hand.‚Äù\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  },
  {
    "objectID": "projects/hw4/index.html#k-means-clustering-implemented-from-scratch",
    "href": "projects/hw4/index.html#k-means-clustering-implemented-from-scratch",
    "title": "Machine Learning",
    "section": "",
    "text": "In this post, I implement the K-Means clustering algorithm from scratch and apply it to the Palmer Penguins dataset.\nThe goal is to: 1. Write the K-Means algorithm by hand in Python. 2. Visualize how it works on real data. 3. Compare it with Python‚Äôs built-in KMeans implementation. 4. Evaluate model performance using within-cluster sum of squares (WCSS) and silhouette scores. 5. Determine the ‚Äúright‚Äù number of clusters.\nWe‚Äôll use two features: bill length and flipper length ‚Äî a natural fit for 2D visualization of clustering in action.\n\nHow the Algorithm Works:\nK-Means is an unsupervised learning algorithm that partitions data into K clusters. The algorithm seeks to minimize the within-cluster sum of squares (WCSS) and follows these steps:\n\nRandomly initialize K centroids.\nAssign each data point to the nearest centroid.\nRecompute each centroid based on current cluster assignments.\nRepeat until centroids converge or max iterations is reached.\n\n\n\n\nWe define functions to: - Initialize centroids randomly - Assign each point to the nearest centroid - Update centroids using the mean of assigned points - Iterate until convergence\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef initialize_centroids(X, k):\n    indices = np.random.choice(len(X), size=k, replace=False)\n    return X[indices]\n\ndef assign_clusters(X, centroids):\n    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n    return np.argmin(distances, axis=1)\n\ndef update_centroids(X, labels, k):\n    return np.array([X[labels == i].mean(axis=0) for i in range(k)])\n\ndef kmeans(X, k, max_iters=100, tol=1e-4):\n    centroids = initialize_centroids(X, k)\n    for i in range(max_iters):\n        old_centroids = centroids\n        labels = assign_clusters(X, centroids)\n        centroids = update_centroids(X, labels, k)\n        if np.allclose(centroids, old_centroids, atol=tol):\n            break\n    return centroids, labels\n\n\nNow let‚Äôs test it on the Palmer Penguins dataset.\n\n\nShow code\n#| code-fold: true\n#| code-summary: \"Show code\"\nimport seaborn as sns\nimport pandas as pd\n\npenguins = sns.load_dataset(\"penguins\").dropna(subset=[\"bill_length_mm\", \"flipper_length_mm\"])\nX = penguins[[\"bill_length_mm\", \"flipper_length_mm\"]].values\n\nplt.figure(figsize=(6, 5))\nplt.scatter(X[:, 0], X[:, 1], c='gray', edgecolor='black')\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.title(\"Palmer Penguins: Bill Length vs Flipper Length\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nTo better understand how K-Means converges, I modified the algorithm to store the centroids and cluster labels at each iteration.\nExecuting such visualization here:\n\n\nShow code\nfrom matplotlib.animation import FuncAnimation\n\ndef kmeans_with_plot(X, k, max_iters=10):\n    centroids = initialize_centroids(X, k)\n    history = []\n    for i in range(max_iters):\n        labels = assign_clusters(X, centroids)\n        history.append((centroids.copy(), labels.copy()))\n        new_centroids = update_centroids(X, labels, k)\n        if np.allclose(new_centroids, centroids):\n            break\n        centroids = new_centroids\n    history.append((centroids.copy(), labels.copy()))\n    return history\n\n# Run it and store the steps\nhistory = kmeans_with_plot(X, k=3)\n\n# Animate it\nfig, ax = plt.subplots(figsize=(6, 5))\n\ndef animate(i):\n    ax.clear()\n    centroids, labels = history[i]\n    for j in range(len(centroids)):\n        ax.scatter(X[labels == j, 0], X[labels == j, 1], label=f\"Cluster {j}\")\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=100, label='Centroids')\n    ax.set_title(f\"K-Means Iteration {i}\")\n    ax.set_xlabel(\"Bill Length (mm)\")\n    ax.set_ylabel(\"Flipper Length (mm)\")\n    ax.legend()\n\nani = FuncAnimation(fig, animate, frames=len(history), interval=1000, repeat=False)\n\n# Save the animation to your output folder (this file must exist in same dir)\nani.save(\"kmeans_penguins_animation.mp4\", writer='ffmpeg')\n\n\n\n\n\n\n\n\n\n\nVideo\n\nEach frame above shows the K-Means algorithm as it: - Reassigns data points to the nearest centroid - Moves centroids to the mean of their assigned points - Gradually stabilizes and stops updating\nThis visualization helps demonstrate how K-Means converges over a few iterations. It‚Äôs especially helpful for building intuition around how cluster boundaries emerge.\n\n\nShow code\nk = 3\ncentroids, labels = kmeans(X, k)\n\nplt.figure(figsize=(6, 5))\nfor i in range(k):\n    cluster_points = X[labels == i]\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i}')\nplt.scatter(centroids[:, 0], centroids[:, 1], color='black', marker='x', s=100, label='Centroids')\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.title(\"Clusters Found by Custom K-Means\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe use the seaborn penguins dataset and select two continuous variables: bill length and flipper length.\nThis makes it easy to visualize clustering in 2D.\nWe choose k=3 and run our K-Means function.\nEach cluster is color-coded, and the final centroids are marked with black X‚Äôs.\n\n\n\nHere‚Äôs a comparison using sklearn‚Äôs KMeans. The cluster separation and centroid locations are nearly identical, validating our implementation.\n\n\nShow code\nfrom sklearn.cluster import KMeans\n\nkmeans_sklearn = KMeans(n_clusters=3, random_state=42).fit(X)\nlabels_sklearn = kmeans_sklearn.labels_\ncentroids_sklearn = kmeans_sklearn.cluster_centers_\n\nplt.figure(figsize=(6, 5))\nfor i in range(3):\n    plt.scatter(X[labels_sklearn == i, 0], X[labels_sklearn == i, 1], label=f'Cluster {i}')\nplt.scatter(centroids_sklearn[:, 0], centroids_sklearn[:, 1], color='red', marker='x', s=100, label='Centroids')\nplt.title(\"Clusters from sklearn KMeans\")\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWe calculate: - Within-Cluster Sum of Squares (WCSS): Lower is better; the ‚Äúelbow‚Äù suggests the optimal k. - Silhouette Score: Measures cohesion and separation; higher is better.\n\n\nShow code\nfrom sklearn.metrics import silhouette_score\n\nwcss = []\nsilhouettes = []\nK_range = range(2, 8)\n\nfor k in K_range:\n    km = KMeans(n_clusters=k, random_state=42).fit(X)\n    wcss.append(km.inertia_)\n    silhouettes.append(silhouette_score(X, km.labels_))\n\nplt.figure()\nplt.plot(K_range, wcss, marker='o')\nplt.title(\"Elbow Plot (WCSS)\")\nplt.xlabel(\"Number of Clusters (K)\")\nplt.ylabel(\"Within-Cluster Sum of Squares\")\nplt.show()\n\n\n\n\n\n\n\n\n\nShow code\nplt.figure()\nplt.plot(K_range, silhouettes, marker='s')\nplt.title(\"Silhouette Score vs K\")\nplt.xlabel(\"Number of Clusters (K)\")\nplt.ylabel(\"Silhouette Score\")\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this case, both metrics suggest k=3 is a strong choice ‚Äî consistent with our earlier result.\n\n\n\nI implemented K-Means from scratch and applied it to the Palmer Penguins dataset. I also compared my implementation to the built-in version from sklearn, and evaluated the results using WCSS and silhouette scores.\nThe optimal number of clusters based on both metrics appears to be 3, which aligns well with the natural grouping of the data.\nThis hands-on implementation helped solidify my understanding of clustering mechanics ‚Äî and provided great visuals for exploring the algorithm step-by-step."
  },
  {
    "objectID": "projects/hw4/index.html#key-drivers-analysis-what-influences-satisfaction",
    "href": "projects/hw4/index.html#key-drivers-analysis-what-influences-satisfaction",
    "title": "Machine Learning",
    "section": "Key Drivers Analysis: What Influences Satisfaction",
    "text": "Key Drivers Analysis: What Influences Satisfaction\nIn this section, I replicate a key drivers analysis from class to explore which perceptual attributes most strongly explain satisfaction for Brand 1.\nThe dataset includes survey responses where each row represents a customer and the columns include: - A set of perception variables (e.g., trust, ease of use, appeal) - A satisfaction score (the target variable we aim to explain)\nI perform a Key Drivers Analysis using six different variable importance metrics, each of which offers a unique lens for interpreting what matters most: 1. Pearson Correlation 2. Standardized Regression Coefficients 3. Usefulness (R¬≤ from univariate regressions) 4. Shapley Values (LMG) 5. Johnson‚Äôs Relative Weights 6. Random Forest Gini Importance\nThis helps compare statistical, machine learning, and variance decomposition approaches to identifying the strongest drivers of customer satisfaction.\nFor simplicity and clarity, I focus on Brand 1 only.\nLet‚Äôs load and filter the data.\n\n\nLoad and filter Brand 1 data\nimport pandas as pd\n\n# Load the dataset\ndf = pd.read_csv(\"data_for_drivers_analysis.csv\")\n\n# Filter to just Brand 1\ndf_brand1 = df[df[\"brand\"] == 1].copy()\n\n# Define predictors and target\nX = df_brand1.drop(columns=[\"brand\", \"id\", \"satisfaction\"])\ny = df_brand1[\"satisfaction\"]\n\n\n\nPearson Correlation\nThe Pearson correlation measures the linear relationship between each perception variable and satisfaction. It ranges from -1 to +1: - A positive value means the perception increases as satisfaction increases. - A larger absolute value means a stronger relationship.\nThis is a simple and intuitive measure of importance, though it doesn‚Äôt account for interactions or multicollinearity.\n\n\nCompute Pearson correlations\nimport numpy as np\nfrom IPython.display import display\n\n# Compute correlations between each column in X and y\npearson_corr = X.corrwith(y)\n\n# Convert to DataFrame for later merging\npearson_df = pearson_corr.to_frame(name=\"Pearson Correlation\")\npearson_df = pearson_df.sort_values(by=\"Pearson Correlation\", ascending=False)\ndisplay(pearson_df)\n\n\n           Pearson Correlation\nappealing             0.213676\nrewarding             0.201421\neasy                  0.174373\nimpact                0.170194\ntrust                 0.161155\npopular               0.150364\nservice               0.146953\ndiffers               0.092293\nbuild                 0.048046\n\n\n\n\nStandardized Regression Coefficients\nThis method involves fitting a linear regression on standardized variables (z-scores). Standardizing ensures that all predictors are on the same scale, so we can directly compare the magnitude of their coefficients.\nLarger coefficients (in absolute value) indicate variables that contribute more to predicting satisfaction. This approach adjusts for collinearity better than Pearson correlation, but it still assumes a linear and additive relationship.\nBelow are the standardized coefficients for each perception variable:\n\n\nStandardized regression coefficients\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardize X and y\nscaler_X = StandardScaler()\nscaler_y = StandardScaler()\nX_std = scaler_X.fit_transform(X)\ny_std = scaler_y.fit_transform(y.values.reshape(-1, 1)).ravel()\n\n# Fit linear regression\nlr = LinearRegression()\nlr.fit(X_std, y_std)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\nStandardized regression coefficients\n# Store coefficients\nstd_coef_df = pd.DataFrame({\n    \"Standardized Coefficient\": lr.coef_\n}, index=X.columns).sort_values(by=\"Standardized Coefficient\", ascending=False)\n\ndisplay(std_coef_df)\n\n\n           Standardized Coefficient\nrewarding                  0.127456\nappealing                  0.126563\nimpact                     0.082937\npopular                    0.057477\ntrust                      0.047231\neasy                       0.045333\nservice                    0.041731\ndiffers                   -0.063249\nbuild                     -0.134827\n\n\n\n\nUsefulness: Univariate R¬≤\n‚ÄúUsefulness‚Äù is measured by the R¬≤ value from a simple linear regression using each predictor individually. It tells us how much of the variance in satisfaction can be explained by that predictor alone ‚Äî without adjusting for overlap with other variables.\nThis metric is easy to interpret and useful for identifying high-signal variables, though it can be misleading if predictors are strongly correlated with each other.\nHere are the results for Brand 1:\n\n\nUnivariate R¬≤ (Usefulness)\nfrom sklearn.linear_model import LinearRegression\n\nr2_scores = {}\n\nfor col in X.columns:\n    model = LinearRegression()\n    model.fit(X[[col]], y)\n    r2_scores[col] = model.score(X[[col]], y)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\nUnivariate R¬≤ (Usefulness)\nusefulness_df = pd.DataFrame.from_dict(r2_scores, orient=\"index\", columns=[\"Usefulness (R¬≤)\"])\nusefulness_df = usefulness_df.sort_values(by=\"Usefulness (R¬≤)\", ascending=False)\n\ndisplay(usefulness_df)\n\n\n           Usefulness (R¬≤)\nappealing         0.045657\nrewarding         0.040571\neasy              0.030406\nimpact            0.028966\ntrust             0.025971\npopular           0.022609\nservice           0.021595\ndiffers           0.008518\nbuild             0.002308\n\n\n\n\nSummary of the First Three Metrics\nSo far, we‚Äôve explored three statistical perspectives on what drives satisfaction:\n\nPearson Correlation gives us a simple view of linear relationships.\nStandardized Coefficients control for overlap between variables in a multivariate setting.\nUsefulness (R¬≤) shows how much variance each variable explains on its own.\n\nDespite their different methodologies, the same few variables consistently emerge as top drivers: - appealing and rewarding show up at the top in all three. - impact, easy, and trust are also moderately strong across metrics. - build scores low across the board ‚Äî potentially indicating that customers don‚Äôt associate this dimension with satisfaction.\nThis early convergence increases confidence in our results ‚Äî though we‚Äôll validate further with machine learning and variance decomposition techniques next.\n\n\nShapley Values (LMG Decomposition in R)\nTo correctly run Shapley value (LMG) decomposition for a linear regression model, I used R‚Äôs relaimpo package. This avoids the misbehavior I encountered using Python‚Äôs dominance-analysis package, which defaults to logistic regression.\nThe table below shows the relative importance (as a % of R¬≤) each predictor contributes based on averaging over all possible model combinations.\n\n\n          result$lmg\ntrust     0.08414815\nbuild     0.07901473\ndiffers   0.02771814\neasy      0.09853352\nappealing 0.22634684\nrewarding 0.21443335\npopular   0.08908973\nservice   0.06453611\nimpact    0.11617944\n\n\nTo accurately compute Shapley values (LMG decomposition) for a linear model, I used the relaimpo package in R. This method attributes each predictor‚Äôs average contribution to the model‚Äôs R¬≤ by considering all possible combinations of variable orderings.\nHere are the results for Brand 1:\n\n\n\nVariable\nLMG (Relative Importance)\n\n\n\n\nappealing\n0.226\n\n\nrewarding\n0.214\n\n\nimpact\n0.116\n\n\neasy\n0.099\n\n\ntrust\n0.084\n\n\npopular\n0.089\n\n\nbuild\n0.079\n\n\nservice\n0.065\n\n\ndiffers\n0.028\n\n\n\nTogether, these add up to 1.0 (100% of model R¬≤), showing how each perception variable contributes to explaining satisfaction.\nAs with earlier metrics, appealing and rewarding again stand out as the most important drivers ‚Äî further validated by this comprehensive decomposition.\n\n\nJohnson‚Äôs Relative Weights\nJohnson‚Äôs Relative Weights decompose the model‚Äôs R¬≤ into contributions from each predictor by first transforming them into orthogonal variables (uncorrelated components), then mapping those contributions back.\nThis approach is computationally efficient and handles multicollinearity well ‚Äî while still providing results on the same scale as R¬≤.\n\n\nCompute Johnson‚Äôs Relative Weights\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Standardize predictors\nX_std = (X - X.mean()) / X.std()\n\n# Step 1: Correlation matrix and eigen decomposition\nR = np.corrcoef(X_std.T)\neigvals, eigvecs = np.linalg.eigh(R)\n\n# Step 2: Project predictors onto eigenvectors\nLambda = eigvecs @ np.diag(np.sqrt(eigvals)) @ eigvecs.T\nZ = X_std @ Lambda  # uncorrelated predictors\n\n# Step 3: Regress on transformed Z\nmodel = LinearRegression().fit(Z, y)\nbeta = model.coef_\n\n# Step 4: Map weights back to original variables\nraw_weights = (Lambda @ beta) ** 2\nrel_weights = raw_weights / np.sum(raw_weights)\n\n# Format results\njohnson_df = pd.DataFrame({\n    \"Johnson's Relative Weight\": rel_weights\n}, index=X.columns).sort_values(by=\"Johnson's Relative Weight\", ascending=False)\n\ndisplay(johnson_df)\n\n\n           Johnson's Relative Weight\nbuild                       0.257294\nrewarding                   0.229933\nappealing                   0.226722\nimpact                      0.097359\ndiffers                     0.056623\npopular                     0.046759\ntrust                       0.031575\neasy                        0.029087\nservice                     0.024649\n\n\n\n\nRandom Forest Variable Importance (Mean Decrease in Gini)\nRandom forests measure the importance of each variable by evaluating how much it reduces node impurity (typically using the Gini index) across all trees.\nThis metric is widely used in machine learning because it can capture nonlinear interactions and doesn‚Äôt assume linearity or independence between variables.\n\n\nRandom Forest Importance (Gini decrease)\nfrom sklearn.ensemble import RandomForestRegressor\nimport pandas as pd\n\n# Fit Random Forest\nrf = RandomForestRegressor(n_estimators=500, random_state=42)\nrf.fit(X, y)\n\n\nRandomForestRegressor(n_estimators=500, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor(n_estimators=500, random_state=42) \n\n\nRandom Forest Importance (Gini decrease)\n# Get feature importances\nrf_importances = rf.feature_importances_\n\n# Format into a DataFrame\nrf_df = pd.DataFrame({\n    \"Random Forest (Gini Importance)\": rf_importances\n}, index=X.columns).sort_values(by=\"Random Forest (Gini Importance)\", ascending=False)\n\ndisplay(rf_df)\n\n\n           Random Forest (Gini Importance)\nappealing                         0.122222\nrewarding                         0.121301\nbuild                             0.117583\nimpact                            0.113222\npopular                           0.111513\nservice                           0.110354\ntrust                             0.102015\neasy                              0.101639\ndiffers                           0.100151\n\n\n\n\nSummary Table: Key Driver Importance Across Methods\nThe table below compares all six variable importance metrics used to evaluate the impact of perception attributes on customer satisfaction for Brand 1. Each column represents a different method:\n\nPearson Correlation: Simple linear relationship between each predictor and satisfaction.\nStandardized Coefficient: Beta weights from a standardized linear regression model.\nUsefulness (R¬≤): The variance in satisfaction explained by each predictor individually.\nShapley Value (LMG): A fair-share decomposition of the full model‚Äôs R¬≤ using all variable combinations (via R‚Äôs relaimpo).\nJohnson‚Äôs Relative Weights: Partitioned R¬≤ based on orthogonal transformations to adjust for multicollinearity.\nRandom Forest (Gini Importance): A machine learning measure based on how much each variable reduces node impurity across all trees.\n\nThis comparative view highlights which variables emerge consistently as strong predictors ‚Äî offering both interpretability and robustness across statistical and machine learning frameworks.\n\n\nSummary Table of Variable Importance\n# Recreate lmg_df from known values (previous output pasted manually for this run)\nlmg_values = {\n    \"trust\": 0.08414815,\n    \"build\": 0.07901473,\n    \"differs\": 0.02771814,\n    \"easy\": 0.09853352,\n    \"appealing\": 0.22634684,\n    \"rewarding\": 0.21443335,\n    \"popular\": 0.08908973,\n    \"service\": 0.06453611,\n    \"impact\": 0.11617944\n}\nlmg_df = pd.DataFrame.from_dict(lmg_values, orient='index', columns=[\"Shapley Value (LMG)\"])\n\n# Now combine all metrics into the final summary table\nsummary_df = (\n    pearson_df\n    .join(std_coef_df)\n    .join(usefulness_df)\n    .join(lmg_df)\n    .join(johnson_df)\n    .join(rf_df)\n)\n\n# Round for readability\nsummary_df_rounded = summary_df.round(3)\n\nfrom IPython.core.display import HTML\nHTML(summary_df_rounded.to_html(classes=\"table table-striped\"))\n\n\n\n\n\n\nPearson Correlation\nStandardized Coefficient\nUsefulness (R¬≤)\nShapley Value (LMG)\nJohnson's Relative Weight\nRandom Forest (Gini Importance)\n\n\n\n\nappealing\n0.214\n0.127\n0.046\n0.226\n0.227\n0.122\n\n\nrewarding\n0.201\n0.127\n0.041\n0.214\n0.230\n0.121\n\n\neasy\n0.174\n0.045\n0.030\n0.099\n0.029\n0.102\n\n\nimpact\n0.170\n0.083\n0.029\n0.116\n0.097\n0.113\n\n\ntrust\n0.161\n0.047\n0.026\n0.084\n0.032\n0.102\n\n\npopular\n0.150\n0.057\n0.023\n0.089\n0.047\n0.112\n\n\nservice\n0.147\n0.042\n0.022\n0.065\n0.025\n0.110\n\n\ndiffers\n0.092\n-0.063\n0.009\n0.028\n0.057\n0.100\n\n\nbuild\n0.048\n-0.135\n0.002\n0.079\n0.257\n0.118\n\n\n\n\n\n\n\nConclusion: What Drives Satisfaction for Brand 1?\nAcAcross all six importance metrics, a consistent pattern emerges:\n\nTop Drivers:\nappealing and rewarding rank among the highest across every method ‚Äî from simple correlation to machine learning and variance decomposition. These two attributes are strong, reliable predictors of customer satisfaction for Brand 1.\nSecondary Influencers:\nimpact, easy, and trust show moderate but consistent importance across several metrics. These may act as supporting attributes ‚Äî not always dominant alone, but valuable when combined with top drivers.\nMethod Sensitivities:\nbuild and differs receive lower rankings in linear metrics (like R¬≤ and standardized coefficients), but perform better in Random Forest and Johnson‚Äôs weights, hinting at non-linear effects or interaction patterns not captured by traditional regression.\nModel Agreement:\nThe agreement between different techniques ‚Äî both statistical and algorithmic ‚Äî adds confidence to the overall result:\n&gt; Brand 1‚Äôs satisfaction is primarily driven by how appealing and rewarding customers find the product or experience.\n\nThis multi-method approach ensures that insights are robust, not artifacts of a single modeling technique, and provides a richer understanding of what truly matters to customers."
  }
]