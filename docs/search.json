[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Brian‚Äôs Resume",
    "section": "",
    "text": "Last updated 8/24/24\nDownload PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\nBrian Pintar\nApr 22, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brian Pintar",
    "section": "",
    "text": "This website is created for MGTA 495 assignments.\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "projects/hw1/index.html",
    "href": "projects/hw1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nPublished in 2007, this experiment was inspired by America‚Äôs significant increase in private charitable giving in the decades prior. It was alluded to the fact that a combination of increased wealth and an aging population in America likely caused this increase. Their analysis originally concluded that the match offer increases both the revenue per solicitation and the response rate. However, larger match ratios (i.e., $3:$1 instead of $1:$1) did not have any additional impact. It was also concluded that the matching had a much larger effect in red states than blue states.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/hw1/index.html#introduction",
    "href": "projects/hw1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nPublished in 2007, this experiment was inspired by America‚Äôs significant increase in private charitable giving in the decades prior. It was alluded to the fact that a combination of increased wealth and an aging population in America likely caused this increase. Their analysis originally concluded that the match offer increases both the revenue per solicitation and the response rate. However, larger match ratios (i.e., $3:$1 instead of $1:$1) did not have any additional impact. It was also concluded that the matching had a much larger effect in red states than blue states.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/hw1/index.html#data",
    "href": "projects/hw1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\nimport pandas as pd\ndf = pd.read_stata('karlan_list_2007.dta')\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows √ó 51 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nimport numpy as np\nfrom scipy.stats import t\n\ndef t_test_randomization_check(df, column, treat_col='treatment', control_col='control'):\n    \"\"\"\n    Performs Welch's t-test to compare treatment and control groups for a given variable.\n\n    Parameters:\n    - df: pandas DataFrame\n    - column: name of the column to compare (e.g., 'mrm2')\n    - treat_col: name of the treatment indicator column (default: 'treatment')\n    - control_col: name of the control indicator column (default: 'control')\n    \"\"\"\n\n    # Extract the groups\n    treat = df[df[treat_col] == 1][column].dropna()\n    control = df[df[control_col] == 1][column].dropna()\n\n    # Means and standard deviations\n    mean_treat = treat.mean()\n    mean_control = control.mean()\n    std_treat = treat.std(ddof=1)\n    std_control = control.std(ddof=1)\n\n    # Sample sizes\n    n_treat = len(treat)\n    n_control = len(control)\n\n    # t-statistic\n    numerator = mean_treat - mean_control\n    denominator = np.sqrt((std_treat**2)/n_treat + (std_control**2)/n_control)\n    t_stat = numerator / denominator\n\n    # Degrees of freedom (Welch-Satterthwaite)\n    var_treat = std_treat**2 / n_treat\n    var_control = std_control**2 / n_control\n    df_numerator = (var_treat + var_control)**2\n    df_denominator = (var_treat**2 / (n_treat - 1)) + (var_control**2 / (n_control - 1))\n    df_welch = df_numerator / df_denominator\n\n    # p-value\n    p_value = 2 * (1 - t.cdf(np.abs(t_stat), df=df_welch))\n\n    # Output\n    print(f\"\\nüß™ T-Test for '{column}': Treatment vs Control\")\n    print(f\"Treatment mean: {mean_treat:.2f}; Control mean: {mean_control:.2f}\")\n    print(f\"Treatment std dev: {std_treat:.2f}; Control std dev: {std_control:.2f}\")\n    print(f\"t-statistic: {t_stat:.4f}\")\n    print(f\"Degrees of freedom: {df_welch:.2f}\")\n    print(f\"p-value: {p_value:.4f}\")\n\n    # Decision\n    if p_value &lt; 0.05:\n        print(\"‚ùó Reject the null hypothesis: The groups are significantly different.\")\n    else:\n        print(\"‚úÖ Fail to reject the null hypothesis: No significant difference between groups.\")\n\n\nt_test_randomization_check(df, 'mrm2')\nt_test_randomization_check(df, 'female')\nt_test_randomization_check(df, 'red0')\n\n\nüß™ T-Test for 'mrm2': Treatment vs Control\nTreatment mean: 13.01; Control mean: 13.00\nTreatment std dev: 12.09; Control std dev: 12.07\nt-statistic: 0.1195\nDegrees of freedom: 33394.48\np-value: 0.9049\n‚úÖ Fail to reject the null hypothesis: No significant difference between groups.\n\nüß™ T-Test for 'female': Treatment vs Control\nTreatment mean: 0.28; Control mean: 0.28\nTreatment std dev: 0.45; Control std dev: 0.45\nt-statistic: -1.7535\nDegrees of freedom: 32450.81\np-value: 0.0795\n‚úÖ Fail to reject the null hypothesis: No significant difference between groups.\n\nüß™ T-Test for 'red0': Treatment vs Control\nTreatment mean: 0.41; Control mean: 0.40\nTreatment std dev: 0.49; Control std dev: 0.49\nt-statistic: 1.8773\nDegrees of freedom: 33450.52\np-value: 0.0605\n‚úÖ Fail to reject the null hypothesis: No significant difference between groups.\n\n\nThe above results confirm randomization of the experiment. There was no significant difference in the treatment and control groups regarding the number of months since last donation, the percentage of male/female participants, or participants in red/blue states, which could have affected the results. Let‚Äôs compare results with a linear regression for the months since last donation variable ‚Äòmrm2‚Äô to confirm our approach.\n\nimport statsmodels.api as sm\n\ndf_reg = df[['mrm2', 'treatment']].dropna()\n\n#define x and y\nX = df_reg['treatment']\nX = sm.add_constant(X)\ny = df_reg['mrm2']\n\n#fit model\nmodel = sm.OLS(y,X).fit()\n\n#extract values\ncoef = model.params['treatment']\nt_stat = model.tvalues['treatment']\np_value = model.pvalues['treatment']\n\nprint(f\"Coefficient (treatment): {coef:.4f}\")\nprint(f\"t-statistic: {t_stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\nCoefficient (treatment): 0.0137\nt-statistic: 0.1195\np-value: 0.9049\n\n\nThe t-statistics and p-values match accordingly, confirming the method.\nTable 1 is included in the original report to provide evidence of randomization of the treatment and control groups. This is proven to be true, being that we fail to reject the null hypothesis with no statistically significant difference in demographics for treatment and control groups."
  },
  {
    "objectID": "projects/hw1/index.html#experimental-results",
    "href": "projects/hw1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n\n#calculate proportions\nprop_treatment = df[df['treatment'] == 1]['gave'].mean()\nprop_control = df[df['control'] == 1]['gave'].mean()\n\n#data for plot\ngroups = ['Treatment', 'Control']\nproportions = [prop_treatment, prop_control]\n\n#plot\nplt.figure(figsize=(6,4))\nplt.bar(groups, proportions)\nplt.ylabel('Proportion Donated')\nplt.title('Donation Rate by Group')\nplt.ylim(0,.05)\nplt.show()\n\n\n\n\n\n\n\n\nBased on the bar chart above, it appears that the treatment had a slight effect on donation rates.\n\ndf_lpm = df[['gave', 'treatment']].dropna()\n\n#define outcome and predictor\nX = sm.add_constant(df_lpm['treatment'])\ny = df_lpm['gave']\n\n#fit linear regression\nmodel = sm.OLS(y, X).fit()\n\n#extract stats\ncoef = model.params['treatment']\nt_stat = model.tvalues['treatment']\np_value = model.pvalues['treatment']\n\n#output\nprint(f\"Coefficient (treatment effect): {coef:.4f}\")\nprint(f\"t-statistic: {t_stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\nCoefficient (treatment effect): 0.0042\nt-statistic: 3.1014\np-value: 0.0019\n\n\nWhile the effect is small, it is statistically significant based on the p-value of less than .05. This implies that people in general provided donations more when told that donations would be matched.\n\ndf_probit = df[['gave', 'treatment']].dropna()\n\n#define predictors and outcome\nX = sm.add_constant(df_probit['treatment'])\ny = df_probit['gave']\n\n#fit probit model\nprobit_model = sm.Probit(y, X).fit()\n\nmfx = probit_model.get_margeff()\nprint(mfx.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\nThe probit regression indicates an increase in probability of donating by 0.43%, holding all else constant, when given the treatment (matching). This is consistent with the linear regression results, as effect is small. P-values also match, and are statistically significant. Treatment has a statistically significant but modest positive effect on probability of donation.\nNOTE: Linear regression results appear replicate Table 3 column 1 in the paper. Probit results do not, despite Table 3 indicating its results come from probit regressions‚Ä¶\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\ndef t_test_donation_rate(df, ratio_a, ratio_b, outcome_col='gave', ratio_col='ratio'):\n    \"\"\"\n    Performs and prints a Welch's t-test comparing donation rates between two match ratio groups.\n    \n    Parameters:\n    - df: pandas DataFrame\n    - ratio_a: first ratio value (e.g., 1, 2, 3, or \"Control\")\n    - ratio_b: second ratio value\n    - outcome_col: name of the binary outcome column (default: 'gave')\n    - ratio_col: name of the match ratio column (default: 'ratio')\n    \n    Returns:\n    - Dictionary of test results\n    \"\"\"\n    \n    group_a = df[df[ratio_col] == ratio_a][outcome_col].dropna()\n    group_b = df[df[ratio_col] == ratio_b][outcome_col].dropna()\n\n    n1, n2 = len(group_a), len(group_b)\n    p1, p2 = group_a.mean(), group_b.mean()\n\n    # Standard error\n    se = np.sqrt((p1 * (1 - p1)) / n1 + (p2 * (1 - p2)) / n2)\n\n    # t-statistic\n    t_stat = (p2 - p1) / se\n\n    # Degrees of freedom\n    var1 = (p1 * (1 - p1)) / n1\n    var2 = (p2 * (1 - p2)) / n2\n    df_welch = (var1 + var2)**2 / ((var1**2)/(n1 - 1) + (var2**2)/(n2 - 1))\n\n    # Two-tailed p-value\n    p_value = 2 * (1 - t.cdf(np.abs(t_stat), df=df_welch))\n\n    # Print results\n    print(f\"\\nüéØ T-Test: {ratio_b}:1 vs {ratio_a}:1 Match Ratio\")\n    print(\"-\" * 40)\n    print(f\"Sample size ({ratio_a}:1): {n1}\")\n    print(f\"Sample size ({ratio_b}:1): {n2}\")\n    print(f\"Donation rate ({ratio_a}:1): {p1:.4f}\")\n    print(f\"Donation rate ({ratio_b}:1): {p2:.4f}\")\n    print(f\"t-statistic: {t_stat:.4f}\")\n    print(f\"Degrees of freedom: {df_welch:.2f}\")\n    print(f\"p-value: {p_value:.4f}\")\n    \n    if p_value &lt; 0.05:\n        print(\"‚úÖ Statistically significant at the 5% level.\")\n    else:\n        print(\"‚ùå Not statistically significant at the 5% level.\")\n\nt_test_donation_rate(df, 1, 2)  # 1:1 vs 2:1\nt_test_donation_rate(df, 2, 3)\nt_test_donation_rate(df, 1, 3)\n\n\nüéØ T-Test: 2:1 vs 1:1 Match Ratio\n----------------------------------------\nSample size (1:1): 11133\nSample size (2:1): 11134\nDonation rate (1:1): 0.0207\nDonation rate (2:1): 0.0226\nt-statistic: 0.9651\nDegrees of freedom: 22225.08\np-value: 0.3345\n‚ùå Not statistically significant at the 5% level.\n\nüéØ T-Test: 3:1 vs 2:1 Match Ratio\n----------------------------------------\nSample size (2:1): 11134\nSample size (3:1): 11129\nDonation rate (2:1): 0.0226\nDonation rate (3:1): 0.0227\nt-statistic: 0.0501\nDegrees of freedom: 22260.85\np-value: 0.9600\n‚ùå Not statistically significant at the 5% level.\n\nüéØ T-Test: 3:1 vs 1:1 Match Ratio\n----------------------------------------\nSample size (1:1): 11133\nSample size (3:1): 11129\nDonation rate (1:1): 0.0207\nDonation rate (3:1): 0.0227\nt-statistic: 1.0151\nDegrees of freedom: 22215.05\np-value: 0.3101\n‚ùå Not statistically significant at the 5% level.\n\n\nAuthor suggests that neither the different match thresholds or example amount had a meaningful influence on behavior. Results above suggest the same. Not enough evidence to conclude that the difference in donation rates is statistically significant for different treatment options.\nCreate the ratio1 variable.\n\n#create the ratio1 variable\ndf['ratio_str'] = df['ratio'].astype('str')\ndf['ratio1'] = (df['ratio_str'] == '1').astype(int)\ndf['ratio2'] = (df['ratio_str'] == '2').astype(int)\ndf['ratio3'] = (df['ratio_str'] == '3').astype(int)\nprint(df[['ratio', 'ratio1', 'ratio2', 'ratio3']].head(10))\n\n     ratio  ratio1  ratio2  ratio3\n0  Control       0       0       0\n1  Control       0       0       0\n2        1       1       0       0\n3        1       1       0       0\n4        1       1       0       0\n5  Control       0       0       0\n6        1       1       0       0\n7        2       0       1       0\n8        2       0       1       0\n9        1       1       0       0\n\n\nRegression results are presented below.\n\ndef compare_match_ratios(df, ratio_a, ratio_b, ratio_col='ratio', outcome_col='gave'):\n    \"\"\"\n    Compare two match ratios using linear regression on a binary outcome.\n\n    Parameters:\n    - df: DataFrame with data\n    - ratio_a: first match ratio to compare (e.g., 1 for 1:1)\n    - ratio_b: second match ratio to compare (e.g., 2 for 2:1)\n    - ratio_col: name of the column containing match ratios\n    - outcome_col: name of the binary outcome column\n\n    Returns:\n    - Dictionary with coefficient and p-value\n    \"\"\"\n\n    # Filter to only the two groups being compared\n    df_sub = df[df[ratio_col].isin([ratio_a, ratio_b])].copy()\n\n    # Create indicator for being in ratio_b group\n    df_sub['is_ratio_b'] = (df_sub[ratio_col] == ratio_b).astype(int)\n\n    # Run regression\n    X = sm.add_constant(df_sub['is_ratio_b'])\n    y = df_sub[outcome_col]\n\n    model = sm.OLS(y, X).fit()\n\n    coef = model.params['is_ratio_b']\n    p_value = model.pvalues['is_ratio_b']\n\n    print(f\"\\nüìä Comparing donation rates: {ratio_b}:1 vs {ratio_a}:1\")\n    print(f\"Coefficient (diff in donation rate): {coef:.4f}\")\n    print(f\"p-value: {p_value:.4f}\")\n\ncompare_match_ratios(df, 1, 2)  # Compare 2:1 vs 1:1\ncompare_match_ratios(df, 2, 3)\ncompare_match_ratios(df, 1, 3) \n\n\nüìä Comparing donation rates: 2:1 vs 1:1\nCoefficient (diff in donation rate): 0.0019\np-value: 0.3345\n\nüìä Comparing donation rates: 3:1 vs 2:1\nCoefficient (diff in donation rate): 0.0001\np-value: 0.9600\n\nüìä Comparing donation rates: 3:1 vs 1:1\nCoefficient (diff in donation rate): 0.0020\np-value: 0.3101\n\n\nThe regression results above indicate the same. The differences between donation rates of the different match ratios are not statistically significant, and therefore do not indicate any effect.\n\n# Response (donation) rates\np_1 = df[df['ratio'] == 1]['gave'].mean()\np_2 = df[df['ratio'] == 2]['gave'].mean()\np_3 = df[df['ratio'] == 3]['gave'].mean()\n\n# Differences in raw proportions\ndiff_2v1 = p_2 - p_1\ndiff_3v2 = p_3 - p_2\ndiff_3v1 = p_3 - p_1\n\nprint(f\"Direct from data:\")\nprint(f\"2:1 vs 1:1: {diff_2v1:.4f}\")\nprint(f\"3:1 vs 2:1: {diff_3v2:.4f}\")\nprint(f\"3:1 vs 1:1: {diff_3v1:.4f}\")\n\nDirect from data:\n2:1 vs 1:1: 0.0019\n3:1 vs 2:1: 0.0001\n3:1 vs 1:1: 0.0020\n\n\nResults from data match the regression coefficients. We can safely conclude that there is no effect that match ratios have on donation rates.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\n\ndf_reg = df[['amount', 'treatment']].dropna()\n\nX = sm.add_constant(df_reg['treatment'])\ny = df_reg['amount']\n\nmodel = sm.OLS(y, X).fit()\n\ncoef = model.params['treatment']\npval = model.pvalues['treatment']\n\nprint(f\"Treatment effect (difference in donation amount): {coef:.4f}\")\nprint(f\"p-value: {pval:.4f}\")\n\nTreatment effect (difference in donation amount): 0.1536\np-value: 0.0628\n\n\nWhile the results from the regression analysis above indicate an increase of 0.15 in the donation amount when given the treatment letter, the p-value is not statistically significant at the 95% confidence level. However, the difference calculated is accurate compared to Table2A of original analysis.\n\n# Filter to donors only\ndf_donors = df[(df['amount'] &gt; 0) & df['treatment'].notna()][['amount', 'treatment']]\n\nX = sm.add_constant(df_donors['treatment'])\ny = df_donors['amount']\n\nmodel = sm.OLS(y, X).fit()\n\ncoef = model.params['treatment']\npval = model.pvalues['treatment']\n\nprint(f\"Treatment effect among donors: {coef:.2f}\")\nprint(f\"p-value: {pval:.4f}\")\n\nTreatment effect among donors: -1.67\np-value: 0.5615\n\n\nWhile the results of the above regression, only considering those who actually donated to begin with, indicate a small decrease in the donation amount, this amount is not statistically significant. Therefore, we cannot conclude that the treatment letters (those with the match ratios) have any significant causal effect on the actual donation amount.\n\n# Filter to donors only\ndf_donors = df[df['amount'] &gt; 0]\n\n# Split groups\ntreatment_amounts = df_donors[df_donors['treatment'] == 1]['amount']\ncontrol_amounts = df_donors[df_donors['control'] == 1]['amount']\n\n# Calculate means\nmean_treatment = treatment_amounts.mean()\nmean_control = control_amounts.mean()\n\n# Plot: Control Group\nplt.figure(figsize=(8, 5))\nplt.hist(control_amounts, bins=30, alpha=0.7, color='gray', edgecolor='black')\nplt.axvline(mean_control, color='red', linestyle='--', linewidth=2, label=f'Mean: ${mean_control:.2f}')\nplt.title('Donation Amounts (Control Group)')\nplt.xlabel('Donation Amount')\nplt.ylabel('Number of Donors')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Plot: Treatment Group\nplt.figure(figsize=(8, 5))\nplt.hist(treatment_amounts, bins=30, alpha=0.7, color='gray', edgecolor='black')\nplt.axvline(mean_treatment, color='red', linestyle='--', linewidth=2, label=f'Mean: ${mean_treatment:.2f}')\nplt.title('Donation Amounts (Treatment Group)')\nplt.xlabel('Donation Amount')\nplt.ylabel('Number of Donors')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe two histograms above indicate a similar average and similar distribution of donation amounts among those who donated to begin with."
  },
  {
    "objectID": "projects/hw1/index.html#simulation-experiment",
    "href": "projects/hw1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic ‚Äúworks,‚Äù in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Step 1: Simulate 10k Bernoulli draws for each group\nn_draws = 10000\ncontrol_draws = np.random.binomial(1, 0.018, size=n_draws)\ntreatment_draws = np.random.binomial(1, 0.022, size=n_draws)\n\n# Step 2: Compute the vector of differences\ndiffs = treatment_draws - control_draws  # element-wise difference\n\n# Step 3: Compute cumulative average of the differences\ncumulative_avg = np.cumsum(diffs) / np.arange(1, n_draws + 1)\n\n# Step 4: Plot cumulative average\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg, label='Cumulative Average Treatment Effect')\nplt.axhline(y=0.004, color='red', linestyle='--', label='True Difference (0.022 - 0.018)')\nplt.title('Cumulative Average of Simulated Treatment Effect')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Cumulative Average Difference')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAccordingly, in the simulation above, the cumulative average eventually converges towards the true difference of 0.004. As the sample size of the random simulation gets larger, the average eventually converges to the true average.\n\n\nCentral Limit Theorem\n\n# Set seed\nnp.random.seed(42)\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\nn_simulations = 1000\nsample_sizes = [50, 200, 500, 1000]\n\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    mean_diffs = []\n    \n    for _ in range(n_simulations):\n        # Sample from each distribution\n        control_sample = np.random.binomial(1, p_control, size=n)\n        treatment_sample = np.random.binomial(1, p_treatment, size=n)\n\n        # Calculate mean difference\n        diff = treatment_sample.mean() - control_sample.mean()\n        mean_diffs.append(diff)\n    \n    # Plot histogram\n    axes[i].hist(mean_diffs, bins=30, edgecolor='black', alpha=0.75)\n    axes[i].axvline(np.mean(mean_diffs), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(mean_diffs):.4f}')\n    axes[i].set_title(f'Sample Size = {n}')\n    axes[i].set_xlabel('Mean Difference')\n    axes[i].set_ylabel('Frequency')\n    axes[i].legend()\n    axes[i].grid(True)\n\n# Final layout\nplt.suptitle('Sampling Distribution of Mean Differences (Treatment - Control)', fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nWhen running a simulation experiment, the sampling distribution takes on a bell-shape very quickly, even at sample size of 50, but gets smoothed out by the time sample size is 1000. Central Limit Theorem takes place earlier in the sequence than anticipated, approximating a normal distribution, but more filled in and smooth by 1000 sample size."
  }
]