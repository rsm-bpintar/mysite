[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Brian‚Äôs Resume",
    "section": "",
    "text": "Last updated 8/24/24\nDownload PDF file."
  },
  {
    "objectID": "projects/hw1/index.html",
    "href": "projects/hw1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nPublished in 2007, this experiment was inspired by America‚Äôs significant increase in private charitable giving in the decades prior. It was alluded to the fact that a combination of increased wealth and an aging population in America likely caused this increase. Their analysis originally concluded that the match offer increases both the revenue per solicitation and the response rate. However, larger match ratios (i.e., $3:$1 instead of $1:$1) did not have any additional impact. It was also concluded that the matching had a much larger effect in red states than blue states.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/hw1/index.html#introduction",
    "href": "projects/hw1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nPublished in 2007, this experiment was inspired by America‚Äôs significant increase in private charitable giving in the decades prior. It was alluded to the fact that a combination of increased wealth and an aging population in America likely caused this increase. Their analysis originally concluded that the match offer increases both the revenue per solicitation and the response rate. However, larger match ratios (i.e., $3:$1 instead of $1:$1) did not have any additional impact. It was also concluded that the matching had a much larger effect in red states than blue states.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/hw1/index.html#data",
    "href": "projects/hw1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\ndf = pd.read_stata('karlan_list_2007.dta')\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows √ó 51 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nimport numpy as np\nfrom scipy.stats import t\n\ndef t_test_randomization_check(df, column, treat_col='treatment', control_col='control'):\n    \"\"\"\n    Performs Welch's t-test to compare treatment and control groups for a given variable.\n\n    Parameters:\n    - df: pandas DataFrame\n    - column: name of the column to compare (e.g., 'mrm2')\n    - treat_col: name of the treatment indicator column (default: 'treatment')\n    - control_col: name of the control indicator column (default: 'control')\n    \"\"\"\n\n    # Extract the groups\n    treat = df[df[treat_col] == 1][column].dropna()\n    control = df[df[control_col] == 1][column].dropna()\n\n    # Means and standard deviations\n    mean_treat = treat.mean()\n    mean_control = control.mean()\n    std_treat = treat.std(ddof=1)\n    std_control = control.std(ddof=1)\n\n    # Sample sizes\n    n_treat = len(treat)\n    n_control = len(control)\n\n    # t-statistic\n    numerator = mean_treat - mean_control\n    denominator = np.sqrt((std_treat**2)/n_treat + (std_control**2)/n_control)\n    t_stat = numerator / denominator\n\n    # Degrees of freedom (Welch-Satterthwaite)\n    var_treat = std_treat**2 / n_treat\n    var_control = std_control**2 / n_control\n    df_numerator = (var_treat + var_control)**2\n    df_denominator = (var_treat**2 / (n_treat - 1)) + (var_control**2 / (n_control - 1))\n    df_welch = df_numerator / df_denominator\n\n    # p-value\n    p_value = 2 * (1 - t.cdf(np.abs(t_stat), df=df_welch))\n\n    # Output\n    print(f\"\\nüß™ T-Test for '{column}': Treatment vs Control\")\n    print(f\"Treatment mean: {mean_treat:.2f}; Control mean: {mean_control:.2f}\")\n    print(f\"Treatment std dev: {std_treat:.2f}; Control std dev: {std_control:.2f}\")\n    print(f\"t-statistic: {t_stat:.4f}\")\n    print(f\"Degrees of freedom: {df_welch:.2f}\")\n    print(f\"p-value: {p_value:.4f}\")\n\n    # Decision\n    if p_value &lt; 0.05:\n        print(\"‚ùó Reject the null hypothesis: The groups are significantly different.\")\n    else:\n        print(\"‚úÖ Fail to reject the null hypothesis: No significant difference between groups.\")\n\n\nt_test_randomization_check(df, 'mrm2')\nt_test_randomization_check(df, 'female')\nt_test_randomization_check(df, 'red0')\n\n\nüß™ T-Test for 'mrm2': Treatment vs Control\nTreatment mean: 13.01; Control mean: 13.00\nTreatment std dev: 12.09; Control std dev: 12.07\nt-statistic: 0.1195\nDegrees of freedom: 33394.48\np-value: 0.9049\n‚úÖ Fail to reject the null hypothesis: No significant difference between groups.\n\nüß™ T-Test for 'female': Treatment vs Control\nTreatment mean: 0.28; Control mean: 0.28\nTreatment std dev: 0.45; Control std dev: 0.45\nt-statistic: -1.7535\nDegrees of freedom: 32450.81\np-value: 0.0795\n‚úÖ Fail to reject the null hypothesis: No significant difference between groups.\n\nüß™ T-Test for 'red0': Treatment vs Control\nTreatment mean: 0.41; Control mean: 0.40\nTreatment std dev: 0.49; Control std dev: 0.49\nt-statistic: 1.8773\nDegrees of freedom: 33450.52\np-value: 0.0605\n‚úÖ Fail to reject the null hypothesis: No significant difference between groups.\n\n\nThe above results confirm randomization of the experiment. There was no significant difference in the treatment and control groups regarding the number of months since last donation, the percentage of male/female participants, or participants in red/blue states, which could have affected the results. Let‚Äôs compare results with a linear regression for the months since last donation variable ‚Äòmrm2‚Äô to confirm our approach.\n\nimport statsmodels.api as sm\n\ndf_reg = df[['mrm2', 'treatment']].dropna()\n\n#define x and y\nX = df_reg['treatment']\nX = sm.add_constant(X)\ny = df_reg['mrm2']\n\n#fit model\nmodel = sm.OLS(y,X).fit()\n\n#extract values\ncoef = model.params['treatment']\nt_stat = model.tvalues['treatment']\np_value = model.pvalues['treatment']\n\nprint(f\"Coefficient (treatment): {coef:.4f}\")\nprint(f\"t-statistic: {t_stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\nCoefficient (treatment): 0.0137\nt-statistic: 0.1195\np-value: 0.9049\n\n\nThe t-statistics and p-values match accordingly, confirming the method.\nTable 1 is included in the original report to provide evidence of randomization of the treatment and control groups. This is proven to be true, being that we fail to reject the null hypothesis with no statistically significant difference in demographics for treatment and control groups."
  },
  {
    "objectID": "projects/hw1/index.html#experimental-results",
    "href": "projects/hw1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n\n#calculate proportions\nprop_treatment = df[df['treatment'] == 1]['gave'].mean()\nprop_control = df[df['control'] == 1]['gave'].mean()\n\n#data for plot\ngroups = ['Treatment', 'Control']\nproportions = [prop_treatment, prop_control]\n\n#plot\nplt.figure(figsize=(6,4))\nplt.bar(groups, proportions)\nplt.ylabel('Proportion Donated')\nplt.title('Donation Rate by Group')\nplt.ylim(0,.05)\nplt.show()\n\n\n\n\n\n\n\n\nBased on the bar chart above, it appears that the treatment had a slight effect on donation rates.\n\ndf_lpm = df[['gave', 'treatment']].dropna()\n\n#define outcome and predictor\nX = sm.add_constant(df_lpm['treatment'])\ny = df_lpm['gave']\n\n#fit linear regression\nmodel = sm.OLS(y, X).fit()\n\n#extract stats\ncoef = model.params['treatment']\nt_stat = model.tvalues['treatment']\np_value = model.pvalues['treatment']\n\n#output\nprint(f\"Coefficient (treatment effect): {coef:.4f}\")\nprint(f\"t-statistic: {t_stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\nCoefficient (treatment effect): 0.0042\nt-statistic: 3.1014\np-value: 0.0019\n\n\nWhile the effect is small, it is statistically significant based on the p-value of less than .05. This implies that people in general provided donations more when told that donations would be matched.\n\ndf_probit = df[['gave', 'treatment']].dropna()\n\n#define predictors and outcome\nX = sm.add_constant(df_probit['treatment'])\ny = df_probit['gave']\n\n#fit probit model\nprobit_model = sm.Probit(y, X).fit()\n\nmfx = probit_model.get_margeff()\nprint(mfx.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\nThe probit regression indicates an increase in probability of donating by 0.43%, holding all else constant, when given the treatment (matching). This is consistent with the linear regression results, as effect is small. P-values also match, and are statistically significant. Treatment has a statistically significant but modest positive effect on probability of donation.\nNOTE: Linear regression results appear replicate Table 3 column 1 in the paper. Probit results do not, despite Table 3 indicating its results come from probit regressions‚Ä¶\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\ndef t_test_donation_rate(df, ratio_a, ratio_b, outcome_col='gave', ratio_col='ratio'):\n    \"\"\"\n    Performs and prints a Welch's t-test comparing donation rates between two match ratio groups.\n    \n    Parameters:\n    - df: pandas DataFrame\n    - ratio_a: first ratio value (e.g., 1, 2, 3, or \"Control\")\n    - ratio_b: second ratio value\n    - outcome_col: name of the binary outcome column (default: 'gave')\n    - ratio_col: name of the match ratio column (default: 'ratio')\n    \n    Returns:\n    - Dictionary of test results\n    \"\"\"\n    \n    group_a = df[df[ratio_col] == ratio_a][outcome_col].dropna()\n    group_b = df[df[ratio_col] == ratio_b][outcome_col].dropna()\n\n    n1, n2 = len(group_a), len(group_b)\n    p1, p2 = group_a.mean(), group_b.mean()\n\n    # Standard error\n    se = np.sqrt((p1 * (1 - p1)) / n1 + (p2 * (1 - p2)) / n2)\n\n    # t-statistic\n    t_stat = (p2 - p1) / se\n\n    # Degrees of freedom\n    var1 = (p1 * (1 - p1)) / n1\n    var2 = (p2 * (1 - p2)) / n2\n    df_welch = (var1 + var2)**2 / ((var1**2)/(n1 - 1) + (var2**2)/(n2 - 1))\n\n    # Two-tailed p-value\n    p_value = 2 * (1 - t.cdf(np.abs(t_stat), df=df_welch))\n\n    # Print results\n    print(f\"\\nüéØ T-Test: {ratio_b}:1 vs {ratio_a}:1 Match Ratio\")\n    print(\"-\" * 40)\n    print(f\"Sample size ({ratio_a}:1): {n1}\")\n    print(f\"Sample size ({ratio_b}:1): {n2}\")\n    print(f\"Donation rate ({ratio_a}:1): {p1:.4f}\")\n    print(f\"Donation rate ({ratio_b}:1): {p2:.4f}\")\n    print(f\"t-statistic: {t_stat:.4f}\")\n    print(f\"Degrees of freedom: {df_welch:.2f}\")\n    print(f\"p-value: {p_value:.4f}\")\n    \n    if p_value &lt; 0.05:\n        print(\"‚úÖ Statistically significant at the 5% level.\")\n    else:\n        print(\"‚ùå Not statistically significant at the 5% level.\")\n\nt_test_donation_rate(df, 1, 2)  # 1:1 vs 2:1\nt_test_donation_rate(df, 2, 3)\nt_test_donation_rate(df, 1, 3)\n\n\nüéØ T-Test: 2:1 vs 1:1 Match Ratio\n----------------------------------------\nSample size (1:1): 11133\nSample size (2:1): 11134\nDonation rate (1:1): 0.0207\nDonation rate (2:1): 0.0226\nt-statistic: 0.9651\nDegrees of freedom: 22225.08\np-value: 0.3345\n‚ùå Not statistically significant at the 5% level.\n\nüéØ T-Test: 3:1 vs 2:1 Match Ratio\n----------------------------------------\nSample size (2:1): 11134\nSample size (3:1): 11129\nDonation rate (2:1): 0.0226\nDonation rate (3:1): 0.0227\nt-statistic: 0.0501\nDegrees of freedom: 22260.85\np-value: 0.9600\n‚ùå Not statistically significant at the 5% level.\n\nüéØ T-Test: 3:1 vs 1:1 Match Ratio\n----------------------------------------\nSample size (1:1): 11133\nSample size (3:1): 11129\nDonation rate (1:1): 0.0207\nDonation rate (3:1): 0.0227\nt-statistic: 1.0151\nDegrees of freedom: 22215.05\np-value: 0.3101\n‚ùå Not statistically significant at the 5% level.\n\n\nAuthor suggests that neither the different match thresholds or example amount had a meaningful influence on behavior. Results above suggest the same. Not enough evidence to conclude that the difference in donation rates is statistically significant for different treatment options.\nCreate the ratio1 variable.\n\n#create the ratio1 variable\ndf['ratio_str'] = df['ratio'].astype('str')\ndf['ratio1'] = (df['ratio_str'] == '1').astype(int)\ndf['ratio2'] = (df['ratio_str'] == '2').astype(int)\ndf['ratio3'] = (df['ratio_str'] == '3').astype(int)\nprint(df[['ratio', 'ratio1', 'ratio2', 'ratio3']].head(10))\n\n     ratio  ratio1  ratio2  ratio3\n0  Control       0       0       0\n1  Control       0       0       0\n2        1       1       0       0\n3        1       1       0       0\n4        1       1       0       0\n5  Control       0       0       0\n6        1       1       0       0\n7        2       0       1       0\n8        2       0       1       0\n9        1       1       0       0\n\n\nRegression results are presented below.\n\ndef compare_match_ratios(df, ratio_a, ratio_b, ratio_col='ratio', outcome_col='gave'):\n    \"\"\"\n    Compare two match ratios using linear regression on a binary outcome.\n\n    Parameters:\n    - df: DataFrame with data\n    - ratio_a: first match ratio to compare (e.g., 1 for 1:1)\n    - ratio_b: second match ratio to compare (e.g., 2 for 2:1)\n    - ratio_col: name of the column containing match ratios\n    - outcome_col: name of the binary outcome column\n\n    Returns:\n    - Dictionary with coefficient and p-value\n    \"\"\"\n\n    # Filter to only the two groups being compared\n    df_sub = df[df[ratio_col].isin([ratio_a, ratio_b])].copy()\n\n    # Create indicator for being in ratio_b group\n    df_sub['is_ratio_b'] = (df_sub[ratio_col] == ratio_b).astype(int)\n\n    # Run regression\n    X = sm.add_constant(df_sub['is_ratio_b'])\n    y = df_sub[outcome_col]\n\n    model = sm.OLS(y, X).fit()\n\n    coef = model.params['is_ratio_b']\n    p_value = model.pvalues['is_ratio_b']\n\n    print(f\"\\nüìä Comparing donation rates: {ratio_b}:1 vs {ratio_a}:1\")\n    print(f\"Coefficient (diff in donation rate): {coef:.4f}\")\n    print(f\"p-value: {p_value:.4f}\")\n\ncompare_match_ratios(df, 1, 2)  # Compare 2:1 vs 1:1\ncompare_match_ratios(df, 2, 3)\ncompare_match_ratios(df, 1, 3) \n\n\nüìä Comparing donation rates: 2:1 vs 1:1\nCoefficient (diff in donation rate): 0.0019\np-value: 0.3345\n\nüìä Comparing donation rates: 3:1 vs 2:1\nCoefficient (diff in donation rate): 0.0001\np-value: 0.9600\n\nüìä Comparing donation rates: 3:1 vs 1:1\nCoefficient (diff in donation rate): 0.0020\np-value: 0.3101\n\n\nThe regression results above indicate the same. The differences between donation rates of the different match ratios are not statistically significant, and therefore do not indicate any effect.\n\n# Response (donation) rates\np_1 = df[df['ratio'] == 1]['gave'].mean()\np_2 = df[df['ratio'] == 2]['gave'].mean()\np_3 = df[df['ratio'] == 3]['gave'].mean()\n\n# Differences in raw proportions\ndiff_2v1 = p_2 - p_1\ndiff_3v2 = p_3 - p_2\ndiff_3v1 = p_3 - p_1\n\nprint(f\"Direct from data:\")\nprint(f\"2:1 vs 1:1: {diff_2v1:.4f}\")\nprint(f\"3:1 vs 2:1: {diff_3v2:.4f}\")\nprint(f\"3:1 vs 1:1: {diff_3v1:.4f}\")\n\nDirect from data:\n2:1 vs 1:1: 0.0019\n3:1 vs 2:1: 0.0001\n3:1 vs 1:1: 0.0020\n\n\nResults from data match the regression coefficients. We can safely conclude that there is no effect that match ratios have on donation rates.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\n\ndf_reg = df[['amount', 'treatment']].dropna()\n\nX = sm.add_constant(df_reg['treatment'])\ny = df_reg['amount']\n\nmodel = sm.OLS(y, X).fit()\n\ncoef = model.params['treatment']\npval = model.pvalues['treatment']\n\nprint(f\"Treatment effect (difference in donation amount): {coef:.4f}\")\nprint(f\"p-value: {pval:.4f}\")\n\nTreatment effect (difference in donation amount): 0.1536\np-value: 0.0628\n\n\nWhile the results from the regression analysis above indicate an increase of 0.15 in the donation amount when given the treatment letter, the p-value is not statistically significant at the 95% confidence level. However, the difference calculated is accurate compared to Table2A of original analysis.\n\n# Filter to donors only\ndf_donors = df[(df['amount'] &gt; 0) & df['treatment'].notna()][['amount', 'treatment']]\n\nX = sm.add_constant(df_donors['treatment'])\ny = df_donors['amount']\n\nmodel = sm.OLS(y, X).fit()\n\ncoef = model.params['treatment']\npval = model.pvalues['treatment']\n\nprint(f\"Treatment effect among donors: {coef:.2f}\")\nprint(f\"p-value: {pval:.4f}\")\n\nTreatment effect among donors: -1.67\np-value: 0.5615\n\n\nWhile the results of the above regression, only considering those who actually donated to begin with, indicate a small decrease in the donation amount, this amount is not statistically significant. Therefore, we cannot conclude that the treatment letters (those with the match ratios) have any significant causal effect on the actual donation amount.\n\n# Filter to donors only\ndf_donors = df[df['amount'] &gt; 0]\n\n# Split groups\ntreatment_amounts = df_donors[df_donors['treatment'] == 1]['amount']\ncontrol_amounts = df_donors[df_donors['control'] == 1]['amount']\n\n# Calculate means\nmean_treatment = treatment_amounts.mean()\nmean_control = control_amounts.mean()\n\n# Plot: Control Group\nplt.figure(figsize=(8, 5))\nplt.hist(control_amounts, bins=30, alpha=0.7, color='gray', edgecolor='black')\nplt.axvline(mean_control, color='red', linestyle='--', linewidth=2, label=f'Mean: ${mean_control:.2f}')\nplt.title('Donation Amounts (Control Group)')\nplt.xlabel('Donation Amount')\nplt.ylabel('Number of Donors')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Plot: Treatment Group\nplt.figure(figsize=(8, 5))\nplt.hist(treatment_amounts, bins=30, alpha=0.7, color='gray', edgecolor='black')\nplt.axvline(mean_treatment, color='red', linestyle='--', linewidth=2, label=f'Mean: ${mean_treatment:.2f}')\nplt.title('Donation Amounts (Treatment Group)')\nplt.xlabel('Donation Amount')\nplt.ylabel('Number of Donors')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe two histograms above indicate a similar average and similar distribution of donation amounts among those who donated to begin with."
  },
  {
    "objectID": "projects/hw1/index.html#simulation-experiment",
    "href": "projects/hw1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic ‚Äúworks,‚Äù in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Step 1: Simulate 10k Bernoulli draws for each group\nn_draws = 10000\ncontrol_draws = np.random.binomial(1, 0.018, size=n_draws)\ntreatment_draws = np.random.binomial(1, 0.022, size=n_draws)\n\n# Step 2: Compute the vector of differences\ndiffs = treatment_draws - control_draws  # element-wise difference\n\n# Step 3: Compute cumulative average of the differences\ncumulative_avg = np.cumsum(diffs) / np.arange(1, n_draws + 1)\n\n# Step 4: Plot cumulative average\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg, label='Cumulative Average Treatment Effect')\nplt.axhline(y=0.004, color='red', linestyle='--', label='True Difference (0.022 - 0.018)')\nplt.title('Cumulative Average of Simulated Treatment Effect')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Cumulative Average Difference')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAccordingly, in the simulation above, the cumulative average eventually converges towards the true difference of 0.004. As the sample size of the random simulation gets larger, the average eventually converges to the true average.\n\n\nCentral Limit Theorem\n\n# Set seed\nnp.random.seed(42)\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\nn_simulations = 1000\nsample_sizes = [50, 200, 500, 1000]\n\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    mean_diffs = []\n    \n    for _ in range(n_simulations):\n        # Sample from each distribution\n        control_sample = np.random.binomial(1, p_control, size=n)\n        treatment_sample = np.random.binomial(1, p_treatment, size=n)\n\n        # Calculate mean difference\n        diff = treatment_sample.mean() - control_sample.mean()\n        mean_diffs.append(diff)\n    \n    # Plot histogram\n    axes[i].hist(mean_diffs, bins=30, edgecolor='black', alpha=0.75)\n    axes[i].axvline(np.mean(mean_diffs), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(mean_diffs):.4f}')\n    axes[i].set_title(f'Sample Size = {n}')\n    axes[i].set_xlabel('Mean Difference')\n    axes[i].set_ylabel('Frequency')\n    axes[i].legend()\n    axes[i].grid(True)\n\n# Final layout\nplt.suptitle('Sampling Distribution of Mean Differences (Treatment - Control)', fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nWhen running a simulation experiment, the sampling distribution takes on a bell-shape very quickly, even at sample size of 50, but gets smoothed out by the time sample size is 1000. Central Limit Theorem takes place earlier in the sequence than anticipated, approximating a normal distribution, but more filled in and smooth by 1000 sample size."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Multinomial Logit Model\n\n\n\n\nBrian Pintar\nMay 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\nBrian Pintar\nMay 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\nBrian Pintar\nApr 22, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brian Pintar",
    "section": "",
    "text": "This website is created for MGTA 495 assignments.\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "projects/hw3/index.html",
    "href": "projects/hw3/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment explores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "projects/hw3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "projects/hw3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer‚Äôs decision as the selection of the product that provides the most utility, and we‚Äôll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "projects/hw3/index.html#simulate-conjoint-data",
    "href": "projects/hw3/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a ‚Äúno choice‚Äù option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\nShow code\nimport numpy as np\nimport pandas as pd\nfrom itertools import product\n\n# Set random seed for reproducibility\nnp.random.seed(123)\n\n# 1. Define attribute levels\nbrands = ['N', 'P', 'H']  # Netflix, Prime, Hulu\nads = ['Yes', 'No']\nprices = np.arange(8, 33, 4)  # $8 to $32\n\n# 2. Generate all possible profiles\nprofiles = pd.DataFrame(list(product(brands, ads, prices)), columns=['brand', 'ad', 'price'])\n\n# 3. Define utility weights\nbrand_util = {'N': 1.0, 'P': 0.5, 'H': 0.0}\nad_util = {'Yes': -0.8, 'No': 0.0}\nprice_util = lambda p: -0.1 * p\n\n# 4. Simulation parameters\nn_respondents = 100\nn_tasks = 10\nn_alts = 3\n\n# 5. Simulate choice tasks for each respondent\nsimulated_data = []\n\nfor respondent_id in range(1, n_respondents + 1):\n    for task_id in range(1, n_tasks + 1):\n        sampled_profiles = profiles.sample(n=n_alts).reset_index(drop=True)\n        sampled_profiles['brand_util'] = sampled_profiles['brand'].map(brand_util)\n        sampled_profiles['ad_util'] = sampled_profiles['ad'].map(ad_util)\n        sampled_profiles['price_util'] = sampled_profiles['price'].apply(price_util)\n        \n        # Compute deterministic utility\n        sampled_profiles['utility'] = (\n            sampled_profiles['brand_util'] +\n            sampled_profiles['ad_util'] +\n            sampled_profiles['price_util']\n        )\n        \n        # Add Gumbel noise\n        gumbel_noise = np.random.gumbel(loc=0, scale=1, size=n_alts)\n        sampled_profiles['total_utility'] = sampled_profiles['utility'] + gumbel_noise\n\n        # Determine choice (1 if max utility, else 0)\n        choice_index = sampled_profiles['total_utility'].idxmax()\n        sampled_profiles['choice'] = 0\n        sampled_profiles.loc[choice_index, 'choice'] = 1\n\n        # Add metadata\n        sampled_profiles['respondent'] = respondent_id\n        sampled_profiles['task'] = task_id\n\n        simulated_data.append(sampled_profiles)\n\n# Combine into a single DataFrame\ndf_simulated = pd.concat(simulated_data, ignore_index=True)"
  },
  {
    "objectID": "projects/hw3/index.html#preparing-the-data-for-estimation",
    "href": "projects/hw3/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe ‚Äúhard part‚Äù of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n\nShow code\n# One-hot encode brand and ad (drop the reference levels: brand_H and ad_No)\ndf_design = pd.get_dummies(df_simulated, columns=['brand', 'ad'], drop_first=True)\n\n# Keep only relevant columns for X matrix\nX_columns = ['brand_N', 'brand_P', 'ad_Yes', 'price']\nX = df_design[X_columns]\n\n# Outcome variable: 1 if alternative was chosen, 0 otherwise\ny = df_design['choice'].values\n\n# Also store respondent and task IDs for future grouping if needed\nrespondent_ids = df_design['respondent'].values\ntask_ids = df_design['task'].values\n\n# Show a preview of the prepared design matrix\ndf_prepared = df_design[['respondent', 'task'] + X_columns + ['choice']]\n\ndf_prepared.head()\n\n\n\n\n\n\n\n\n\nrespondent\ntask\nbrand_N\nbrand_P\nad_Yes\nprice\nchoice\n\n\n\n\n0\n1\n1\nFalse\nTrue\nFalse\n32\n0\n\n\n1\n1\n1\nTrue\nFalse\nFalse\n28\n1\n\n\n2\n1\n1\nTrue\nFalse\nFalse\n24\n0\n\n\n3\n1\n2\nFalse\nFalse\nFalse\n28\n0\n\n\n4\n1\n2\nFalse\nFalse\nFalse\n8\n1"
  },
  {
    "objectID": "projects/hw3/index.html#estimation-via-maximum-likelihood",
    "href": "projects/hw3/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nTo estimate the part-worth parameters of the multinomial logit model, we define the log-likelihood function based on the softmax probability formulation. Each respondent‚Äôs utility for each alternative is calculated as a linear function of the attributes (brand, ad presence, and price). We then maximize the log-likelihood using scipy.optimize.minimize() with the BFGS algorithm.\nThe estimated coefficients correspond to: - \\(\\beta_{\\text{netflix}}\\): preference for Netflix (vs.¬†Hulu) - \\(\\beta_{\\text{prime}}\\): preference for Prime (vs.¬†Hulu) - \\(\\beta_{\\text{ads}}\\): penalty for ad-included options (vs.¬†ad-free) - \\(\\beta_{\\text{price}}\\): marginal disutility per dollar\nWe also compute standard errors from the inverse Hessian matrix and report 95% confidence intervals for each parameter estimate.\n\n\nShow code\n# Re-import necessary packages after code execution state reset\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\n\n# Ensure X and y are NumPy arrays\nX = df_design[['brand_N', 'brand_P', 'ad_Yes', 'price']].to_numpy(dtype=np.float64)\ny = df_design['choice'].to_numpy(dtype=np.int64)\n\n# Reshape y into (n_tasks, 3)\ny_matrix = y.reshape((-1, 3))\nn_tasks_total = y_matrix.shape[0]\n\n# Define negative log-likelihood for MNL\ndef neg_log_likelihood(beta):\n    beta = np.asarray(beta, dtype=np.float64)\n    utilities = X @ beta\n    utilities = utilities.reshape((-1, 3))\n    exp_utils = np.exp(utilities)\n    probs = exp_utils / np.sum(exp_utils, axis=1, keepdims=True)\n    chosen_probs = probs[np.arange(n_tasks_total), y_matrix.argmax(axis=1)]\n    return -np.sum(np.log(chosen_probs))\n\n# Initial guess\nbeta_init = np.zeros(X.shape[1])\n\n# Minimize the negative log-likelihood\nresult = minimize(neg_log_likelihood, beta_init, method='BFGS')\n\n# Extract estimates and standard errors\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\nz = 1.96\n\n# Confidence intervals\nconf_int = np.column_stack([\n    beta_hat - z * std_errors,\n    beta_hat + z * std_errors\n])\n\n# Package results\nparam_names = ['brand_N', 'brand_P', 'ad_Yes', 'price']\nresults_df = pd.DataFrame({\n    'Parameter': param_names,\n    'Estimate': beta_hat,\n    'Std. Error': std_errors,\n    'CI Lower (95%)': conf_int[:, 0],\n    'CI Upper (95%)': conf_int[:, 1]\n})\n\nresults_df\n\n\n\n\n\n\n\n\n\nParameter\nEstimate\nStd. Error\nCI Lower (95%)\nCI Upper (95%)\n\n\n\n\n0\nbrand_N\n0.974795\n0.125815\n0.728199\n1.221392\n\n\n1\nbrand_P\n0.426656\n0.115195\n0.200874\n0.652439\n\n\n2\nad_Yes\n-0.728157\n0.095708\n-0.915744\n-0.540569\n\n\n3\nprice\n-0.111804\n0.006604\n-0.124747\n-0.098861\n\n\n\n\n\n\n\nThe table above reports the estimated part-worth utilities for each attribute level in the multinomial logit (MNL) model. All four coefficients are statistically significant at the 95% confidence level, as none of their confidence intervals include zero. The signs and magnitudes of the estimates align with economic intuition and the true values used in the simulation.\nBrand Preferences: Consumers exhibit a strong preference for Netflix over Hulu (baseline), with an estimated utility gain of approximately 0.98 units. Amazon Prime is also preferred over Hulu, though to a lesser degree (0.43 units). These estimates closely match the simulated values of 1.0 and 0.5, respectively.\nAdvertising: The presence of advertisements decreases the utility of a streaming offer by about 0.73 units. This negative effect is in line with the simulated disutility of -0.8 and indicates that consumers have a strong preference for ad-free experiences.\nPrice Sensitivity: Each additional dollar in monthly price reduces utility by approximately 0.11 units. This estimate is tightly bounded and very close to the simulated effect of -0.1, suggesting that the model has successfully captured consumers‚Äô price sensitivity.\nOverall, the MNL model recovers the true preference structure well, validating both the simulation setup and the estimation procedure."
  },
  {
    "objectID": "projects/hw3/index.html#estimation-via-bayesian-methods",
    "href": "projects/hw3/index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nWe now estimate the multinomial logit model using a Bayesian approach via Metropolis-Hastings MCMC. The posterior distribution is defined by combining the log-likelihood with prior distributions:\n\nNormal priors \\(\\mathcal{N}(0, 5)\\) for the binary attribute coefficients (brand_N, brand_P, ad_Yes)\nA tighter prior \\(\\mathcal{N}(0, 1)\\) for the continuous price coefficient\n\nWe run the sampler for 11,000 iterations, discarding the first 1,000 as burn-in, and use the remaining 10,000 to summarize the posterior distribution.\nTo generate proposals, we use a multivariate normal distribution with independent components: - \\(\\mathcal{N}(0, 0.05)\\) for the three binary variables - \\(\\mathcal{N}(0, 0.005)\\) for the price coefficient\nWe assess convergence by examining a trace plot and posterior histogram for the price parameter.\n\n\nShow code\nimport matplotlib.pyplot as plt\nfrom itertools import product\n\n# Re-run data simulation and preparation\nnp.random.seed(123)\nbrands = ['N', 'P', 'H']\nads = ['Yes', 'No']\nprices = np.arange(8, 33, 4)\nprofiles = pd.DataFrame(list(product(brands, ads, prices)), columns=['brand', 'ad', 'price'])\n\nbrand_util = {'N': 1.0, 'P': 0.5, 'H': 0.0}\nad_util = {'Yes': -0.8, 'No': 0.0}\nprice_util = lambda p: -0.1 * p\n\nn_respondents = 100\nn_tasks = 10\nn_alts = 3\n\nsimulated_data = []\nfor respondent_id in range(1, n_respondents + 1):\n    for task_id in range(1, n_tasks + 1):\n        sampled = profiles.sample(n=n_alts).reset_index(drop=True)\n        sampled['brand_util'] = sampled['brand'].map(brand_util)\n        sampled['ad_util'] = sampled['ad'].map(ad_util)\n        sampled['price_util'] = sampled['price'].apply(price_util)\n        sampled['utility'] = sampled['brand_util'] + sampled['ad_util'] + sampled['price_util']\n        noise = np.random.gumbel(0, 1, n_alts)\n        sampled['total_utility'] = sampled['utility'] + noise\n        sampled['choice'] = 0\n        sampled.loc[sampled['total_utility'].idxmax(), 'choice'] = 1\n        sampled['respondent'] = respondent_id\n        sampled['task'] = task_id\n        simulated_data.append(sampled)\n\ndf_simulated = pd.concat(simulated_data, ignore_index=True)\ndf_design = pd.get_dummies(df_simulated, columns=['brand', 'ad'], drop_first=True)\nX = df_design[['brand_N', 'brand_P', 'ad_Yes', 'price']].to_numpy(dtype=np.float64)\ny = df_design['choice'].to_numpy(dtype=np.int64)\ny_matrix = y.reshape((-1, 3))\nn_tasks_total = y_matrix.shape[0]\n\n# Define functions\ndef log_likelihood(beta):\n    utilities = X @ beta\n    utilities = utilities.reshape((-1, 3))\n    exp_utilities = np.exp(utilities)\n    probs = exp_utilities / exp_utilities.sum(axis=1, keepdims=True)\n    chosen_probs = probs[np.arange(n_tasks_total), y_matrix.argmax(axis=1)]\n    return np.sum(np.log(chosen_probs))\n\ndef log_prior(beta):\n    return (\n        -0.5 * (beta[0]**2 / 25)\n        -0.5 * (beta[1]**2 / 25)\n        -0.5 * (beta[2]**2 / 25)\n        -0.5 * (beta[3]**2 / 1)\n    )\n\ndef log_posterior(beta):\n    return log_likelihood(beta) + log_prior(beta)\n\n# Metropolis-Hastings MCMC\nn_iter = 11000\nbeta_samples = np.zeros((n_iter, 4))\nbeta_current = np.zeros(4)\nlog_post_current = log_posterior(beta_current)\n\nfor i in range(1, n_iter):\n    proposal = beta_current + np.random.normal(loc=0, scale=[0.05, 0.05, 0.05, 0.005])\n    log_post_proposal = log_posterior(proposal)\n    accept_prob = np.exp(log_post_proposal - log_post_current)\n    if np.random.rand() &lt; accept_prob:\n        beta_current = proposal\n        log_post_current = log_post_proposal\n    beta_samples[i] = beta_current\n\n# Posterior summaries after burn-in\nburn_in = 1000\nbeta_samples_post = beta_samples[burn_in:]\nposterior_means = beta_samples_post.mean(axis=0)\nposterior_sds = beta_samples_post.std(axis=0)\ncred_intervals = np.percentile(beta_samples_post, [2.5, 97.5], axis=0).T\n\n# Results DataFrame\nparam_names = ['brand_N', 'brand_P', 'ad_Yes', 'price']\nbayes_results_df = pd.DataFrame({\n    'Parameter': param_names,\n    'Posterior Mean': posterior_means,\n    'Posterior SD': posterior_sds,\n    'CI Lower (95%)': cred_intervals[:, 0],\n    'CI Upper (95%)': cred_intervals[:, 1]\n})\n\nbayes_results_df.head()\n\n# Trace plot and histogram for price\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(beta_samples_post[:, 3])\nplt.title('Trace Plot: price')\nplt.xlabel('Iteration')\nplt.ylabel('Value')\n\nplt.subplot(1, 2, 2)\nplt.hist(beta_samples_post[:, 3], bins=30, edgecolor='k')\nplt.title('Posterior Histogram: price')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe posterior summary table shows the means, standard deviations, and 95% credible intervals for each parameter. These estimates closely align with those obtained from maximum likelihood estimation in Section 4.\n\nThe posterior means match the MLE estimates within small margins.\nThe credible intervals are tight and all exclude zero, indicating strong evidence for the direction and magnitude of each effect.\nThe trace plot for the price parameter shows good mixing behavior, and the posterior distribution appears approximately normal.\n\nThese results confirm that both MLE and Bayesian methods yield consistent and interpretable insights about consumer preferences in this simulated conjoint setting."
  },
  {
    "objectID": "projects/hw3/index.html#discussion",
    "href": "projects/hw3/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\n\nInterpreting Parameter Estimates\nEven without knowing the true data-generating process (i.e., in a real-world setting), the estimated parameters from the multinomial logit model align with strong economic intuition:\n\n\\(\\beta_{\\text{Netflix}} &gt; \\beta_{\\text{Prime}}\\) implies that, on average, consumers have a stronger preference for Netflix than Amazon Prime, relative to the baseline brand (Hulu). This could be due to perceived content quality, popularity, or brand loyalty.\nA negative \\(\\beta_{\\text{price}}\\) is expected and logical. It means that, all else equal, consumers are less likely to choose a more expensive plan. The magnitude reflects how sensitive utility is to price ‚Äî a critical insight for pricing strategy.\nThe ad penalty coefficient, \\(\\beta_{\\text{ads}} &lt; 0\\), indicates that consumers value ad-free experiences and are willing to give up some utility (or pay more) to avoid ads.\n\nThese results demonstrate how the MNL model can quantify the impact of product features on consumer decision-making, even when the true parameters are unknown.\n\n\nToward Hierarchical (Multilevel) Models\nIn real-world conjoint studies, different consumers often have different preferences ‚Äî not everyone values brand or price the same way. To capture this heterogeneity, we would move from a fixed-parameter MNL model to a hierarchical (random-parameter) model.\nThis change involves two key modifications:\n\nData Simulation: Instead of assigning one fixed set of \\(\\beta\\) values to all respondents, we would draw individual-level parameters from a population distribution. This would better reflect individual variation in preferences.\nEstimation: We would use a Bayesian hierarchical model (via MCMC) or a Mixed Logit model (via simulation-based MLE) to estimate both:\n\nThe population-level means \\(\\mu\\)\nThe variation across individuals, captured by \\(\\Sigma\\)\n\n\nThese models are more flexible and realistic, and are commonly used in industry applications of conjoint analysis.\nIn summary, while the standard MNL model is a solid starting point, moving to a hierarchical framework is critical for analyzing real consumer choice data with preference heterogeneity."
  },
  {
    "objectID": "projects/hw2/index.html",
    "href": "projects/hw2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\nFirst, we can read in the data and do some exploratory analysis.\n\nimport pandas as pd\n\ndf1 = pd.read_csv('blueprinty.csv')\ndf1.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nLet‚Äôs compare histograms and means of number of patents by customer status.\n\nimport matplotlib.pyplot as plt\n\n# Create two histograms side by side\nplt.figure(figsize=(12, 5))\n\n# Histogram for non-customers\nplt.subplot(1, 2, 1)\nplt.hist(df1[df1['iscustomer'] == 0]['patents'], bins=30, alpha=0.7)\nplt.title('Non-Customers: Number of Patents')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\n\n# Histogram for customers\nplt.subplot(1, 2, 2)\nplt.hist(df1[df1['iscustomer'] == 1]['patents'], bins=30, alpha=0.7, color='orange')\nplt.title('Customers: Number of Patents')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nHistograms indicate somewhat similar results. Let‚Äôs compare means to test for statistical significance.\n\nfrom scipy.stats import ttest_ind\n\n# Separate the groups\npatents_customers = df1[df1['iscustomer'] == 1]['patents']\npatents_non_customers = df1[df1['iscustomer'] == 0]['patents']\n\n# Calculate means\nmean_customers = patents_customers.mean()\nmean_non_customers = patents_non_customers.mean()\n\n# Perform the t-test\nt_stat, p_value = ttest_ind(patents_customers, patents_non_customers, equal_var=False)\n\n# Print results\nprint(f\"Mean patents (customers): {mean_customers:.2f}\")\nprint(f\"Mean patents (non-customers): {mean_non_customers:.2f}\")\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_value:.3f}\")\n\nMean patents (customers): 4.13\nMean patents (non-customers): 3.47\nT-statistic: 4.873\nP-value: 0.000\n\n\nBased on our t-test results, we reject the null hypothesis that the means are equal.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nLet‚Äôs compare age between the customer and non customer groups first.\n\nimport seaborn as sns\n\nplt.figure(figsize=(10, 5))\n\nsns.kdeplot(data=df1, x='age', hue='iscustomer', fill=True, common_norm=False, alpha=0.5)\nplt.title('Age Distribution by Customer Status')\nplt.xlabel('Age')\nplt.ylabel('Density')\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.show()\n\n\n\n\n\n\n\n\nThere appears to be a slight difference in age between customers and non customers. Let‚Äôs run a t-test to check.\n\n# Separate the age values\nage_customers = df1[df1['iscustomer'] == 1]['age']\nage_non_customers = df1[df1['iscustomer'] == 0]['age']\n\n# Calculate means\nmean_age_customers = age_customers.mean()\nmean_age_non_customers = age_non_customers.mean()\n\n# Perform the t-test\nt_stat_age, p_value_age = ttest_ind(age_customers, age_non_customers, equal_var=False)\n\n# Print results\nprint(f\"Mean age (customers): {mean_age_customers:.2f}\")\nprint(f\"Mean age (non-customers): {mean_age_non_customers:.2f}\")\nprint(f\"T-statistic (age): {t_stat_age:.3f}\")\nprint(f\"P-value (age): {p_value_age:.3f}\")\n\nMean age (customers): 26.90\nMean age (non-customers): 26.10\nT-statistic (age): 1.913\nP-value (age): 0.056\n\n\nWe fail to reject the null hypothesis, indicating no statistically significant difference in age between the customer and non customer groups. Now we repeat the process for region.\n\n# Create a count table (not normalized)\nregion_counts = pd.crosstab(df1['region'], df1['iscustomer'])\nregion_counts.columns = ['Non-Customers', 'Customers']\n\n# Plot grouped bar chart\nregion_counts.plot(kind='bar', figsize=(10, 6))\nplt.title('Number of Customers and Non-Customers by Region')\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.legend(title='Customer Status')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIt appears based on the chart that there are some differences in the proportions of customers vs.¬†non customers in different regions. We‚Äôll run a chi-squared test to check this.\n\nfrom scipy.stats import chi2_contingency\n\n# Crosstab: region vs customer status\nregion_table = pd.crosstab(df1['region'], df1['iscustomer'])\n\n# Chi-squared test\nchi2_stat, p_value_region, dof, expected = chi2_contingency(region_table)\n\nprint(\"Chi-squared Statistic:\", round(chi2_stat, 2))\nprint(\"Degrees of Freedom:\", dof)\nprint(\"P-value:\", round(p_value_region, 4))\n\nChi-squared Statistic: 233.63\nDegrees of Freedom: 4\nP-value: 0.0\n\n\nBased on the low p-value, we can assume there is a statistically significant relationship between region and customer status. Accordingly, the distribution of customers across regions is not random, as some are more or less likely to have customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nLet‚Äôs mathematically write the likelihood for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nThe likelihood for all observations is:\n\\(L(\\lambda \\mid Y) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\\)\nThe log-likelihood is:\n\\(\\ell(\\lambda \\mid Y) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\\)\nNow let‚Äôs write the log-likelihood function for the Poisson model as a function of lambda and Y.\n\nimport numpy as np\nfrom scipy.special import gammaln  # log(Y!) = gammaln(Y + 1)\n\ndef poisson_loglikelihood(lambd, Y):\n    \"\"\"\n    Compute the log-likelihood of a Poisson model given lambda and observed data Y.\n\n    Parameters:\n    - lambd: float, the Poisson rate parameter (must be &gt; 0)\n    - Y: array-like, observed count data\n\n    Returns:\n    - log-likelihood value\n    \"\"\"\n    if lambd &lt;= 0:\n        return -np.inf  # log-likelihood is undefined for lambda &lt;= 0\n    return np.sum(-lambd + Y * np.log(lambd) - gammaln(Y + 1))\n\nNow, let‚Äôs use the function to plot lambda on the horizontal axis and the log-likelihood on the vertical axis for a range of lambdas.\n\n# Get observed patent data\nY_obs = df1['patents'].values\n\n# Define a range of lambda values to evaluate\nlambda_vals = np.linspace(0.1, 20, 200)\n\n# Compute the log-likelihood for each lambda\nloglik_vals = [poisson_loglikelihood(l, Y_obs) for l in lambda_vals]\n\n# Plot the log-likelihood curve\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='blue', lw=2)\nplt.title('Log-Likelihood of Poisson Model vs. Lambda')\nplt.xlabel(r'$\\lambda$')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nBased on the graph above, the highest log-likelihood appears to occur when lambda is around 3. Now let‚Äôs check and see if our numerical mean is the same as our analytic mean.\nWe start with the log-likelihood:\n\\[\n\\ell(\\lambda \\mid Y) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nTaking the derivative:\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSet this equal to zero and solve:\n\\[\n-n + \\frac{1}{\\lambda} \\sum Y_i = 0 \\Rightarrow \\lambda = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\nTherefore, the MLE of \\(\\lambda\\) is the sample mean \\(\\bar{Y}\\).\nLet‚Äôs find the MLE by optimizing the likelihood function.\n\nfrom scipy.optimize import minimize_scalar\n\n# Reuse observed data\nY_obs = df1['patents'].values\n\n# Define negative log-likelihood function\ndef neg_loglik_poisson(lambd):\n    return -poisson_loglikelihood(lambd, Y_obs)\n\n# Use minimize_scalar to find the lambda that minimizes negative log-likelihood\nresult = minimize_scalar(neg_loglik_poisson, bounds=(0.01, 50), method='bounded')\n\n# Extract MLE\nlambda_mle_numerical = result.x\nloglik_at_mle = -result.fun\n\nprint(f\"MLE (Numerical): {lambda_mle_numerical:.4f}\")\nprint(f\"Log-Likelihood at MLE: {loglik_at_mle:.4f}\")\n\n# Compare to analytic MLE\nprint(f\"MLE (Analytic Mean): {np.mean(Y_obs):.4f}\")\n\nMLE (Numerical): 3.6847\nLog-Likelihood at MLE: -3367.6838\nMLE (Analytic Mean): 3.6847\n\n\nBoth methods agree that the best fitting Poisson model indicates that firms receive, on average, 3.68 patents every five years.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nLet‚Äôs define our updated regression function.\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"\n    Log-likelihood for Poisson regression.\n\n    Parameters:\n    - beta: (p,) vector of coefficients\n    - Y: (n,) vector of observed counts\n    - X: (n, p) matrix of covariates\n\n    Returns:\n    - scalar log-likelihood value\n    \"\"\"\n    beta = np.asarray(beta, dtype=np.float64)  # ensure correct type\n    X = np.asarray(X, dtype=np.float64)        # ensure matrix format\n    Xb = np.dot(X, beta)                       # matrix multiplication\n    lambdas = np.exp(Xb)                       # element-wise exponential\n    return np.sum(-lambdas + Y * Xb - gammaln(Y + 1))\n\nNow, let‚Äôs create the covariate matrix X.\n\nimport patsy  # optional, but can simplify dummy creation\n\n# Create a copy of the data to work with\ndf = df1.copy()\n\n# Create new variables\n# Rescale age and age_squared\ndf['age'] = df['age'] / 10\ndf['age_squared'] = df['age'] ** 2\n\n# One-hot encode region, dropping one to avoid multicollinearity\nregion_dummies = pd.get_dummies(df['region'], drop_first=True)\n\n# Combine into one design matrix\nX = pd.concat([\n    pd.Series(1, index=df.index, name='intercept'),\n    df[['age', 'age_squared', 'iscustomer']],\n    pd.get_dummies(df['region'], drop_first=True)\n], axis=1)\n\nY = df['patents'].values\nX_mat = X.values  # for optimization input only\n\n# Column names\ncolumn_names = X.columns\n\nLet‚Äôs now optimize to find the MLE vector and the Hessian of the Poisson model with covariates.\n\nfrom scipy.optimize import minimize\n\nX_mat = X.to_numpy().astype(np.float64)  # make sure X is a NumPy array\nY_vec = Y.astype(np.float64)             # make sure Y is a NumPy array\ninit_beta = np.zeros(X_mat.shape[1])     # starting guess for beta (all 0s)\n\n# Define the negative log-likelihood function for use in optimizer\nneg_loglik = lambda b: -poisson_regression_loglikelihood(b, Y_vec, X_mat)\n\n# Optimize\nresult = minimize(neg_loglik, init_beta, method='BFGS')\n\nLet‚Äôs now extract the results.\n\n# Extract estimated coefficients and standard errors\nbeta_hat = result.x\nvcov_beta = result.hess_inv\nstandard_errors = np.sqrt(np.diag(vcov_beta))\n\n# Show results in a table\nsummary_table = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': standard_errors\n}, index=X.columns)\n\nsummary_table\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.508925\n0.185670\n\n\nage\n1.486199\n0.139210\n\n\nage_squared\n-0.297048\n0.025840\n\n\niscustomer\n0.207591\n0.032190\n\n\nNortheast\n0.029170\n0.046083\n\n\nNorthwest\n-0.017574\n0.055659\n\n\nSouth\n0.056562\n0.054619\n\n\nSouthwest\n0.050576\n0.048116\n\n\n\n\n\n\n\nThe Poisson regression estimates how the expected number of patents awarded to a firm varies with its characteristics. To improve numerical stability during optimization, the age variable was scaled by a factor of 10 (i.e., a one-unit change in age represents a 10-year difference).\nThe coefficient for age is 1.49, and for age_squared is ‚àí0.30, indicating a non-linear relationship: as firms age, their expected number of patents initially increases, but the rate of increase slows and eventually declines with age.\nThe coefficient on iscustomer is 0.208, meaning that, holding all else equal, firms that use Blueprinty‚Äôs software have a materially higher expected patent count compared to non-customers.\nRegional effects appear minimal, with all region dummy coefficients close to zero and standard errors indicating low statistical significance relative to the reference region.\nNow let‚Äôs compare results against the statsmodels ‚Äòsm.GLM()‚Äô function.\n\nimport statsmodels.api as sm\n\n# Recreate X_sm and ensure it's numeric\nX_sm = sm.add_constant(X.drop(columns='intercept'), has_constant='add')\nX_sm = X_sm.astype(float)  # &lt;== This line prevents the object dtype error\n\n# Also ensure Y is numeric\nY_numeric = Y.astype(float)\n\n# Fit GLM model\nglm_model = sm.GLM(Y_numeric, X_sm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary\n# Create a clean summary table\nsummary_table = pd.DataFrame({\n    'Coefficient': glm_results.params,\n    'Std. Error': glm_results.bse,\n    'z value': glm_results.tvalues,\n    'P&gt;|z|': glm_results.pvalues\n})\n\n# Optional: round for readability\nsummary_table = summary_table.round(4)\n\nsummary_table\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz value\nP&gt;|z|\n\n\n\n\nconst\n-0.5089\n0.1832\n-2.7783\n0.0055\n\n\nage\n1.4862\n0.1387\n10.7162\n0.0000\n\n\nage_squared\n-0.2970\n0.0258\n-11.5132\n0.0000\n\n\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n\n\nNortheast\n0.0292\n0.0436\n0.6686\n0.5037\n\n\nNorthwest\n-0.0176\n0.0538\n-0.3268\n0.7438\n\n\nSouth\n0.0566\n0.0527\n1.0740\n0.2828\n\n\nSouthwest\n0.0506\n0.0472\n1.0716\n0.2839\n\n\n\n\n\n\n\nThe results closely matched those from the custom maximum likelihood estimation (MLE) approach, with all coefficients and standard errors agreeing to several decimal places. This confirms that our manually coded log-likelihood function and optimization routine are correctly specified.\nWe‚Äôll now try to quantify the effect of using Blueprinty‚Äôs software by simulating predicted patent counts for the firms in our data in two scenarios, one where all firms are non-customers, and one where all were customers, holding all else constant.\n\n# Create counterfactual datasets\nX_0 = X.copy()\nX_1 = X.copy()\n\n# Set iscustomer to 0 and 1 respectively\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n# Ensure matrix and beta vector are both float64\nX0_mat = X_0.to_numpy().astype(np.float64)\nX1_mat = X_1.to_numpy().astype(np.float64)\nbeta_vec = np.asarray(beta_hat, dtype=np.float64)  # just to be safe\n\n# Predict expected patent counts\ny_pred_0 = np.exp(np.dot(X0_mat, beta_vec))\ny_pred_1 = np.exp(np.dot(X1_mat, beta_vec))\n\neffect_diff = y_pred_1 - y_pred_0\navg_effect = np.mean(effect_diff)\n\nprint(f\"Average predicted increase in patent count from using Blueprinty's software: {avg_effect:.3f}\")\n\nAverage predicted increase in patent count from using Blueprinty's software: 0.793\n\n\nWe find that the average predicted number of patents increased by about 0.793 when firms used Blueprinty‚Äôs software. Resultingly, Blueprinty customers are expected to recieve one additional patent over a five year period compared to similar non-customers."
  },
  {
    "objectID": "projects/hw2/index.html#blueprinty-case-study",
    "href": "projects/hw2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty‚Äôs software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty‚Äôs software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm‚Äôs number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty‚Äôs software. The marketing team would like to use this data to make the claim that firms using Blueprinty‚Äôs software are more successful in getting their patent applications approved.\n\n\n\nFirst, we can read in the data and do some exploratory analysis.\n\nimport pandas as pd\n\ndf1 = pd.read_csv('blueprinty.csv')\ndf1.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nLet‚Äôs compare histograms and means of number of patents by customer status.\n\nimport matplotlib.pyplot as plt\n\n# Create two histograms side by side\nplt.figure(figsize=(12, 5))\n\n# Histogram for non-customers\nplt.subplot(1, 2, 1)\nplt.hist(df1[df1['iscustomer'] == 0]['patents'], bins=30, alpha=0.7)\nplt.title('Non-Customers: Number of Patents')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\n\n# Histogram for customers\nplt.subplot(1, 2, 2)\nplt.hist(df1[df1['iscustomer'] == 1]['patents'], bins=30, alpha=0.7, color='orange')\nplt.title('Customers: Number of Patents')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nHistograms indicate somewhat similar results. Let‚Äôs compare means to test for statistical significance.\n\nfrom scipy.stats import ttest_ind\n\n# Separate the groups\npatents_customers = df1[df1['iscustomer'] == 1]['patents']\npatents_non_customers = df1[df1['iscustomer'] == 0]['patents']\n\n# Calculate means\nmean_customers = patents_customers.mean()\nmean_non_customers = patents_non_customers.mean()\n\n# Perform the t-test\nt_stat, p_value = ttest_ind(patents_customers, patents_non_customers, equal_var=False)\n\n# Print results\nprint(f\"Mean patents (customers): {mean_customers:.2f}\")\nprint(f\"Mean patents (non-customers): {mean_non_customers:.2f}\")\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value: {p_value:.3f}\")\n\nMean patents (customers): 4.13\nMean patents (non-customers): 3.47\nT-statistic: 4.873\nP-value: 0.000\n\n\nBased on our t-test results, we reject the null hypothesis that the means are equal.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nLet‚Äôs compare age between the customer and non customer groups first.\n\nimport seaborn as sns\n\nplt.figure(figsize=(10, 5))\n\nsns.kdeplot(data=df1, x='age', hue='iscustomer', fill=True, common_norm=False, alpha=0.5)\nplt.title('Age Distribution by Customer Status')\nplt.xlabel('Age')\nplt.ylabel('Density')\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.show()\n\n\n\n\n\n\n\n\nThere appears to be a slight difference in age between customers and non customers. Let‚Äôs run a t-test to check.\n\n# Separate the age values\nage_customers = df1[df1['iscustomer'] == 1]['age']\nage_non_customers = df1[df1['iscustomer'] == 0]['age']\n\n# Calculate means\nmean_age_customers = age_customers.mean()\nmean_age_non_customers = age_non_customers.mean()\n\n# Perform the t-test\nt_stat_age, p_value_age = ttest_ind(age_customers, age_non_customers, equal_var=False)\n\n# Print results\nprint(f\"Mean age (customers): {mean_age_customers:.2f}\")\nprint(f\"Mean age (non-customers): {mean_age_non_customers:.2f}\")\nprint(f\"T-statistic (age): {t_stat_age:.3f}\")\nprint(f\"P-value (age): {p_value_age:.3f}\")\n\nMean age (customers): 26.90\nMean age (non-customers): 26.10\nT-statistic (age): 1.913\nP-value (age): 0.056\n\n\nWe fail to reject the null hypothesis, indicating no statistically significant difference in age between the customer and non customer groups. Now we repeat the process for region.\n\n# Create a count table (not normalized)\nregion_counts = pd.crosstab(df1['region'], df1['iscustomer'])\nregion_counts.columns = ['Non-Customers', 'Customers']\n\n# Plot grouped bar chart\nregion_counts.plot(kind='bar', figsize=(10, 6))\nplt.title('Number of Customers and Non-Customers by Region')\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.legend(title='Customer Status')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIt appears based on the chart that there are some differences in the proportions of customers vs.¬†non customers in different regions. We‚Äôll run a chi-squared test to check this.\n\nfrom scipy.stats import chi2_contingency\n\n# Crosstab: region vs customer status\nregion_table = pd.crosstab(df1['region'], df1['iscustomer'])\n\n# Chi-squared test\nchi2_stat, p_value_region, dof, expected = chi2_contingency(region_table)\n\nprint(\"Chi-squared Statistic:\", round(chi2_stat, 2))\nprint(\"Degrees of Freedom:\", dof)\nprint(\"P-value:\", round(p_value_region, 4))\n\nChi-squared Statistic: 233.63\nDegrees of Freedom: 4\nP-value: 0.0\n\n\nBased on the low p-value, we can assume there is a statistically significant relationship between region and customer status. Accordingly, the distribution of customers across regions is not random, as some are more or less likely to have customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nLet‚Äôs mathematically write the likelihood for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nThe likelihood for all observations is:\n\\(L(\\lambda \\mid Y) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\\)\nThe log-likelihood is:\n\\(\\ell(\\lambda \\mid Y) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\\)\nNow let‚Äôs write the log-likelihood function for the Poisson model as a function of lambda and Y.\n\nimport numpy as np\nfrom scipy.special import gammaln  # log(Y!) = gammaln(Y + 1)\n\ndef poisson_loglikelihood(lambd, Y):\n    \"\"\"\n    Compute the log-likelihood of a Poisson model given lambda and observed data Y.\n\n    Parameters:\n    - lambd: float, the Poisson rate parameter (must be &gt; 0)\n    - Y: array-like, observed count data\n\n    Returns:\n    - log-likelihood value\n    \"\"\"\n    if lambd &lt;= 0:\n        return -np.inf  # log-likelihood is undefined for lambda &lt;= 0\n    return np.sum(-lambd + Y * np.log(lambd) - gammaln(Y + 1))\n\nNow, let‚Äôs use the function to plot lambda on the horizontal axis and the log-likelihood on the vertical axis for a range of lambdas.\n\n# Get observed patent data\nY_obs = df1['patents'].values\n\n# Define a range of lambda values to evaluate\nlambda_vals = np.linspace(0.1, 20, 200)\n\n# Compute the log-likelihood for each lambda\nloglik_vals = [poisson_loglikelihood(l, Y_obs) for l in lambda_vals]\n\n# Plot the log-likelihood curve\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='blue', lw=2)\nplt.title('Log-Likelihood of Poisson Model vs. Lambda')\nplt.xlabel(r'$\\lambda$')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nBased on the graph above, the highest log-likelihood appears to occur when lambda is around 3. Now let‚Äôs check and see if our numerical mean is the same as our analytic mean.\nWe start with the log-likelihood:\n\\[\n\\ell(\\lambda \\mid Y) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nTaking the derivative:\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSet this equal to zero and solve:\n\\[\n-n + \\frac{1}{\\lambda} \\sum Y_i = 0 \\Rightarrow \\lambda = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\nTherefore, the MLE of \\(\\lambda\\) is the sample mean \\(\\bar{Y}\\).\nLet‚Äôs find the MLE by optimizing the likelihood function.\n\nfrom scipy.optimize import minimize_scalar\n\n# Reuse observed data\nY_obs = df1['patents'].values\n\n# Define negative log-likelihood function\ndef neg_loglik_poisson(lambd):\n    return -poisson_loglikelihood(lambd, Y_obs)\n\n# Use minimize_scalar to find the lambda that minimizes negative log-likelihood\nresult = minimize_scalar(neg_loglik_poisson, bounds=(0.01, 50), method='bounded')\n\n# Extract MLE\nlambda_mle_numerical = result.x\nloglik_at_mle = -result.fun\n\nprint(f\"MLE (Numerical): {lambda_mle_numerical:.4f}\")\nprint(f\"Log-Likelihood at MLE: {loglik_at_mle:.4f}\")\n\n# Compare to analytic MLE\nprint(f\"MLE (Analytic Mean): {np.mean(Y_obs):.4f}\")\n\nMLE (Numerical): 3.6847\nLog-Likelihood at MLE: -3367.6838\nMLE (Analytic Mean): 3.6847\n\n\nBoth methods agree that the best fitting Poisson model indicates that firms receive, on average, 3.68 patents every five years.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nLet‚Äôs define our updated regression function.\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    \"\"\"\n    Log-likelihood for Poisson regression.\n\n    Parameters:\n    - beta: (p,) vector of coefficients\n    - Y: (n,) vector of observed counts\n    - X: (n, p) matrix of covariates\n\n    Returns:\n    - scalar log-likelihood value\n    \"\"\"\n    beta = np.asarray(beta, dtype=np.float64)  # ensure correct type\n    X = np.asarray(X, dtype=np.float64)        # ensure matrix format\n    Xb = np.dot(X, beta)                       # matrix multiplication\n    lambdas = np.exp(Xb)                       # element-wise exponential\n    return np.sum(-lambdas + Y * Xb - gammaln(Y + 1))\n\nNow, let‚Äôs create the covariate matrix X.\n\nimport patsy  # optional, but can simplify dummy creation\n\n# Create a copy of the data to work with\ndf = df1.copy()\n\n# Create new variables\n# Rescale age and age_squared\ndf['age'] = df['age'] / 10\ndf['age_squared'] = df['age'] ** 2\n\n# One-hot encode region, dropping one to avoid multicollinearity\nregion_dummies = pd.get_dummies(df['region'], drop_first=True)\n\n# Combine into one design matrix\nX = pd.concat([\n    pd.Series(1, index=df.index, name='intercept'),\n    df[['age', 'age_squared', 'iscustomer']],\n    pd.get_dummies(df['region'], drop_first=True)\n], axis=1)\n\nY = df['patents'].values\nX_mat = X.values  # for optimization input only\n\n# Column names\ncolumn_names = X.columns\n\nLet‚Äôs now optimize to find the MLE vector and the Hessian of the Poisson model with covariates.\n\nfrom scipy.optimize import minimize\n\nX_mat = X.to_numpy().astype(np.float64)  # make sure X is a NumPy array\nY_vec = Y.astype(np.float64)             # make sure Y is a NumPy array\ninit_beta = np.zeros(X_mat.shape[1])     # starting guess for beta (all 0s)\n\n# Define the negative log-likelihood function for use in optimizer\nneg_loglik = lambda b: -poisson_regression_loglikelihood(b, Y_vec, X_mat)\n\n# Optimize\nresult = minimize(neg_loglik, init_beta, method='BFGS')\n\nLet‚Äôs now extract the results.\n\n# Extract estimated coefficients and standard errors\nbeta_hat = result.x\nvcov_beta = result.hess_inv\nstandard_errors = np.sqrt(np.diag(vcov_beta))\n\n# Show results in a table\nsummary_table = pd.DataFrame({\n    'Coefficient': beta_hat,\n    'Std. Error': standard_errors\n}, index=X.columns)\n\nsummary_table\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.508925\n0.185670\n\n\nage\n1.486199\n0.139210\n\n\nage_squared\n-0.297048\n0.025840\n\n\niscustomer\n0.207591\n0.032190\n\n\nNortheast\n0.029170\n0.046083\n\n\nNorthwest\n-0.017574\n0.055659\n\n\nSouth\n0.056562\n0.054619\n\n\nSouthwest\n0.050576\n0.048116\n\n\n\n\n\n\n\nThe Poisson regression estimates how the expected number of patents awarded to a firm varies with its characteristics. To improve numerical stability during optimization, the age variable was scaled by a factor of 10 (i.e., a one-unit change in age represents a 10-year difference).\nThe coefficient for age is 1.49, and for age_squared is ‚àí0.30, indicating a non-linear relationship: as firms age, their expected number of patents initially increases, but the rate of increase slows and eventually declines with age.\nThe coefficient on iscustomer is 0.208, meaning that, holding all else equal, firms that use Blueprinty‚Äôs software have a materially higher expected patent count compared to non-customers.\nRegional effects appear minimal, with all region dummy coefficients close to zero and standard errors indicating low statistical significance relative to the reference region.\nNow let‚Äôs compare results against the statsmodels ‚Äòsm.GLM()‚Äô function.\n\nimport statsmodels.api as sm\n\n# Recreate X_sm and ensure it's numeric\nX_sm = sm.add_constant(X.drop(columns='intercept'), has_constant='add')\nX_sm = X_sm.astype(float)  # &lt;== This line prevents the object dtype error\n\n# Also ensure Y is numeric\nY_numeric = Y.astype(float)\n\n# Fit GLM model\nglm_model = sm.GLM(Y_numeric, X_sm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary\n# Create a clean summary table\nsummary_table = pd.DataFrame({\n    'Coefficient': glm_results.params,\n    'Std. Error': glm_results.bse,\n    'z value': glm_results.tvalues,\n    'P&gt;|z|': glm_results.pvalues\n})\n\n# Optional: round for readability\nsummary_table = summary_table.round(4)\n\nsummary_table\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz value\nP&gt;|z|\n\n\n\n\nconst\n-0.5089\n0.1832\n-2.7783\n0.0055\n\n\nage\n1.4862\n0.1387\n10.7162\n0.0000\n\n\nage_squared\n-0.2970\n0.0258\n-11.5132\n0.0000\n\n\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n\n\nNortheast\n0.0292\n0.0436\n0.6686\n0.5037\n\n\nNorthwest\n-0.0176\n0.0538\n-0.3268\n0.7438\n\n\nSouth\n0.0566\n0.0527\n1.0740\n0.2828\n\n\nSouthwest\n0.0506\n0.0472\n1.0716\n0.2839\n\n\n\n\n\n\n\nThe results closely matched those from the custom maximum likelihood estimation (MLE) approach, with all coefficients and standard errors agreeing to several decimal places. This confirms that our manually coded log-likelihood function and optimization routine are correctly specified.\nWe‚Äôll now try to quantify the effect of using Blueprinty‚Äôs software by simulating predicted patent counts for the firms in our data in two scenarios, one where all firms are non-customers, and one where all were customers, holding all else constant.\n\n# Create counterfactual datasets\nX_0 = X.copy()\nX_1 = X.copy()\n\n# Set iscustomer to 0 and 1 respectively\nX_0['iscustomer'] = 0\nX_1['iscustomer'] = 1\n# Ensure matrix and beta vector are both float64\nX0_mat = X_0.to_numpy().astype(np.float64)\nX1_mat = X_1.to_numpy().astype(np.float64)\nbeta_vec = np.asarray(beta_hat, dtype=np.float64)  # just to be safe\n\n# Predict expected patent counts\ny_pred_0 = np.exp(np.dot(X0_mat, beta_vec))\ny_pred_1 = np.exp(np.dot(X1_mat, beta_vec))\n\neffect_diff = y_pred_1 - y_pred_0\navg_effect = np.mean(effect_diff)\n\nprint(f\"Average predicted increase in patent count from using Blueprinty's software: {avg_effect:.3f}\")\n\nAverage predicted increase in patent count from using Blueprinty's software: 0.793\n\n\nWe find that the average predicted number of patents increased by about 0.793 when firms used Blueprinty‚Äôs software. Resultingly, Blueprinty customers are expected to recieve one additional patent over a five year period compared to similar non-customers."
  },
  {
    "objectID": "projects/hw2/index.html#airbnb-case-study",
    "href": "projects/hw2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped off 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nLet‚Äôs load the data and clean it.\n\ndf2 = pd.read_csv('airbnb.csv')\n\ndf2.info()\ndf2.describe()\ndf2.isnull().sum()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 40628 entries, 0 to 40627\nData columns (total 14 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   Unnamed: 0                 40628 non-null  int64  \n 1   id                         40628 non-null  int64  \n 2   days                       40628 non-null  int64  \n 3   last_scraped               40628 non-null  object \n 4   host_since                 40593 non-null  object \n 5   room_type                  40628 non-null  object \n 6   bathrooms                  40468 non-null  float64\n 7   bedrooms                   40552 non-null  float64\n 8   price                      40628 non-null  int64  \n 9   number_of_reviews          40628 non-null  int64  \n 10  review_scores_cleanliness  30433 non-null  float64\n 11  review_scores_location     30374 non-null  float64\n 12  review_scores_value        30372 non-null  float64\n 13  instant_bookable           40628 non-null  object \ndtypes: float64(5), int64(5), object(4)\nmemory usage: 4.3+ MB\n\n\nUnnamed: 0                       0\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\nLet‚Äôs drop the null values in the non-review score columns. Since we‚Äôre using number of reviews as a proxy for bookings, review_scores_‚Ä¶ are not relevant to predict bookings since they happen after the stay. Accordingly, we won‚Äôt remove all those rows with null review scores.\n\n# Keep only relevant, pre-booking predictors\ncols = ['number_of_reviews', 'days', 'room_type', 'bathrooms', 'bedrooms', 'price', 'instant_bookable']\ndf_model = df2[cols].dropna().copy()\n\nLet‚Äôs now prepare the X matrix and outcome Y variables for another Poisson regression model.\n\n# Rescale numeric variables to improve model stability\ndf_model['days'] = df_model['days'] / 100\ndf_model['price'] = df_model['price'] / 100\ndf_model['bathrooms'] = df_model['bathrooms'] / 10\ndf_model['bedrooms'] = df_model['bedrooms'] / 10\n\n# Convert instant_bookable to binary\ndf_model['instant_bookable'] = (df_model['instant_bookable'] == 't').astype(int)\n\n# One-hot encode room_type (drop first category to avoid multicollinearity)\nroom_dummies = pd.get_dummies(df_model['room_type'], drop_first=True)\n\n# Combine into design matrix\nX = pd.concat([\n    df_model[['days', 'bathrooms', 'bedrooms', 'price', 'instant_bookable']],\n    room_dummies\n], axis=1)\n\n# Outcome variable\nY = df_model['number_of_reviews']\n\nX.head()\n\n\n\n\n\n\n\n\ndays\nbathrooms\nbedrooms\nprice\ninstant_bookable\nPrivate room\nShared room\n\n\n\n\n0\n31.30\n0.1\n0.1\n0.59\n0\nTrue\nFalse\n\n\n1\n31.27\n0.1\n0.0\n2.30\n0\nFalse\nFalse\n\n\n2\n30.50\n0.1\n0.1\n1.50\n0\nTrue\nFalse\n\n\n3\n30.38\n0.1\n0.1\n0.89\n0\nFalse\nFalse\n\n\n5\n29.81\n0.1\n0.1\n2.12\n0\nFalse\nFalse\n\n\n\n\n\n\n\nLet‚Äôs fit the Poisson model with statsmodels.\n\nimport statsmodels.api as sm\n\n# Add intercept\nX_sm = sm.add_constant(X)\n\nX_sm = sm.add_constant(X).astype(float)\nY_sm = Y.astype(float)\n\n# Fit the model\nglm_model = sm.GLM(Y, X_sm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Print results\n# Create a clean summary table\nsummary_table = pd.DataFrame({\n    'Coefficient': glm_results.params,\n    'Std. Error': glm_results.bse,\n    'z value': glm_results.tvalues,\n    'P&gt;|z|': glm_results.pvalues\n})\n\n# Optional: round for readability\nsummary_table = summary_table.round(4)\n\nsummary_table\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz value\nP&gt;|z|\n\n\n\n\nconst\n2.7715\n0.0045\n622.5201\n0.0\n\n\ndays\n0.0050\n0.0000\n140.8254\n0.0\n\n\nbathrooms\n-1.0642\n0.0384\n-27.7226\n0.0\n\n\nbedrooms\n0.9817\n0.0200\n48.9806\n0.0\n\n\nprice\n-0.0461\n0.0012\n-37.5604\n0.0\n\n\ninstant_bookable\n0.3748\n0.0029\n130.0457\n0.0\n\n\nPrivate room\n-0.1531\n0.0029\n-53.6916\n0.0\n\n\nShared room\n-0.4080\n0.0087\n-47.1200\n0.0\n\n\n\n\n\n\n\nBased on the output above, the Poisson regression results show several strong predictors of bookings (per the reviews proxy) Listings that are instant bookable recieve almost 50% more bookings, holding other features constant. More bedrooms are also strongly associated with more bookings, whereas higher prices are associated with fewer bookings. Listings classified as private rooms or shared rooms have fewer bookings compared to entire homes. While number of days a listing has been active is positively associated with bookings, this could be a reflection of the cumulative exposure over time."
  }
]